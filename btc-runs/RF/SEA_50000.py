#!/usr/bin/env python3
#
# This code has been produced by an enterprise version of Brainome(tm) licensed to: andy Stevko.
# Portions of this code copyright (c) 2019-2022 by Brainome, Inc. All Rights Reserved.
# Distribution and use of this code or commercial use is permitted within the license terms
# set forth in a written contractual agreement between Brainome, Inc and brainome-user.
# Please contact support@brainome.ai with any questions.
# Use of predictions results at your own risk.
#
# Output of Brainome v1.8-120-prod.
# Invocation: brainome TRAIN_TEST_SPLITS/SEA_50000-clean-train.csv -f RF -y -split 70 -modelonly -q -o btc-runs/RF/SEA_50000.py -json btc-runs/RF/SEA_50000.json
# Total compiler execution time: 0:02:18.60. Finished on: Feb-26-2022 18:32:28.
# This source code requires Python 3.
#
"""

[01;1mPredictor:[0m                        btc-runs/RF/SEA_50000.py
    Classifier Type:              Random Forest
    System Type:                  Binary classifier
    Training / Validation Split:  70% : 30%
    Accuracy:
      Best-guess accuracy:        61.43%
      Training accuracy:          84.52% (414168/489999 correct)
      Validation Accuracy:        83.90% (176193/210001 correct)
      Combined Model Accuracy:    84.33% (590361/700000 correct)


    Model Capacity (MEC):        828    bits
    Generalization Ratio:        481.17 bits/bit
    Percent of Data Memorized:     0.42%
    Resilience to Noise:          -2.70 dB







    Training Confusion Matrix:
              Actual | Predicted
              ------ | ---------
                   1 |  146989   41988 
                   0 |   33843  267179 

    Validation Confusion Matrix:
              Actual | Predicted
              ------ | ---------
                   1 |   62476   18515 
                   0 |   15293  113717 

    Training Accuracy by Class:
               class |      TP      FP      TN      FN     TPR      TNR      PPV      NPV       F1       TS 
               ----- | ------- ------- ------- ------- -------- -------- -------- -------- -------- --------
                   1 |  146989   33843  267179   41988   77.78%   88.76%   81.28%   86.42%   79.49%   65.97%
                   0 |  267179   41988  146989   33843   88.76%   77.78%   86.42%   81.28%   87.57%   77.89%

    Validation Accuracy by Class:
               class |      TP      FP      TN      FN     TPR      TNR      PPV      NPV       F1       TS 
               ----- | ------- ------- ------- ------- -------- -------- -------- -------- -------- --------
                   1 |   62476   15293  113717   18515   77.14%   88.15%   80.34%   86.00%   78.70%   64.89%
                   0 |  113717   18515   62476   15293   88.15%   77.14%   86.00%   80.34%   87.06%   77.08%


    Attribute Ranking:
                                      Feature | Relative Importance
                                      attrib2 :   0.5013
                                      attrib1 :   0.4625
                                      attrib3 :   0.0362
         

"""

import sys
import math
import argparse
import csv
import binascii
import faulthandler
import json
try:
    import numpy as np  # For numpy see: http://numpy.org
except ImportError as e:
    print("This predictor requires the Numpy library. Please run 'python3 -m pip install numpy'.", file=sys.stderr)
    raise e
try:
    from scipy.sparse import coo_matrix
    report_cmat = True
except ImportError:
    print("Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix. Try 'python3 -m pip install scipy'.", file=sys.stderr)
    report_cmat = False
try:
    import multiprocessing
    var_dict = {}
    default_to_serial = False
except:
    default_to_serial = True

IOBUF = 100000000
sys.setrecursionlimit(1000000)
TRAINFILE = ['TRAIN_TEST_SPLITS/SEA_50000-clean-train.csv']
mapping = {'1': 0, '0': 1}
ignorelabels = []
ignorecolumns = []
target = ''
target_column = 3
important_idxs = [0, 1, 2]
ignore_idxs = []
classifier_type = 'RF'
num_attr = 3
n_classes = 2
model_cap = 828
logits_dict = {0: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.454140037, -0.504600048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.454140037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.252300024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.378450006, -0.378450006, 0.504600048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.540642858, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.540642858, 0.0, 0.0, -0.588699996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.454140037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.540642858, 0.0, 0.0, 0.0, 0.0, 0.252300024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.504600048, -0.705820858, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.504600048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.252300024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.378450006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.378450006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.504600048, 0.0, 0.0, 0.0, 0.0, -0.504600048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.588699996, 0.0, -0.378450006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.665154576, -0.454140037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.504600048, 0.0, 0.0, -0.504600048, 0.378450006, 0.0, 0.0, 0.691082656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.567674994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.540642858, 0.0, -0.454140037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.454140037, 0.0, 0.0, -0.454140037, 0.0, 0.0, 0.0, 0.0, 0.648771465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.540642858, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.74062258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.454140037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.567674994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.378450006, 0.0, 0.0, 0.0, 0.0, 0.378450006, 0.0, 0.0, 0.0, 0.0, -0.73680532, 0.0, 0.0, -0.378450006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.454140037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.378450006, -0.60552001, -0.0, -0.540642858, 0.540642858, -0.378450006, 0.0, 0.0, 0.0, 0.0, -0.378450006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.378450006, 0.0, 0.0, 0.0, -0.504600048, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.151380002, -0.567674994, 0.0, 0.0, 0.0, 0.0, 0.0, -0.378450006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.454140037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.378450006, 0.0, 0.504600048, 0.0, 0.378450006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.324385732, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.504600048, 0.0, 0.567674994, -0.378450006, 0.504600048, 0.0, 0.0, 0.0, 0.0, 0.0, -0.540642858, 0.0, 0.0, -0.540642858, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.540642858, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.252300024, 0.0, 0.0, -0.151380002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.454140037, 0.0, 0.454140037, 0.0, 0.0, 0.0, 0.0, 0.567674994, -0.378450006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.691082656, 0.0, 0.302760005, 0.0, 0.378450006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.378450006, 0.0, -0.0, -0.60552001, 0.0, -0.378450006, 0.63075, 0.504600048, 0.0, 0.0, 0.0, 0.640453875, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.588699996, 0.0, 0.0, 0.540642858, -0.540642858, 0.60810864, 0.575660765, 0.454140037, -0.540642858, 0.0841000006, 0.631017685, -0.540642858, 0.610007286, 0.151380002, 0.567674994, 0.590214312, -0.151380002, 0.634135604, 0.449752152, -0.504600048, 0.612469018, 0.0946125016, 0.453642875, 0.63075, -0.454140037, 0.403680027, 0.279519111, 0.378450006, -0.0, -0.0, -0.42050004, -0.108128577, 0.272807807, 0.358117312, 0.0504600033, 0.51054287, 0.593782485, 0.527220011, -0.648771465, 0.6185655, 0.657621443, 0.400075734, 0.608921826, 0.0168200005, 0.526984513, 0.15934737, 0.359383076, -0.068809092, 0.676065981, 0.0291115399, 0.350280583, 0.108128577, -0.454140037, 0.311439455, 0.391065001, -0.252300024, 0.157092452, -0.0, 0.344558954, -0.504600048, 0.186849788, -0.0386737213, 0.166148782, -0.206427276, 0.374521464, 0.259634316, 0.11644616, -0.454140037, 0.0576118156, -0.324385732, -0.0, 0.350758553, 0.0329086967, -0.0329086967, -0.378450006, 0.0285366252, -0.0876410529, 0.252300024, -0.567674994, 0.559179187, -0.454140037, 0.611473024, -0.540642858, 0.252300024, 0.692155004, 0.501666307, 0.644696236, 0.51606822, 0.369169235, -0.252300024, 0.499839664, 0.639877737, 0.361995667, -0.0, 0.61150682, -0.324385732, 0.396975517, 0.231704086, 0.535584211, -0.324385732, 0.33255446, 0.189225003, -0.378450006, 0.405873924, 0.294424683, 0.565645158, 0.252300024, 0.206427276, -0.34404546, 0.433052272, 0.310275316, 0.404679239, 0.292376518, -0.252300024, 0.329338163, -0.137618184, 0.145557702, 0.304220259, 0.112729788, -0.378450006, 0.0329086967, 0.435790896, -0.0582230799, 0.361995667, 0.0302760005, -0.324385732, 0.0177146811, -0.378450006, 0.0582230799, -0.222617656, 0.0311054792, -0.108128577, -0.378450006, 0.100920007, 0.316106737, 0.219102636, 0.473062515, -0.302760005, 0.0100920005, 0.218463853, 0.470505416, 0.00342488708, 0.193625584, -0.0387630723, -0.358531594, 0.17626439, -0.00241821096, -0.0908280015, -0.34404546, 0.0210250001, -0.17602326, -0.0, -0.378450006, 0.296178252, -0.00784113351, 0.018460976, 0.40756157, -0.264914989, -0.0509283058, -0.334821701, -0.173456252, 0.252300024, -0.122080646, -0.132789478, 0.151380002, -0.358531594, -0.0, 0.252300024, -0.252300024, 0.00314439181, -0.189225003, -0.230360866, -0.0226474013, -0.0880116299, -0.34404546, -0.301845342, 0.504600048, -0.397372484, -0.616081357, -0.168895036, -0.310635269, -0.0, 0.540642858, -0.434885532, -0.637837052, -0.273108244, -0.655979991, -0.212878123, -0.557434618, -0.43251431, 0.540642858, -0.572075605, -0.669163704, -0.454140037, 0.479725361, -0.619281828, 0.378450006, 0.331143767, 0.639219463, 0.604977429, 0.401122123, -0.0841000006, 0.51787895, -0.567674994, -0.108128577, 0.438205272, 0.282303244, 0.674329102, 0.43251431, -0.504600048, 0.343771338, 0.23745884, -0.0720857158, 0.318258673, 0.255709469, -0.0, -0.252300024, 0.373659492, 0.265525401, -0.0677820891, 0.378450006, 0.17202273, -0.0513152555, 0.200810209, 0.363576412, -0.189225003, 0.2746557, 0.172626317, 0.312768608, 0.21348463, 0.0273578316, 0.302760005, 0.144499093, 0.216257155, -0.0620409846, -0.0298419371, 0.080833979, -0.151380002, 0.311664701, 0.126150012, 0.42050004, 0.222617656, -0.00625537196, -0.210683495, 0.0244161282, 0.00766758341, -0.069243215, -0.0110094547, 0.283837497, -0.0417920277, -0.402103126, -0.0658173934, -0.299539149, -0.259831369, -0.379741639, -0.481663644, -0.0706439987, 0.0527144559, -0.0379566364, -0.106231585, 0.0292136855, 0.252300024, -0.162192866, -0.21773836, -0.0463408157, 0.0999679267, 0.454140037, -0.278581262, -0.588699996, 0.378450006, -0.216257155, -0.288342863, -0.0445235297, -0.383934796, 0.0841000006, -0.362506419, 0.454140037, -0.175845459, -0.51046747, -0.571267903, 0.454140037, 0.0987260863, -0.115459323, -0.15287143, -0.322686255, -0.376236856, -0.557162523, 0.40756157, -0.454140037, -0.151380002, -0.588699996, 0.40756157, -0.302760005, -0.547425866, 0.454140037, -0.649469078, -0.199184224, -0.365750343, -0.602682412, -0.610421956, -0.357425004, 0.504600048, 0.276914656, 0.26759091, 0.103213638, -0.00850449502, 0.1131, 0.00391500024, 0.270321429, 0.184755117, -0.0398368426, -0.158855557, 0.0676902458, -0.0887983292, -0.283257067, -0.117740005, -0.504600048, -0.0395592339, 0.0161615666, -0.0896328986, 0.169916332, -0.383364946, -0.0841000006, -0.0, -0.222617656, -0.16454348, -0.295375615, -0.233839035, -0.382009119, -0.649411261, -0.400711805, 0.540642858, 0.0326505899, -0.292438626, 0.0360428579, -0.10987258, -0.298728079, -0.40627718, -0.064877145, -0.622006953, -0.252300024, -0.546956956, -0.631810129, -0.126150012, 0.504600048, -0.313955307, -0.619281828, -0.537273705, -0.627882957, -0.540642858, 0.315375, 0.540642858, -0.612576962, 0.151380002, 0.504600048, -0.0, -0.333036005, 0.43251431, -0.00749405939, -0.443700016, -0.0, -0.21348463, 0.068809092, -0.475262791, -0.255268246, -0.545672119, -0.252300024, -0.504600048, -0.151380002, -0.524007738, 0.588699996, -0.638285756, -0.559934974, 0.454140037, -0.454140037, -0.567674994, -0.168200001, -0.570678592, -0.606144965, -0.492440969, 0.504600048, 0.630249381, 0.587801039, -0.454140037, 0.504600048, 0.0504600033, 0.68809092, -0.540642858, 0.336400002, 0.625428557, 0.544968009, 0.579217374, 0.151380002, 0.255042404, 0.42050004, 0.61753428, -0.567674994, -0.504600048, 0.325430453, 0.206427276, -0.524007738, 0.318606019, 0.108128577, 0.627061427, 0.718672752, 0.67805624, 0.333036005, -0.0756900012, 0.355513632, 0.578805923, 0.377563715, 0.26305908, -0.302760005, 0.389918178, 0.113535009, 0.252300024, -0.540642858, 0.465784639, 0.281694174, 0.322244555, 0.167484254, 0.00958101265, 0.222617656, -0.18565473, 0.0321864225, 0.507793725, -0.540642858, 0.0841000006, 0.639211297, 0.467958957, -0.378450006, -0.378450006, 0.552261531, 0.330423355, 0.108128577, -0.489758849, 0.252300024, 0.415392011, 0.297657311, -0.108128577, 0.588699996, 0.417600006, 0.338802874, 0.223015189, 0.308205336, 0.272484004, -0.00890470576, 0.310974419, 0.179265797, 0.236531258, -0.0504600033, 0.00982987043, 0.324385732, -0.0478042103, 0.0316389091, -0.189225003, -0.0, 0.362897247, 0.199935853, -0.0805212781, 0.227070019, 0.540642858, 0.135853857, 0.221247703, -0.0420500003, 0.00484571094, 0.230360866, -0.1479, 0.00850449502, -0.00734854396, -0.25627321, -0.252300024, -0.068809092, -0.540642858, -0.151380002, -0.336400002, 0.00924928673, -0.0104256198, -0.278857887, 0.0323461555, -0.111308828, -0.263960928, 0.222617656, -0.318606019, 0.42050004, -0.486997694, 0.324385732, 0.297701031, 0.0796736851, 0.603326082, 0.315073192, 0.313267112, 0.135160714, 0.176761076, 0.0427877195, 0.396471471, 0.133570597, -0.130500004, 0.0720020905, -0.0398368426, 0.0715477616, -0.0946125016, 0.0454140007, 0.0160835162, 0.504600048, -0.00831758231, -0.259831369, 0.225024328, -0.0591814816, -0.63075, -0.198235735, -0.011185715, -0.134663194, 0.378450006, 0.126150012, -0.0946125016, -0.294914991, -0.40756157, -0.599212527, 0.0792942867, -0.0803787634, -0.281134307, 0.108128577, -0.175379276, -0.309239298, -0.351869911, 0.454140037, -0.664595127, 0.378450006, -0.189225003, -0.623329401, -0.5532251, -0.19407694, -0.588699996, -0.29494226, -0.628330708, 0.151380002, -0.620674431, -0.315375, -0.031537503, -0.412854552, -0.0, 0.230360866, -0.0856867954, -0.306851357, -0.0, 0.524007738, 0.144171432, -0.0375765972, 0.302760005, -0.331675291, -0.239684999, -0.392926246, -0.168200001, 0.567674994, -0.44452861, -0.254064351, -0.423350871, -0.635057569, -0.529829979, 0.311664701, -0.0, -0.650294363, 0.454140037, -0.570777059, -0.133570597, -0.31231311, -0.302760005, -0.491513908, -0.603828192, 0.108128577, 0.324385732, -0.488322586, -0.683302522, -0.623701155, 0.504600048, -0.51787895, -0.108128577, 0.540642858, -0.565471053, -0.189225003, -0.531440437, -0.60388732, 0.189225003, -0.0841000006, -0.252300024, 0.341491997, 0.299239546, 0.0467222221, 0.108128577, 0.310655117, 0.369219542, -0.0, -0.0445235297, -0.588699996, 0.151380002, 0.0117805451, -0.0483127683, 0.218660012, -0.0053850594, 0.178594381, -0.0513733067, -0.222617656, 0.135853857, -0.068809092, -0.324385732, 0.108128577, -0.540642858, -0.0562452264, -0.378450006, -0.162192866, -0.588699996, -0.364433348, 0.068809092, 0.287099987, -0.222617656, 0.0228374992, -0.0256078206, 0.15583235, 0.0392466672, -0.333036005, 0.0754859895, -0.108128577, -0.0323461555, -0.259212345, 0.068809092, -0.233651742, -0.363259166, -0.0, -0.326299489, 0.34404546, -0.174669236, -0.691082656, -0.0194076933, -0.489758849, -0.380226761, -0.602954268, -0.524007738, -0.688914955, -0.555060029, 0.454140037, 0.524007738, -0.504600048, -0.0963327289, 0.0408286303, 0.126150012, 0.454140037, -0.0280333348, -0.208421737, -0.436673075, -0.235338673, -0.151380002, -0.520368755, -0.144171432, -0.289677799, -0.211932003, -0.705584764, -0.363312006, -0.0970384702, 0.302760005, -0.53993845, 0.481663644, -0.0, -0.552024841, -0.678353786, -0.354796886, 0.252300024, -0.715235829, -0.634386241, 0.454140037, -0.504600048, 0.108128577, -0.22664237, -0.57942003, -0.0, -0.589765847, -0.617685735, -0.735578895, -0.30725643, -0.561199665, -0.468861967, 0.252300024, -0.454140037, -0.0, -0.302760005, 0.324385732, 0.0331973694, -0.0, -0.364433348, 0.133570597, -0.071598649, -0.0, 0.42050004, -0.206427276, -0.0, -0.286100954, -0.425108224, -0.0, 0.454140037, -0.298495799, -0.454140037, 0.504600048, -0.133570597, -0.463906467, 0.252300024, -0.252300024, 0.0946125016, -0.0841000006, -0.588699996, -0.416588396, 0.252300024, -0.1576875, 0.454140037, -0.639059305, -0.390658051, -0.0, 0.454140037, -0.600260198, -0.431351602, -0.671715498, -0.582020581, 0.567674994, -0.378450006, -0.624626219, -0.329086959, -0.252300024, -0.62781626, -0.189225003, 0.504600048, -0.567674994, 0.540642858, 0.252300024, -0.599407613, -0.316173404, -0.54473865, -0.616985261, -0.690223634, 0.378450006, -0.606830657, 0.504600048, -0.454140037, -0.547164083, -0.650891602]), 1: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.454140037, 0.504600048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.454140037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.252300024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.378450006, 0.378450006, -0.504600048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.540642858, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.540642858, 0.0, 0.0, 0.588699996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.454140037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.540642858, 0.0, 0.0, 0.0, 0.0, -0.252300024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.504600048, 0.705820858, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.504600048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.252300024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.378450006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.378450006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.504600048, 0.0, 0.0, 0.0, 0.0, 0.504600048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.588699996, 0.0, 0.378450006, 0.0, 0.0, 0.0, 0.0, 0.0, -0.665154576, 0.454140037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.504600048, 0.0, 0.0, 0.504600048, -0.378450006, 0.0, 0.0, -0.691082656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.567674994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.540642858, 0.0, 0.454140037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.454140037, 0.0, 0.0, 0.454140037, 0.0, 0.0, 0.0, 0.0, -0.648771465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.540642858, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.74062258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.454140037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.567674994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.378450006, 0.0, 0.0, 0.0, 0.0, -0.378450006, 0.0, 0.0, 0.0, 0.0, 0.73680532, 0.0, 0.0, 0.378450006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.454140037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.378450006, 0.60552001, -0.0, 0.540642858, -0.540642858, 0.378450006, 0.0, 0.0, 0.0, 0.0, 0.378450006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.378450006, 0.0, 0.0, 0.0, 0.504600048, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.151380002, 0.567674994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.378450006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.454140037, 0.0, 0.0, 0.0, 0.0, 0.0, -0.378450006, 0.0, -0.504600048, 0.0, -0.378450006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.324385732, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.504600048, 0.0, -0.567674994, 0.378450006, -0.504600048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.540642858, 0.0, 0.0, 0.540642858, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.540642858, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.252300024, 0.0, 0.0, 0.151380002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.454140037, 0.0, -0.454140037, 0.0, 0.0, 0.0, 0.0, -0.567674994, 0.378450006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.691082656, 0.0, -0.302760005, 0.0, -0.378450006, 0.0, 0.0, 0.0, 0.0, 0.0, -0.378450006, 0.0, -0.0, 0.60552001, 0.0, 0.378450006, -0.63075, -0.504600048, 0.0, 0.0, 0.0, -0.640453875, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.588699996, 0.0, 0.0, -0.540642858, 0.540642858, -0.60810864, -0.575660765, -0.454140037, 0.540642858, -0.0841000006, -0.631017685, 0.540642858, -0.610007286, -0.151380002, -0.567674994, -0.590214312, 0.151380002, -0.634135604, -0.449752152, 0.504600048, -0.612469018, -0.0946125016, -0.453642875, -0.63075, 0.454140037, -0.403680027, -0.279519111, -0.378450006, -0.0, -0.0, 0.42050004, 0.108128577, -0.272807807, -0.358117312, -0.0504600033, -0.51054287, -0.593782485, -0.527220011, 0.648771465, -0.6185655, -0.657621443, -0.400075734, -0.608921826, -0.0168200005, -0.526984513, -0.15934737, -0.359383076, 0.068809092, -0.676065981, -0.0291115399, -0.350280583, -0.108128577, 0.454140037, -0.311439455, -0.391065001, 0.252300024, -0.157092452, -0.0, -0.344558954, 0.504600048, -0.186849788, 0.0386737213, -0.166148782, 0.206427276, -0.374521464, -0.259634316, -0.11644616, 0.454140037, -0.0576118156, 0.324385732, -0.0, -0.350758553, -0.0329086967, 0.0329086967, 0.378450006, -0.0285366252, 0.0876410529, -0.252300024, 0.567674994, -0.559179187, 0.454140037, -0.611473024, 0.540642858, -0.252300024, -0.692155004, -0.501666307, -0.644696236, -0.51606822, -0.369169235, 0.252300024, -0.499839664, -0.639877737, -0.361995667, -0.0, -0.61150682, 0.324385732, -0.396975517, -0.231704086, -0.535584211, 0.324385732, -0.33255446, -0.189225003, 0.378450006, -0.405873924, -0.294424683, -0.565645158, -0.252300024, -0.206427276, 0.34404546, -0.433052272, -0.310275316, -0.404679239, -0.292376518, 0.252300024, -0.329338163, 0.137618184, -0.145557702, -0.304220259, -0.112729788, 0.378450006, -0.0329086967, -0.435790896, 0.0582230799, -0.361995667, -0.0302760005, 0.324385732, -0.0177146811, 0.378450006, -0.0582230799, 0.222617656, -0.0311054792, 0.108128577, 0.378450006, -0.100920007, -0.316106737, -0.219102636, -0.473062515, 0.302760005, -0.0100920005, -0.218463853, -0.470505416, -0.00342488708, -0.193625584, 0.0387630723, 0.358531594, -0.17626439, 0.00241821096, 0.0908280015, 0.34404546, -0.0210250001, 0.17602326, -0.0, 0.378450006, -0.296178252, 0.00784113351, -0.018460976, -0.40756157, 0.264914989, 0.0509283058, 0.334821701, 0.173456252, -0.252300024, 0.122080646, 0.132789478, -0.151380002, 0.358531594, -0.0, -0.252300024, 0.252300024, -0.00314439181, 0.189225003, 0.230360866, 0.0226474013, 0.0880116299, 0.34404546, 0.301845342, -0.504600048, 0.397372484, 0.616081357, 0.168895036, 0.310635269, -0.0, -0.540642858, 0.434885532, 0.637837052, 0.273108244, 0.655979991, 0.212878123, 0.557434618, 0.43251431, -0.540642858, 0.572075605, 0.669163704, 0.454140037, -0.479725361, 0.619281828, -0.378450006, -0.331143767, -0.639219463, -0.604977429, -0.401122123, 0.0841000006, -0.51787895, 0.567674994, 0.108128577, -0.438205272, -0.282303244, -0.674329102, -0.43251431, 0.504600048, -0.343771338, -0.23745884, 0.0720857158, -0.318258673, -0.255709469, -0.0, 0.252300024, -0.373659492, -0.265525401, 0.0677820891, -0.378450006, -0.17202273, 0.0513152555, -0.200810209, -0.363576412, 0.189225003, -0.2746557, -0.172626317, -0.312768608, -0.21348463, -0.0273578316, -0.302760005, -0.144499093, -0.216257155, 0.0620409846, 0.0298419371, -0.080833979, 0.151380002, -0.311664701, -0.126150012, -0.42050004, -0.222617656, 0.00625537196, 0.210683495, -0.0244161282, -0.00766758341, 0.069243215, 0.0110094547, -0.283837497, 0.0417920277, 0.402103126, 0.0658173934, 0.299539149, 0.259831369, 0.379741639, 0.481663644, 0.0706439987, -0.0527144559, 0.0379566364, 0.106231585, -0.0292136855, -0.252300024, 0.162192866, 0.21773836, 0.0463408157, -0.0999679267, -0.454140037, 0.278581262, 0.588699996, -0.378450006, 0.216257155, 0.288342863, 0.0445235297, 0.383934796, -0.0841000006, 0.362506419, -0.454140037, 0.175845459, 0.51046747, 0.571267903, -0.454140037, -0.0987260863, 0.115459323, 0.15287143, 0.322686255, 0.376236856, 0.557162523, -0.40756157, 0.454140037, 0.151380002, 0.588699996, -0.40756157, 0.302760005, 0.547425866, -0.454140037, 0.649469078, 0.199184224, 0.365750343, 0.602682412, 0.610421956, 0.357425004, -0.504600048, -0.276914656, -0.26759091, -0.103213638, 0.00850449502, -0.1131, -0.00391500024, -0.270321429, -0.184755117, 0.0398368426, 0.158855557, -0.0676902458, 0.0887983292, 0.283257067, 0.117740005, 0.504600048, 0.0395592339, -0.0161615666, 0.0896328986, -0.169916332, 0.383364946, 0.0841000006, -0.0, 0.222617656, 0.16454348, 0.295375615, 0.233839035, 0.382009119, 0.649411261, 0.400711805, -0.540642858, -0.0326505899, 0.292438626, -0.0360428579, 0.10987258, 0.298728079, 0.40627718, 0.064877145, 0.622006953, 0.252300024, 0.546956956, 0.631810129, 0.126150012, -0.504600048, 0.313955307, 0.619281828, 0.537273705, 0.627882957, 0.540642858, -0.315375, -0.540642858, 0.612576962, -0.151380002, -0.504600048, -0.0, 0.333036005, -0.43251431, 0.00749405939, 0.443700016, -0.0, 0.21348463, -0.068809092, 0.475262791, 0.255268246, 0.545672119, 0.252300024, 0.504600048, 0.151380002, 0.524007738, -0.588699996, 0.638285756, 0.559934974, -0.454140037, 0.454140037, 0.567674994, 0.168200001, 0.570678592, 0.606144965, 0.492440969, -0.504600048, -0.630249381, -0.587801039, 0.454140037, -0.504600048, -0.0504600033, -0.68809092, 0.540642858, -0.336400002, -0.625428557, -0.544968009, -0.579217374, -0.151380002, -0.255042404, -0.42050004, -0.61753428, 0.567674994, 0.504600048, -0.325430453, -0.206427276, 0.524007738, -0.318606019, -0.108128577, -0.627061427, -0.718672752, -0.67805624, -0.333036005, 0.0756900012, -0.355513632, -0.578805923, -0.377563715, -0.26305908, 0.302760005, -0.389918178, -0.113535009, -0.252300024, 0.540642858, -0.465784639, -0.281694174, -0.322244555, -0.167484254, -0.00958101265, -0.222617656, 0.18565473, -0.0321864225, -0.507793725, 0.540642858, -0.0841000006, -0.639211297, -0.467958957, 0.378450006, 0.378450006, -0.552261531, -0.330423355, -0.108128577, 0.489758849, -0.252300024, -0.415392011, -0.297657311, 0.108128577, -0.588699996, -0.417600006, -0.338802874, -0.223015189, -0.308205336, -0.272484004, 0.00890470576, -0.310974419, -0.179265797, -0.236531258, 0.0504600033, -0.00982987043, -0.324385732, 0.0478042103, -0.0316389091, 0.189225003, -0.0, -0.362897247, -0.199935853, 0.0805212781, -0.227070019, -0.540642858, -0.135853857, -0.221247703, 0.0420500003, -0.00484571094, -0.230360866, 0.1479, -0.00850449502, 0.00734854396, 0.25627321, 0.252300024, 0.068809092, 0.540642858, 0.151380002, 0.336400002, -0.00924928673, 0.0104256198, 0.278857887, -0.0323461555, 0.111308828, 0.263960928, -0.222617656, 0.318606019, -0.42050004, 0.486997694, -0.324385732, -0.297701031, -0.0796736851, -0.603326082, -0.315073192, -0.313267112, -0.135160714, -0.176761076, -0.0427877195, -0.396471471, -0.133570597, 0.130500004, -0.0720020905, 0.0398368426, -0.0715477616, 0.0946125016, -0.0454140007, -0.0160835162, -0.504600048, 0.00831758231, 0.259831369, -0.225024328, 0.0591814816, 0.63075, 0.198235735, 0.011185715, 0.134663194, -0.378450006, -0.126150012, 0.0946125016, 0.294914991, 0.40756157, 0.599212527, -0.0792942867, 0.0803787634, 0.281134307, -0.108128577, 0.175379276, 0.309239298, 0.351869911, -0.454140037, 0.664595127, -0.378450006, 0.189225003, 0.623329401, 0.5532251, 0.19407694, 0.588699996, 0.29494226, 0.628330708, -0.151380002, 0.620674431, 0.315375, 0.031537503, 0.412854552, -0.0, -0.230360866, 0.0856867954, 0.306851357, -0.0, -0.524007738, -0.144171432, 0.0375765972, -0.302760005, 0.331675291, 0.239684999, 0.392926246, 0.168200001, -0.567674994, 0.44452861, 0.254064351, 0.423350871, 0.635057569, 0.529829979, -0.311664701, -0.0, 0.650294363, -0.454140037, 0.570777059, 0.133570597, 0.31231311, 0.302760005, 0.491513908, 0.603828192, -0.108128577, -0.324385732, 0.488322586, 0.683302522, 0.623701155, -0.504600048, 0.51787895, 0.108128577, -0.540642858, 0.565471053, 0.189225003, 0.531440437, 0.60388732, -0.189225003, 0.0841000006, 0.252300024, -0.341491997, -0.299239546, -0.0467222221, -0.108128577, -0.310655117, -0.369219542, -0.0, 0.0445235297, 0.588699996, -0.151380002, -0.0117805451, 0.0483127683, -0.218660012, 0.0053850594, -0.178594381, 0.0513733067, 0.222617656, -0.135853857, 0.068809092, 0.324385732, -0.108128577, 0.540642858, 0.0562452264, 0.378450006, 0.162192866, 0.588699996, 0.364433348, -0.068809092, -0.287099987, 0.222617656, -0.0228374992, 0.0256078206, -0.15583235, -0.0392466672, 0.333036005, -0.0754859895, 0.108128577, 0.0323461555, 0.259212345, -0.068809092, 0.233651742, 0.363259166, -0.0, 0.326299489, -0.34404546, 0.174669236, 0.691082656, 0.0194076933, 0.489758849, 0.380226761, 0.602954268, 0.524007738, 0.688914955, 0.555060029, -0.454140037, -0.524007738, 0.504600048, 0.0963327289, -0.0408286303, -0.126150012, -0.454140037, 0.0280333348, 0.208421737, 0.436673075, 0.235338673, 0.151380002, 0.520368755, 0.144171432, 0.289677799, 0.211932003, 0.705584764, 0.363312006, 0.0970384702, -0.302760005, 0.53993845, -0.481663644, -0.0, 0.552024841, 0.678353786, 0.354796886, -0.252300024, 0.715235829, 0.634386241, -0.454140037, 0.504600048, -0.108128577, 0.22664237, 0.57942003, -0.0, 0.589765847, 0.617685735, 0.735578895, 0.30725643, 0.561199665, 0.468861967, -0.252300024, 0.454140037, -0.0, 0.302760005, -0.324385732, -0.0331973694, -0.0, 0.364433348, -0.133570597, 0.071598649, -0.0, -0.42050004, 0.206427276, -0.0, 0.286100954, 0.425108224, -0.0, -0.454140037, 0.298495799, 0.454140037, -0.504600048, 0.133570597, 0.463906467, -0.252300024, 0.252300024, -0.0946125016, 0.0841000006, 0.588699996, 0.416588396, -0.252300024, 0.1576875, -0.454140037, 0.639059305, 0.390658051, -0.0, -0.454140037, 0.600260198, 0.431351602, 0.671715498, 0.582020581, -0.567674994, 0.378450006, 0.624626219, 0.329086959, 0.252300024, 0.62781626, 0.189225003, -0.504600048, 0.567674994, -0.540642858, -0.252300024, 0.599407613, 0.316173404, 0.54473865, 0.616985261, 0.690223634, -0.378450006, 0.606830657, -0.504600048, 0.454140037, 0.547164083, 0.650891602])}
right_children_dict = {0: np.array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99, 101, 103, 105, 107, 109, 111, 113, 115, 117, 119, 121, 123, -1, -1, 125, 127, 129, 131, 133, 135, 137, 139, 141, 143, 145, 147, 149, 151, 153, 155, 157, 159, 161, 163, 165, 167, 169, 171, 173, 175, 177, 179, 181, 183, 185, 187, 189, 191, 193, 195, 197, 199, 201, 203, 205, 207, 209, 211, 213, 215, 217, 219, 221, 223, 225, 227, 229, 231, 233, 235, 237, 239, 241, 243, 245, 247, 249, 251, 253, 255, 257, -1, 259, 261, 263, 265, 267, 269, 271, 273, 275, 277, 279, 281, 283, 285, 287, 289, 291, 293, 295, 297, 299, 301, 303, 305, 307, 309, 311, 313, 315, 317, 319, 321, 323, 325, 327, 329, 331, 333, -1, 335, 337, 339, 341, 343, 345, 347, 349, 351, 353, 355, -1, -1, -1, 357, 359, 361, 363, 365, 367, 369, 371, 373, 375, 377, 379, 381, 383, 385, 387, 389, 391, 393, 395, 397, 399, 401, 403, 405, 407, 409, 411, 413, 415, 417, 419, 421, 423, 425, 427, 429, 431, 433, 435, 437, 439, 441, 443, 445, 447, 449, 451, -1, 453, 455, 457, 459, 461, 463, 465, 467, 469, 471, 473, 475, 477, 479, 481, -1, 483, 485, -1, 487, 489, 491, 493, 495, 497, 499, 501, 503, 505, -1, 507, 509, 511, 513, 515, 517, 519, 521, 523, 525, 527, 529, 531, 533, 535, 537, 539, 541, 543, 545, 547, 549, 551, 553, 555, 557, 559, 561, 563, 565, 567, 569, 571, 573, 575, 577, 579, 581, 583, 585, 587, 589, 591, 593, 595, 597, 599, 601, 603, 605, 607, 609, 611, 613, 615, 617, 619, 621, 623, 625, 627, 629, 631, 633, -1, 635, 637, 639, 641, -1, 643, 645, 647, 649, 651, 653, 655, 657, 659, 661, 663, 665, 667, 669, 671, 673, 675, 677, 679, 681, 683, 685, 687, 689, -1, -1, 691, 693, 695, 697, 699, 701, 703, 705, 707, 709, 711, 713, 715, 717, 719, 721, 723, 725, 727, 729, 731, 733, 735, 737, 739, 741, 743, 745, 747, 749, 751, 753, 755, 757, 759, 761, 763, 765, 767, 769, 771, 773, 775, 777, 779, 781, 783, 785, -1, 787, 789, 791, 793, 795, 797, 799, -1, 801, 803, 805, 807, 809, 811, -1, 813, 815, 817, 819, 821, 823, 825, 827, 829, 831, 833, 835, 837, 839, 841, 843, 845, 847, 849, 851, 853, 855, 857, 859, 861, 863, 865, 867, -1, 869, 871, 873, 875, 877, 879, 881, 883, 885, 887, 889, 891, 893, 895, 897, 899, 901, -1, 903, 905, 907, 909, -1, 911, 913, 915, 917, 919, 921, 923, -1, -1, 925, -1, 927, 929, 931, 933, 935, -1, -1, 937, 939, 941, 943, 945, 947, -1, 949, 951, -1, -1, 953, 955, -1, 957, 959, 961, 963, 965, 967, 969, 971, 973, 975, 977, -1, 979, 981, 983, 985, 987, 989, 991, 993, 995, -1, 997, -1, 999, 1001, 1003, 1005, 1007, 1009, 1011, 1013, 1015, 1017, -1, 1019, 1021, -1, 1023, 1025, 1027, 1029, -1, 1031, 1033, 1035, 1037, 1039, 1041, 1043, 1045, 1047, 1049, 1051, 1053, 1055, 1057, 1059, 1061, 1063, 1065, 1067, 1069, 1071, 1073, 1075, 1077, 1079, 1081, 1083, 1085, 1087, 1089, 1091, 1093, 1095, -1, 1097, 1099, 1101, 1103, 1105, 1107, 1109, 1111, -1, 1113, 1115, 1117, 1119, 1121, 1123, 1125, 1127, 1129, 1131, 1133, 1135, 1137, 1139, 1141, 1143, 1145, 1147, 1149, 1151, 1153, 1155, 1157, 1159, 1161, 1163, 1165, 1167, 1169, 1171, 1173, 1175, 1177, 1179, 1181, 1183, 1185, 1187, -1, 1189, 1191, 1193, 1195, 1197, 1199, 1201, 1203, 1205, 1207, 1209, 1211, 1213, 1215, 1217, -1, 1219, 1221, 1223, 1225, 1227, 1229, 1231, 1233, 1235, 1237, 1239, 1241, 1243, 1245, -1, 1247, 1249, 1251, 1253, -1, 1255, 1257, 1259, 1261, -1, 1263, 1265, -1, 1267, 1269, 1271, 1273, 1275, 1277, 1279, -1, 1281, 1283, 1285, 1287, 1289, 1291, -1, -1, -1, -1, -1, -1, 1293, 1295, 1297, 1299, -1, 1301, 1303, 1305, 1307, 1309, 1311, 1313, -1, 1315, 1317, 1319, -1, 1321, 1323, 1325, 1327, 1329, -1, -1, -1, 1331, 1333, 1335, 1337, 1339, -1, 1341, 1343, 1345, 1347, 1349, 1351, 1353, 1355, 1357, 1359, 1361, 1363, 1365, 1367, 1369, 1371, 1373, 1375, 1377, 1379, 1381, 1383, 1385, 1387, 1389, 1391, 1393, 1395, 1397, 1399, 1401, 1403, 1405, 1407, 1409, 1411, 1413, 1415, 1417, 1419, 1421, 1423, 1425, 1427, 1429, 1431, 1433, 1435, -1, 1437, 1439, 1441, 1443, 1445, -1, 1447, -1, 1449, -1, 1451, 1453, 1455, 1457, 1459, 1461, 1463, 1465, -1, 1467, 1469, 1471, 1473, 1475, 1477, -1, 1479, -1, -1, -1, 1481, 1483, 1485, 1487, 1489, -1, 1491, 1493, -1, 1495, 1497, 1499, 1501, 1503, 1505, 1507, 1509, 1511, 1513, 1515, 1517, 1519, 1521, 1523, 1525, 1527, 1529, 1531, 1533, 1535, 1537, 1539, 1541, 1543, -1, 1545, 1547, 1549, 1551, 1553, 1555, 1557, 1559, 1561, 1563, 1565, 1567, -1, 1569, 1571, -1, 1573, 1575, 1577, 1579, 1581, 1583, -1, 1585, -1, 1587, 1589, 1591, 1593, -1, -1, 1595, 1597, 1599, 1601, 1603, 1605, 1607, 1609, 1611, 1613, 1615, -1, 1617, -1, 1619, -1, 1621, 1623, 1625, 1627, 1629, -1, 1631, -1, -1, 1633, -1, -1, -1, 1635, 1637, 1639, -1, 1641, 1643, 1645, 1647, 1649, -1, -1, 1651, 1653, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 1: np.array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99, 101, 103, 105, 107, 109, 111, 113, 115, 117, 119, 121, 123, -1, -1, 125, 127, 129, 131, 133, 135, 137, 139, 141, 143, 145, 147, 149, 151, 153, 155, 157, 159, 161, 163, 165, 167, 169, 171, 173, 175, 177, 179, 181, 183, 185, 187, 189, 191, 193, 195, 197, 199, 201, 203, 205, 207, 209, 211, 213, 215, 217, 219, 221, 223, 225, 227, 229, 231, 233, 235, 237, 239, 241, 243, 245, 247, 249, 251, 253, 255, 257, -1, 259, 261, 263, 265, 267, 269, 271, 273, 275, 277, 279, 281, 283, 285, 287, 289, 291, 293, 295, 297, 299, 301, 303, 305, 307, 309, 311, 313, 315, 317, 319, 321, 323, 325, 327, 329, 331, 333, -1, 335, 337, 339, 341, 343, 345, 347, 349, 351, 353, 355, -1, -1, -1, 357, 359, 361, 363, 365, 367, 369, 371, 373, 375, 377, 379, 381, 383, 385, 387, 389, 391, 393, 395, 397, 399, 401, 403, 405, 407, 409, 411, 413, 415, 417, 419, 421, 423, 425, 427, 429, 431, 433, 435, 437, 439, 441, 443, 445, 447, 449, 451, -1, 453, 455, 457, 459, 461, 463, 465, 467, 469, 471, 473, 475, 477, 479, 481, -1, 483, 485, -1, 487, 489, 491, 493, 495, 497, 499, 501, 503, 505, -1, 507, 509, 511, 513, 515, 517, 519, 521, 523, 525, 527, 529, 531, 533, 535, 537, 539, 541, 543, 545, 547, 549, 551, 553, 555, 557, 559, 561, 563, 565, 567, 569, 571, 573, 575, 577, 579, 581, 583, 585, 587, 589, 591, 593, 595, 597, 599, 601, 603, 605, 607, 609, 611, 613, 615, 617, 619, 621, 623, 625, 627, 629, 631, 633, -1, 635, 637, 639, 641, -1, 643, 645, 647, 649, 651, 653, 655, 657, 659, 661, 663, 665, 667, 669, 671, 673, 675, 677, 679, 681, 683, 685, 687, 689, -1, -1, 691, 693, 695, 697, 699, 701, 703, 705, 707, 709, 711, 713, 715, 717, 719, 721, 723, 725, 727, 729, 731, 733, 735, 737, 739, 741, 743, 745, 747, 749, 751, 753, 755, 757, 759, 761, 763, 765, 767, 769, 771, 773, 775, 777, 779, 781, 783, 785, -1, 787, 789, 791, 793, 795, 797, 799, -1, 801, 803, 805, 807, 809, 811, -1, 813, 815, 817, 819, 821, 823, 825, 827, 829, 831, 833, 835, 837, 839, 841, 843, 845, 847, 849, 851, 853, 855, 857, 859, 861, 863, 865, 867, -1, 869, 871, 873, 875, 877, 879, 881, 883, 885, 887, 889, 891, 893, 895, 897, 899, 901, -1, 903, 905, 907, 909, -1, 911, 913, 915, 917, 919, 921, 923, -1, -1, 925, -1, 927, 929, 931, 933, 935, -1, -1, 937, 939, 941, 943, 945, 947, -1, 949, 951, -1, -1, 953, 955, -1, 957, 959, 961, 963, 965, 967, 969, 971, 973, 975, 977, -1, 979, 981, 983, 985, 987, 989, 991, 993, 995, -1, 997, -1, 999, 1001, 1003, 1005, 1007, 1009, 1011, 1013, 1015, 1017, -1, 1019, 1021, -1, 1023, 1025, 1027, 1029, -1, 1031, 1033, 1035, 1037, 1039, 1041, 1043, 1045, 1047, 1049, 1051, 1053, 1055, 1057, 1059, 1061, 1063, 1065, 1067, 1069, 1071, 1073, 1075, 1077, 1079, 1081, 1083, 1085, 1087, 1089, 1091, 1093, 1095, -1, 1097, 1099, 1101, 1103, 1105, 1107, 1109, 1111, -1, 1113, 1115, 1117, 1119, 1121, 1123, 1125, 1127, 1129, 1131, 1133, 1135, 1137, 1139, 1141, 1143, 1145, 1147, 1149, 1151, 1153, 1155, 1157, 1159, 1161, 1163, 1165, 1167, 1169, 1171, 1173, 1175, 1177, 1179, 1181, 1183, 1185, 1187, -1, 1189, 1191, 1193, 1195, 1197, 1199, 1201, 1203, 1205, 1207, 1209, 1211, 1213, 1215, 1217, -1, 1219, 1221, 1223, 1225, 1227, 1229, 1231, 1233, 1235, 1237, 1239, 1241, 1243, 1245, -1, 1247, 1249, 1251, 1253, -1, 1255, 1257, 1259, 1261, -1, 1263, 1265, -1, 1267, 1269, 1271, 1273, 1275, 1277, 1279, -1, 1281, 1283, 1285, 1287, 1289, 1291, -1, -1, -1, -1, -1, -1, 1293, 1295, 1297, 1299, -1, 1301, 1303, 1305, 1307, 1309, 1311, 1313, -1, 1315, 1317, 1319, -1, 1321, 1323, 1325, 1327, 1329, -1, -1, -1, 1331, 1333, 1335, 1337, 1339, -1, 1341, 1343, 1345, 1347, 1349, 1351, 1353, 1355, 1357, 1359, 1361, 1363, 1365, 1367, 1369, 1371, 1373, 1375, 1377, 1379, 1381, 1383, 1385, 1387, 1389, 1391, 1393, 1395, 1397, 1399, 1401, 1403, 1405, 1407, 1409, 1411, 1413, 1415, 1417, 1419, 1421, 1423, 1425, 1427, 1429, 1431, 1433, 1435, -1, 1437, 1439, 1441, 1443, 1445, -1, 1447, -1, 1449, -1, 1451, 1453, 1455, 1457, 1459, 1461, 1463, 1465, -1, 1467, 1469, 1471, 1473, 1475, 1477, -1, 1479, -1, -1, -1, 1481, 1483, 1485, 1487, 1489, -1, 1491, 1493, -1, 1495, 1497, 1499, 1501, 1503, 1505, 1507, 1509, 1511, 1513, 1515, 1517, 1519, 1521, 1523, 1525, 1527, 1529, 1531, 1533, 1535, 1537, 1539, 1541, 1543, -1, 1545, 1547, 1549, 1551, 1553, 1555, 1557, 1559, 1561, 1563, 1565, 1567, -1, 1569, 1571, -1, 1573, 1575, 1577, 1579, 1581, 1583, -1, 1585, -1, 1587, 1589, 1591, 1593, -1, -1, 1595, 1597, 1599, 1601, 1603, 1605, 1607, 1609, 1611, 1613, 1615, -1, 1617, -1, 1619, -1, 1621, 1623, 1625, 1627, 1629, -1, 1631, -1, -1, 1633, -1, -1, -1, 1635, 1637, 1639, -1, 1641, 1643, 1645, 1647, 1649, -1, -1, 1651, 1653, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])}
split_feats_dict = {0: np.array([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 2, 0, 2, 1, 2, 2, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 2, 1, 0, 0, 1, 0, 0, 1, 2, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 2, 2, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 2, 0, 0, 0, 0, 1, 1, 2, 0, 0, 1, 1, 2, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 2, 2, 1, 0, 0, 0, 1, 1, 0, 1, 2, 1, 1, 1, 1, 0, 2, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 2, 2, 0, 1, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 2, 1, 2, 2, 0, 0, 0, 1, 1, 0, 2, 1, 2, 0, 0, 1, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 2, 1, 1, 1, 1, 1, 2, 0, 0, 2, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 1, 2, 1, 1, 0, 1, 2, 0, 0, 0, 2, 0, 0, 0, 1, 1, 2, 1, 1, 0, 1, 1, 1, 0, 0, 0, 2, 2, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 0, 0, 1, 1, 1, 0, 0, 2, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 2, 0, 0, 1, 2, 1, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 1, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 2, 0, 0, 0, 0, 2, 2, 2, 0, 2, 2, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 1, 0, 1, 0, 2, 2, 0, 2, 1, 0, 0, 1, 0, 0, 2, 2, 1, 0, 1, 0, 2, 2, 0, 0, 1, 0, 2, 0, 1, 0, 1, 0, 1, 0, 2, 0, 1, 0, 1, 2, 2, 0, 2, 0, 0, 0, 2, 2, 0, 2, 1, 0, 0, 0, 2, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 2, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 2, 1, 0, 2, 1, 0, 1, 0, 1, 2, 0, 1, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 1, 0, 0, 2, 0, 2, 1, 0, 1, 1, 1, 2, 1, 2, 0, 2, 0, 0, 1, 0, 2, 1, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 2, 2, 0, 2, 2, 0, 2, 0, 2, 2, 0, 1, 0, 1, 2, 2, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 1, 0, 0, 2, 1, 0, 1, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2, 0, 1, 2, 1, 1, 2, 0, 2, 1, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 0, 2, 2, 0, 2, 2, 0, 0, 1, 2, 1, 1, 0, 0, 2, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 2, 1, 1, 2, 1, 2, 2, 1, 0, 2, 0, 0, 0, 2, 1, 0, 0, 1, 0, 2, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 2, 2, 1, 1, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 2, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 2, 1, 1, 1, 0, 0, 1, 1, 2, 0, 2, 0, 2, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 2, 0, 0, 1, 0, 0, 2, 0, 1, 1, 1, 2, 0, 0, 0, 1, 0, 1, 2, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 2, 0, 1, 2, 1, 0, 1, 0, 1, 0, 0, 2, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 2, 0, 2, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 1: np.array([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 2, 0, 2, 1, 2, 2, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 2, 1, 0, 0, 1, 0, 0, 1, 2, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 2, 2, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 2, 0, 0, 0, 0, 1, 1, 2, 0, 0, 1, 1, 2, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 2, 2, 1, 0, 0, 0, 1, 1, 0, 1, 2, 1, 1, 1, 1, 0, 2, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 2, 2, 0, 1, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 2, 1, 2, 2, 0, 0, 0, 1, 1, 0, 2, 1, 2, 0, 0, 1, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 2, 1, 1, 1, 1, 1, 2, 0, 0, 2, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 1, 2, 1, 1, 0, 1, 2, 0, 0, 0, 2, 0, 0, 0, 1, 1, 2, 1, 1, 0, 1, 1, 1, 0, 0, 0, 2, 2, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 1, 1, 0, 0, 1, 1, 1, 0, 0, 2, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 2, 0, 0, 1, 2, 1, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 1, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 2, 0, 0, 0, 0, 2, 2, 2, 0, 2, 2, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 1, 0, 1, 0, 2, 2, 0, 2, 1, 0, 0, 1, 0, 0, 2, 2, 1, 0, 1, 0, 2, 2, 0, 0, 1, 0, 2, 0, 1, 0, 1, 0, 1, 0, 2, 0, 1, 0, 1, 2, 2, 0, 2, 0, 0, 0, 2, 2, 0, 2, 1, 0, 0, 0, 2, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 2, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 2, 1, 0, 2, 1, 0, 1, 0, 1, 2, 0, 1, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 1, 0, 0, 2, 0, 2, 1, 0, 1, 1, 1, 2, 1, 2, 0, 2, 0, 0, 1, 0, 2, 1, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 2, 2, 0, 2, 2, 0, 2, 0, 2, 2, 0, 1, 0, 1, 2, 2, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 1, 0, 0, 2, 1, 0, 1, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2, 0, 1, 2, 1, 1, 2, 0, 2, 1, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 0, 2, 2, 0, 2, 2, 0, 0, 1, 2, 1, 1, 0, 0, 2, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 2, 1, 1, 2, 1, 2, 2, 1, 0, 2, 0, 0, 0, 2, 1, 0, 0, 1, 0, 2, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 2, 2, 1, 1, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 2, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 2, 1, 1, 1, 0, 0, 1, 1, 2, 0, 2, 0, 2, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 2, 0, 0, 1, 0, 0, 2, 0, 1, 1, 1, 2, 0, 0, 0, 1, 0, 1, 2, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 2, 0, 1, 2, 1, 0, 1, 0, 1, 0, 0, 2, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 2, 0, 2, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
split_vals_dict = {0: np.array([4.49343729, 6.00253391, 2.54766369, 4.44183922, 1.51696801, 7.26109648, 4.11555147, 3.5985074, 2.79674459, 8.03351212, 7.22332859, 6.16110992, 8.27483845, 6.00265121, 5.01680088, 2.84994793, 3.07493901, 1.98668253, 3.52611494, 6.95812225, 8.71343613, 2.61801004, 7.59564781, 1.79338408, 1.44609499, 1.38297939, 0.562618971, 3.28847599, 6.53277683, 4.68567181, 9.99995995, 0.000230999998, 3.827878, 2.52191401, 3.84765291, 1.34451449, 5.53880548, 5.16515827, 5.20963144, 0.621227026, 0.798491001, 0.735219002, 0.425974011, 2.06265783, 3.06089044, 2.0312705, 0.00342399999, 5.40696144, 5.38802099, 0.745553493, 1.95692801, 0.683198988, 1.86748099, 9.05969715, 8.86323929, 5.26025105, 5.59020042, 3.17023158, 9.30273628, 4.41034508, 5.01011086, 9.94260597, 0.0, 0.0, 9.99748802, 2.85042334, 3.10625887, 3.59874392, 4.22507572, 4.15673637, 4.01289749, 5.65139771, 5.30051041, 4.89049721, 2.28088856, 3.13874912, 5.68536949, 4.03421307, 3.96955919, 6.73806286, 1.248353, 7.64062595, 7.62219143, 0.567203999, 1.09373498, 9.34695625, 1.51583099, 6.42043877, 6.66902351, 6.46104717, 6.36724806, 1.67018056, 7.59555054, 2.06705594, 4.46745014, 0.0244050007, 1.19974053, 4.7966547, 1.97034097, 6.46984768, 6.81758404, 6.34593487, 6.69675064, 7.80259895, 7.92737913, 7.76947403, 7.44841337, 8.69834518, 9.36070442, 1.17671943, 1.02644944, 4.88763237, 3.08194256, 5.1542654, 3.68589854, 2.90913701, 3.34871244, 2.68418503, 9.30384922, 4.77806473, 9.39378166, 4.90970039, 1.82953095, 0.0227585007, 9.94319725, 1.98332345, 0.807707489, 2.85029793, 3.36849499, 3.97099161, 3.83405113, 0.0, 3.78003263, 2.52307081, 4.22617531, 3.29004145, 3.59896708, 4.25649834, 0.522124529, 4.48583412, 1.04598451, 1.74659204, 5.58746529, 2.38363647, 2.70679355, 5.84551954, 9.00734997, 5.00422859, 4.77875328, 0.960305989, 3.18644428, 4.49789667, 4.81099415, 5.65006256, 5.4839654, 6.04062796, 0.17167899, 6.21630096, 6.55621004, 0.459105492, 0.302353501, 0.944546461, 1.14985597, 8.45279694, 8.31921577, 8.17863083, 8.70559311, 0.284805, 0.301471502, 0.00329199992, 0.0, 1.83795452, 6.93801451, 6.56131697, 2.29894543, 6.20991421, 6.66339016, 3.27856398, 7.22108507, 7.44037724, 7.58805466, 9.81595802, 0.0, 0.0, 0.0, 4.46730137, 9.92674637, 1.08395803, 9.88646317, 5.94263077, 5.6494832, 2.27195787, 2.28421354, 9.91385651, 5.80261612, 0.605758488, 0.270085514, 6.416749, 1.02657449, 1.80283999, 1.68352842, 6.17224264, 2.23732448, 0.253251493, 8.01565075, 7.78896952, 0.906191468, 7.49048519, 7.9290638, 2.10620642, 8.27417564, 0.398752987, 0.147648007, 0.222129494, 0.560171485, 0.737236023, 0.00318450015, 9.95735168, 1.028036, 0.141577005, 2.9159236, 7.86435461, 5.84556961, 4.64619923, 3.60628605, 3.65889597, 5.98904228, 6.17265558, 6.46379375, 3.3472929, 6.20921612, 6.81984425, 4.11540222, 0.0, 9.99660206, 4.83556271, 4.14459229, 4.89476156, 4.74167252, 4.65855646, 9.53120995, 8.82756615, 5.01657963, 9.64037132, 0.706618547, 6.72679138, 9.96512794, 1.98265505, 2.53786707, 0.0, 9.99803352, 6.77105522, 0.0, 3.38698149, 3.31458759, 2.8836875, 3.97142053, 3.38378, 3.40778756, 3.77992749, 0.149246007, 3.90486503, 3.97788715, 0.0, 2.5343194, 3.80132961, 3.59959936, 3.12141514, 4.35834837, 3.746665, 3.60983133, 0.140523002, 0.86195147, 0.632369518, 1.25609708, 5.65188408, 1.04788208, 4.47184086, 5.05264091, 1.64018047, 1.34703994, 9.98088264, 4.4466815, 4.89132214, 5.23995972, 2.27763605, 6.89152718, 5.55940294, 9.31015205, 0.0740565062, 3.03136158, 3.37488747, 3.33026147, 5.28979874, 5.17006302, 2.86588144, 8.79914856, 4.48911667, 0.323936522, 4.36279202, 4.35846424, 5.64779806, 3.96638012, 4.14878941, 0.321929514, 6.03621054, 6.40116596, 6.832551, 6.74291325, 0.79479748, 0.653127015, 1.25206304, 5.33933783, 7.22644234, 7.33642387, 7.95648623, 8.02824974, 7.08783102, 1.19766355, 1.02813244, 1.32111645, 0.805455506, 0.384461522, 7.13473797, 0.724791527, 0.973604977, 1.09312296, 8.14460182, 0.0, 8.85136604, 0.420830011, 0.136823505, 4.43793011, 0.0, 9.0076313, 6.22471905, 2.055861, 1.99851394, 1.82493901, 2.07956147, 2.41965866, 6.77239323, 7.05098295, 2.96522665, 0.0980380028, 2.94636297, 7.22055435, 1.19412303, 6.36441803, 6.39712858, 3.75719786, 7.30992842, 3.34260941, 1.98615801, 1.77386689, 2.27699542, 9.85142899, 4.45158005, 0.0, 0.0, 9.94759846, 0.740164995, 0.0134679992, 1.791978, 1.04457247, 5.40740156, 1.00201607, 1.46039557, 1.22066545, 4.49379587, 4.58090782, 4.79809761, 5.64818859, 0.204717994, 1.88273001, 5.39322996, 2.1234355, 6.21096802, 0.60630846, 6.89495087, 6.47800732, 6.17862892, 1.29006648, 7.14394474, 1.13045704, 1.45585048, 6.29452419, 6.55481339, 7.09814739, 6.17153454, 2.3628931, 6.94825554, 6.98235226, 0.142716497, 7.47118282, 0.1697945, 3.61702442, 1.28638744, 1.11651754, 8.25354195, 8.15847492, 1.67186451, 1.86476445, 1.71930504, 8.12498951, 7.44730091, 9.91818619, 2.5465951, 0.0, 8.30804062, 0.556015968, 8.94696426, 0.550128996, 0.00324800005, 9.34538078, 9.57115936, 0.0, 8.86266327, 1.17371547, 8.76660156, 2.50532103, 9.99458218, 9.96799278, 0.0, 0.850231051, 4.52522993, 4.79848862, 2.81405306, 0.50831151, 2.61114693, 9.51576233, 1.69435799, 5.96113586, 3.46561289, 4.03108025, 3.50835562, 3.8386631, 3.65044355, 3.66113496, 3.87752891, 3.98411345, 2.90035963, 2.70505667, 6.03336048, 3.14647293, 6.19172096, 6.07381201, 6.18772411, 6.51037788, 6.81304836, 9.99310684, 4.10337496, 0.0, 4.08623457, 9.99966145, 4.13756561, 5.44229031, 4.91191244, 4.40265656, 4.68280458, 4.54167652, 9.91244698, 4.64272642, 4.85032892, 4.90546751, 9.52905464, 5.00735903, 0.781572998, 5.01299572, 5.01049042, 0.0, 9.69812202, 0.0177469999, 0.657628, 0.802487493, 0.0, 5.37922001, 0.0455644988, 9.97216034, 1.57755256, 0.99655503, 0.00372349983, 2.53855085, 0.0, 0.0, 2.85005999, 0.0, 3.38601255, 9.77647781, 3.36991596, 3.38894081, 2.88105154, 0.0, 0.0, 4.24783611, 3.31239462, 3.8286438, 3.1110692, 9.77334309, 2.16178346, 0.0, 0.141616493, 2.00183702, 0.0, 0.0, 2.59188795, 3.99723291, 0.0, 1.09365845, 0.841815472, 3.09970188, 3.59911013, 3.67825842, 4.25621462, 0.115581997, 0.62826252, 4.44921064, 3.6058898, 4.10213995, 0.0, 0.3284235, 4.31203079, 4.2323184, 4.35954475, 4.39506817, 0.946068048, 4.48556805, 1.25586247, 0.072296001, 0.0, 5.71494675, 0.0, 5.78234386, 1.08682346, 9.8398037, 1.74840546, 0.0896950066, 1.37948954, 1.64358544, 1.3467325, 2.08677912, 9.67510986, 0.0, 5.51241255, 4.53667259, 0.0, 2.04992104, 2.70929909, 4.31423759, 2.16972399, 0.0, 5.84971714, 5.96876431, 2.4747653, 5.56085587, 5.96059322, 5.64216805, 2.94099188, 2.81560087, 9.29212856, 3.05342245, 3.34346604, 7.34361839, 3.31776237, 3.37928581, 5.20249844, 5.46574306, 5.16565132, 5.17704391, 2.85614634, 2.890522, 5.9346981, 8.82488537, 4.48363161, 3.99859762, 0.0568595007, 5.19616318, 4.04666328, 4.57526588, 4.35722923, 5.08968067, 2.82158041, 3.78415108, 5.87499714, 0.0, 5.43256474, 1.14955759, 5.8671217, 4.0404377, 0.0775714964, 0.501980484, 0.0209405012, 0.332361996, 0.0, 6.83503056, 6.74121761, 6.78270626, 6.18587685, 0.79945451, 0.644008994, 1.09537649, 1.2507236, 1.33122993, 1.50715899, 6.78397083, 6.99607038, 7.22865105, 2.77713799, 0.551994979, 7.79027605, 0.0209324993, 0.743766487, 0.524739504, 0.820649981, 0.466237992, 7.30822086, 1.41620147, 8.02461815, 7.94441223, 7.79199743, 3.73613048, 8.07065296, 7.01731157, 0.201542497, 0.675814509, 0.64218998, 0.719092011, 8.69988251, 0.728219986, 1.78191257, 8.16463852, 8.7127285, 0.0, 1.29406202, 1.515728, 0.117361501, 0.0757070035, 9.15103912, 9.45365047, 0.0187055003, 0.240283489, 4.39351988, 9.93142509, 0.650069952, 9.85000992, 1.17476344, 1.61022902, 6.52956676, 0.0, 9.77440834, 7.43527794, 1.57408047, 7.09425068, 2.07741117, 6.34398556, 6.83626652, 5.9700222, 2.26004601, 2.14499235, 4.3533287, 6.82322121, 0.573130012, 7.85575247, 0.0, 2.73062301, 2.90508199, 3.04995251, 2.76693702, 0.0, 6.32459545, 6.31119823, 4.93804407, 3.74568796, 0.0, 6.39745569, 3.56637669, 0.0, 5.70980597, 7.34807968, 1.65409708, 7.50645447, 0.681152999, 2.01040602, 1.70236146, 0.0, 2.27151394, 2.98089552, 7.52157307, 7.49526787, 7.77170753, 9.96945953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.54548073, 4.72525692, 4.74137211, 1.12560153, 0.0, 7.56029034, 6.15987682, 1.13995552, 9.87177658, 1.46251655, 1.21377993, 9.62618828, 0.0, 7.0834384, 2.50983953, 4.59440327, 0.0, 4.86678886, 5.51663113, 9.39110279, 0.153674006, 0.531718016, 0.0, 0.0, 0.0, 5.46039438, 2.00354195, 2.15154052, 6.2097187, 0.00314699998, 0.0, 9.94248104, 6.4699831, 9.00070763, 5.541605, 6.65960693, 9.08423519, 3.73875856, 6.58212948, 6.63511848, 0.888131022, 2.59223747, 1.10593748, 1.29143798, 6.26182032, 6.29100895, 8.03438759, 6.30317593, 1.65697742, 1.66783655, 1.70665693, 1.79332352, 6.16765833, 2.31319904, 1.96247947, 6.56771517, 2.06076598, 2.23300052, 6.97976017, 7.25557327, 0.130957499, 3.29638958, 0.630985498, 0.357483983, 2.24311686, 0.238295496, 1.49087501, 8.97047043, 1.27809, 7.66962433, 1.92186904, 7.79656458, 8.10721397, 8.2579813, 1.33966649, 1.267591, 7.34693623, 7.46213245, 1.48485947, 0.0, 9.96628761, 7.92337799, 1.41197848, 1.77158296, 1.88526893, 0.0, 2.53695655, 0.0, 2.54112053, 0.0, 0.210070997, 0.386564493, 8.54479218, 1.34149599, 1.79657698, 0.0126304999, 0.437114, 8.92523956, 0.0, 9.10192013, 1.33450651, 5.29758358, 0.00568050006, 9.57157612, 8.40120029, 0.0, 8.40353584, 0.0, 0.0, 0.0, 8.86222649, 2.51040936, 4.32197762, 2.59139109, 9.26022434, 0.0, 0.722581029, 1.13226104, 0.0, 4.67403221, 2.54980659, 3.14426446, 4.91974211, 5.10433865, 0.315761507, 3.00789356, 2.56393242, 3.05321193, 8.89255524, 2.92582941, 3.09711909, 3.08620048, 5.89016628, 5.97102356, 3.32722712, 3.49845362, 9.58951664, 4.91055775, 8.05513954, 5.42165041, 0.749666989, 5.57146263, 3.64810848, 5.83516979, 0.0, 5.88197565, 5.80706549, 3.48063993, 6.00047016, 4.07110691, 6.03580809, 6.52237892, 6.40263271, 3.46330404, 6.00897884, 6.11024237, 2.97001839, 0.0, 6.13562107, 0.201238006, 0.0, 3.34825468, 3.84772396, 4.02636003, 1.75854206, 6.51179886, 6.5609436, 0.0, 2.68340492, 0.0, 2.952806, 3.83604646, 3.63802743, 4.1057682, 0.0, 0.0, 4.12083244, 4.15673065, 4.58501434, 4.53033161, 4.81723356, 4.13231897, 7.27215052, 4.81425476, 4.65088034, 2.37991047, 4.50812531, 0.0, 4.55398178, 0.0, 4.50451851, 0.0, 4.65657806, 8.53364182, 8.44416809, 4.96720648, 9.50063324, 0.0, 9.86847687, 0.0, 0.0, 5.01474285, 0.0, 0.0, 0.0, 8.14749527, 4.73166847, 0.0160144996, 0.0, 0.0216875002, 5.01979065, 0.670138478, 7.64033508, 0.802636504, 0.0, 0.0, 9.96338081, 8.11244869, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 1: np.array([4.49343729, 6.00253391, 2.54766369, 4.44183922, 1.51696801, 7.26109648, 4.11555147, 3.5985074, 2.79674459, 8.03351212, 7.22332859, 6.16110992, 8.27483845, 6.00265121, 5.01680088, 2.84994793, 3.07493901, 1.98668253, 3.52611494, 6.95812225, 8.71343613, 2.61801004, 7.59564781, 1.79338408, 1.44609499, 1.38297939, 0.562618971, 3.28847599, 6.53277683, 4.68567181, 9.99995995, 0.000230999998, 3.827878, 2.52191401, 3.84765291, 1.34451449, 5.53880548, 5.16515827, 5.20963144, 0.621227026, 0.798491001, 0.735219002, 0.425974011, 2.06265783, 3.06089044, 2.0312705, 0.00342399999, 5.40696144, 5.38802099, 0.745553493, 1.95692801, 0.683198988, 1.86748099, 9.05969715, 8.86323929, 5.26025105, 5.59020042, 3.17023158, 9.30273628, 4.41034508, 5.01011086, 9.94260597, 0.0, 0.0, 9.99748802, 2.85042334, 3.10625887, 3.59874392, 4.22507572, 4.15673637, 4.01289749, 5.65139771, 5.30051041, 4.89049721, 2.28088856, 3.13874912, 5.68536949, 4.03421307, 3.96955919, 6.73806286, 1.248353, 7.64062595, 7.62219143, 0.567203999, 1.09373498, 9.34695625, 1.51583099, 6.42043877, 6.66902351, 6.46104717, 6.36724806, 1.67018056, 7.59555054, 2.06705594, 4.46745014, 0.0244050007, 1.19974053, 4.7966547, 1.97034097, 6.46984768, 6.81758404, 6.34593487, 6.69675064, 7.80259895, 7.92737913, 7.76947403, 7.44841337, 8.69834518, 9.36070442, 1.17671943, 1.02644944, 4.88763237, 3.08194256, 5.1542654, 3.68589854, 2.90913701, 3.34871244, 2.68418503, 9.30384922, 4.77806473, 9.39378166, 4.90970039, 1.82953095, 0.0227585007, 9.94319725, 1.98332345, 0.807707489, 2.85029793, 3.36849499, 3.97099161, 3.83405113, 0.0, 3.78003263, 2.52307081, 4.22617531, 3.29004145, 3.59896708, 4.25649834, 0.522124529, 4.48583412, 1.04598451, 1.74659204, 5.58746529, 2.38363647, 2.70679355, 5.84551954, 9.00734997, 5.00422859, 4.77875328, 0.960305989, 3.18644428, 4.49789667, 4.81099415, 5.65006256, 5.4839654, 6.04062796, 0.17167899, 6.21630096, 6.55621004, 0.459105492, 0.302353501, 0.944546461, 1.14985597, 8.45279694, 8.31921577, 8.17863083, 8.70559311, 0.284805, 0.301471502, 0.00329199992, 0.0, 1.83795452, 6.93801451, 6.56131697, 2.29894543, 6.20991421, 6.66339016, 3.27856398, 7.22108507, 7.44037724, 7.58805466, 9.81595802, 0.0, 0.0, 0.0, 4.46730137, 9.92674637, 1.08395803, 9.88646317, 5.94263077, 5.6494832, 2.27195787, 2.28421354, 9.91385651, 5.80261612, 0.605758488, 0.270085514, 6.416749, 1.02657449, 1.80283999, 1.68352842, 6.17224264, 2.23732448, 0.253251493, 8.01565075, 7.78896952, 0.906191468, 7.49048519, 7.9290638, 2.10620642, 8.27417564, 0.398752987, 0.147648007, 0.222129494, 0.560171485, 0.737236023, 0.00318450015, 9.95735168, 1.028036, 0.141577005, 2.9159236, 7.86435461, 5.84556961, 4.64619923, 3.60628605, 3.65889597, 5.98904228, 6.17265558, 6.46379375, 3.3472929, 6.20921612, 6.81984425, 4.11540222, 0.0, 9.99660206, 4.83556271, 4.14459229, 4.89476156, 4.74167252, 4.65855646, 9.53120995, 8.82756615, 5.01657963, 9.64037132, 0.706618547, 6.72679138, 9.96512794, 1.98265505, 2.53786707, 0.0, 9.99803352, 6.77105522, 0.0, 3.38698149, 3.31458759, 2.8836875, 3.97142053, 3.38378, 3.40778756, 3.77992749, 0.149246007, 3.90486503, 3.97788715, 0.0, 2.5343194, 3.80132961, 3.59959936, 3.12141514, 4.35834837, 3.746665, 3.60983133, 0.140523002, 0.86195147, 0.632369518, 1.25609708, 5.65188408, 1.04788208, 4.47184086, 5.05264091, 1.64018047, 1.34703994, 9.98088264, 4.4466815, 4.89132214, 5.23995972, 2.27763605, 6.89152718, 5.55940294, 9.31015205, 0.0740565062, 3.03136158, 3.37488747, 3.33026147, 5.28979874, 5.17006302, 2.86588144, 8.79914856, 4.48911667, 0.323936522, 4.36279202, 4.35846424, 5.64779806, 3.96638012, 4.14878941, 0.321929514, 6.03621054, 6.40116596, 6.832551, 6.74291325, 0.79479748, 0.653127015, 1.25206304, 5.33933783, 7.22644234, 7.33642387, 7.95648623, 8.02824974, 7.08783102, 1.19766355, 1.02813244, 1.32111645, 0.805455506, 0.384461522, 7.13473797, 0.724791527, 0.973604977, 1.09312296, 8.14460182, 0.0, 8.85136604, 0.420830011, 0.136823505, 4.43793011, 0.0, 9.0076313, 6.22471905, 2.055861, 1.99851394, 1.82493901, 2.07956147, 2.41965866, 6.77239323, 7.05098295, 2.96522665, 0.0980380028, 2.94636297, 7.22055435, 1.19412303, 6.36441803, 6.39712858, 3.75719786, 7.30992842, 3.34260941, 1.98615801, 1.77386689, 2.27699542, 9.85142899, 4.45158005, 0.0, 0.0, 9.94759846, 0.740164995, 0.0134679992, 1.791978, 1.04457247, 5.40740156, 1.00201607, 1.46039557, 1.22066545, 4.49379587, 4.58090782, 4.79809761, 5.64818859, 0.204717994, 1.88273001, 5.39322996, 2.1234355, 6.21096802, 0.60630846, 6.89495087, 6.47800732, 6.17862892, 1.29006648, 7.14394474, 1.13045704, 1.45585048, 6.29452419, 6.55481339, 7.09814739, 6.17153454, 2.3628931, 6.94825554, 6.98235226, 0.142716497, 7.47118282, 0.1697945, 3.61702442, 1.28638744, 1.11651754, 8.25354195, 8.15847492, 1.67186451, 1.86476445, 1.71930504, 8.12498951, 7.44730091, 9.91818619, 2.5465951, 0.0, 8.30804062, 0.556015968, 8.94696426, 0.550128996, 0.00324800005, 9.34538078, 9.57115936, 0.0, 8.86266327, 1.17371547, 8.76660156, 2.50532103, 9.99458218, 9.96799278, 0.0, 0.850231051, 4.52522993, 4.79848862, 2.81405306, 0.50831151, 2.61114693, 9.51576233, 1.69435799, 5.96113586, 3.46561289, 4.03108025, 3.50835562, 3.8386631, 3.65044355, 3.66113496, 3.87752891, 3.98411345, 2.90035963, 2.70505667, 6.03336048, 3.14647293, 6.19172096, 6.07381201, 6.18772411, 6.51037788, 6.81304836, 9.99310684, 4.10337496, 0.0, 4.08623457, 9.99966145, 4.13756561, 5.44229031, 4.91191244, 4.40265656, 4.68280458, 4.54167652, 9.91244698, 4.64272642, 4.85032892, 4.90546751, 9.52905464, 5.00735903, 0.781572998, 5.01299572, 5.01049042, 0.0, 9.69812202, 0.0177469999, 0.657628, 0.802487493, 0.0, 5.37922001, 0.0455644988, 9.97216034, 1.57755256, 0.99655503, 0.00372349983, 2.53855085, 0.0, 0.0, 2.85005999, 0.0, 3.38601255, 9.77647781, 3.36991596, 3.38894081, 2.88105154, 0.0, 0.0, 4.24783611, 3.31239462, 3.8286438, 3.1110692, 9.77334309, 2.16178346, 0.0, 0.141616493, 2.00183702, 0.0, 0.0, 2.59188795, 3.99723291, 0.0, 1.09365845, 0.841815472, 3.09970188, 3.59911013, 3.67825842, 4.25621462, 0.115581997, 0.62826252, 4.44921064, 3.6058898, 4.10213995, 0.0, 0.3284235, 4.31203079, 4.2323184, 4.35954475, 4.39506817, 0.946068048, 4.48556805, 1.25586247, 0.072296001, 0.0, 5.71494675, 0.0, 5.78234386, 1.08682346, 9.8398037, 1.74840546, 0.0896950066, 1.37948954, 1.64358544, 1.3467325, 2.08677912, 9.67510986, 0.0, 5.51241255, 4.53667259, 0.0, 2.04992104, 2.70929909, 4.31423759, 2.16972399, 0.0, 5.84971714, 5.96876431, 2.4747653, 5.56085587, 5.96059322, 5.64216805, 2.94099188, 2.81560087, 9.29212856, 3.05342245, 3.34346604, 7.34361839, 3.31776237, 3.37928581, 5.20249844, 5.46574306, 5.16565132, 5.17704391, 2.85614634, 2.890522, 5.9346981, 8.82488537, 4.48363161, 3.99859762, 0.0568595007, 5.19616318, 4.04666328, 4.57526588, 4.35722923, 5.08968067, 2.82158041, 3.78415108, 5.87499714, 0.0, 5.43256474, 1.14955759, 5.8671217, 4.0404377, 0.0775714964, 0.501980484, 0.0209405012, 0.332361996, 0.0, 6.83503056, 6.74121761, 6.78270626, 6.18587685, 0.79945451, 0.644008994, 1.09537649, 1.2507236, 1.33122993, 1.50715899, 6.78397083, 6.99607038, 7.22865105, 2.77713799, 0.551994979, 7.79027605, 0.0209324993, 0.743766487, 0.524739504, 0.820649981, 0.466237992, 7.30822086, 1.41620147, 8.02461815, 7.94441223, 7.79199743, 3.73613048, 8.07065296, 7.01731157, 0.201542497, 0.675814509, 0.64218998, 0.719092011, 8.69988251, 0.728219986, 1.78191257, 8.16463852, 8.7127285, 0.0, 1.29406202, 1.515728, 0.117361501, 0.0757070035, 9.15103912, 9.45365047, 0.0187055003, 0.240283489, 4.39351988, 9.93142509, 0.650069952, 9.85000992, 1.17476344, 1.61022902, 6.52956676, 0.0, 9.77440834, 7.43527794, 1.57408047, 7.09425068, 2.07741117, 6.34398556, 6.83626652, 5.9700222, 2.26004601, 2.14499235, 4.3533287, 6.82322121, 0.573130012, 7.85575247, 0.0, 2.73062301, 2.90508199, 3.04995251, 2.76693702, 0.0, 6.32459545, 6.31119823, 4.93804407, 3.74568796, 0.0, 6.39745569, 3.56637669, 0.0, 5.70980597, 7.34807968, 1.65409708, 7.50645447, 0.681152999, 2.01040602, 1.70236146, 0.0, 2.27151394, 2.98089552, 7.52157307, 7.49526787, 7.77170753, 9.96945953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.54548073, 4.72525692, 4.74137211, 1.12560153, 0.0, 7.56029034, 6.15987682, 1.13995552, 9.87177658, 1.46251655, 1.21377993, 9.62618828, 0.0, 7.0834384, 2.50983953, 4.59440327, 0.0, 4.86678886, 5.51663113, 9.39110279, 0.153674006, 0.531718016, 0.0, 0.0, 0.0, 5.46039438, 2.00354195, 2.15154052, 6.2097187, 0.00314699998, 0.0, 9.94248104, 6.4699831, 9.00070763, 5.541605, 6.65960693, 9.08423519, 3.73875856, 6.58212948, 6.63511848, 0.888131022, 2.59223747, 1.10593748, 1.29143798, 6.26182032, 6.29100895, 8.03438759, 6.30317593, 1.65697742, 1.66783655, 1.70665693, 1.79332352, 6.16765833, 2.31319904, 1.96247947, 6.56771517, 2.06076598, 2.23300052, 6.97976017, 7.25557327, 0.130957499, 3.29638958, 0.630985498, 0.357483983, 2.24311686, 0.238295496, 1.49087501, 8.97047043, 1.27809, 7.66962433, 1.92186904, 7.79656458, 8.10721397, 8.2579813, 1.33966649, 1.267591, 7.34693623, 7.46213245, 1.48485947, 0.0, 9.96628761, 7.92337799, 1.41197848, 1.77158296, 1.88526893, 0.0, 2.53695655, 0.0, 2.54112053, 0.0, 0.210070997, 0.386564493, 8.54479218, 1.34149599, 1.79657698, 0.0126304999, 0.437114, 8.92523956, 0.0, 9.10192013, 1.33450651, 5.29758358, 0.00568050006, 9.57157612, 8.40120029, 0.0, 8.40353584, 0.0, 0.0, 0.0, 8.86222649, 2.51040936, 4.32197762, 2.59139109, 9.26022434, 0.0, 0.722581029, 1.13226104, 0.0, 4.67403221, 2.54980659, 3.14426446, 4.91974211, 5.10433865, 0.315761507, 3.00789356, 2.56393242, 3.05321193, 8.89255524, 2.92582941, 3.09711909, 3.08620048, 5.89016628, 5.97102356, 3.32722712, 3.49845362, 9.58951664, 4.91055775, 8.05513954, 5.42165041, 0.749666989, 5.57146263, 3.64810848, 5.83516979, 0.0, 5.88197565, 5.80706549, 3.48063993, 6.00047016, 4.07110691, 6.03580809, 6.52237892, 6.40263271, 3.46330404, 6.00897884, 6.11024237, 2.97001839, 0.0, 6.13562107, 0.201238006, 0.0, 3.34825468, 3.84772396, 4.02636003, 1.75854206, 6.51179886, 6.5609436, 0.0, 2.68340492, 0.0, 2.952806, 3.83604646, 3.63802743, 4.1057682, 0.0, 0.0, 4.12083244, 4.15673065, 4.58501434, 4.53033161, 4.81723356, 4.13231897, 7.27215052, 4.81425476, 4.65088034, 2.37991047, 4.50812531, 0.0, 4.55398178, 0.0, 4.50451851, 0.0, 4.65657806, 8.53364182, 8.44416809, 4.96720648, 9.50063324, 0.0, 9.86847687, 0.0, 0.0, 5.01474285, 0.0, 0.0, 0.0, 8.14749527, 4.73166847, 0.0160144996, 0.0, 0.0216875002, 5.01979065, 0.670138478, 7.64033508, 0.802636504, 0.0, 0.0, 9.96338081, 8.11244869, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])}


class PredictorError(Exception):

    def __init__(self, msg, code):
        self.msg = msg
        self.code = code

    def __str__(self):
        return self.msg

def __convert(cell):
    value = str(cell)
    try:
        result = int(value)
        return result
    except ValueError:
        try:
            result = float(value)
            if math.isnan(result):
                raise PredictorError('NaN value found. Aborting.', code=1)
            return result
        except ValueError:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            return result
        except Exception as e:
            raise e


def __get_key(val, dictionary):
    if dictionary == {}:
        return val
    for key, value in dictionary.items():
        if val == value:
            return key
    if val not in dictionary.values():
        raise PredictorError(f"Label {val} key does not exist", code=2)


def __confusion_matrix(y_true, y_pred, json):
    stats = {}
    labels = np.array(list(mapping.keys()))
    sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
    for class_i in range(n_classes):
        class_i_label = __get_key(class_i, mapping)
        stats[int(class_i)] = {}
        class_i_indices = np.argwhere(y_true == class_i_label)
        not_class_i_indices = np.argwhere(y_true != class_i_label)
        # None represents N/A in this case
        stats[int(class_i)]['TP'] = TP = int(np.sum(y_pred[class_i_indices] == class_i_label)) if class_i_indices.size > 0 else None
        stats[int(class_i)]['FN'] = FN = int(np.sum(y_pred[class_i_indices] != class_i_label)) if class_i_indices.size > 0 else None
        stats[int(class_i)]['TN'] = TN = int(np.sum(y_pred[not_class_i_indices] != class_i_label)) if not_class_i_indices.size > 0 else None
        stats[int(class_i)]['FP'] = FP = int(np.sum(y_pred[not_class_i_indices] == class_i_label)) if not_class_i_indices.size > 0 else None
        if TP is None or FN is None or (TP + FN == 0):
            stats[int(class_i)]['TPR'] = None
        else:
            stats[int(class_i)]['TPR'] = (TP / (TP + FN))
        if TN is None or FP is None or (TN + FP == 0):
            stats[int(class_i)]['TNR'] = None
        else:
            stats[int(class_i)]['TNR'] = (TN / (TN + FP))
        if TP is None or FP is None or (TP + FP == 0):
            stats[int(class_i)]['PPV'] = None
        else:
            stats[int(class_i)]['PPV'] = (TP / (TP + FP))
        if TN is None or FN is None or (TN + FN == 0):
            stats[int(class_i)]['NPV'] = None
        else:
            stats[int(class_i)]['NPV'] = (TN / (TN + FN))
        if TP is None or FP is None or FN is None or (TP + FP + FN == 0):
            stats[int(class_i)]['F1'] = None
        else:
            stats[int(class_i)]['F1'] = ((2 * TP) / (2 * TP + FP + FN))
        if TP is None or FP is None or FN is None or (TP + FP + FN == 0):
            stats[int(class_i)]['TS'] = None
        else:
            stats[int(class_i)]['TS'] = (TP / (TP + FP + FN))

    if not report_cmat:
        return np.array([]), stats

    label_to_ind = {label: i for i, label in enumerate(labels)}
    y_pred = np.array([label_to_ind.get(x, n_classes + 1) for x in y_pred])
    y_true = np.array([label_to_ind.get(x, n_classes + 1) for x in y_true])

    ind = np.logical_and(y_pred < n_classes, y_true < n_classes)
    y_pred = y_pred[ind]
    y_true = y_true[ind]
    sample_weight = sample_weight[ind]

    cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_classes, n_classes), dtype=np.int64).toarray()
    with np.errstate(all='ignore'):
        cm = np.nan_to_num(cm)

    return cm, stats


def __preprocess_and_clean_in_memory(arr):
    clean_arr = np.zeros((len(arr), len(important_idxs)))
    for i, row in enumerate(arr):
        try:
            row_used_cols_only = [row[i] for i in important_idxs]
        except IndexError:
            error_str = f"The input has shape ({len(arr)}, {len(row)}) but the expected shape is (*, {len(ignorecolumns) + len(important_idxs)})."
            if len(arr) == num_attr and len(arr[0]) != num_attr:
                error_str += "\n\nNote: You may have passed an input directly to 'preprocess_and_clean_in_memory' or 'predict_in_memory' "
                error_str += "rather than as an element of a list. Make sure that even single instances "
                error_str += "are enclosed in a list. Example: predict_in_memory(0) is invalid but "
                error_str += "predict_in_memory([0]) is valid."
            raise PredictorError(error_str, 3)
        clean_arr[i] = [float(__convert(field)) for field in row_used_cols_only]
    return clean_arr


def __evaluate_tree(xs, split_vals, split_feats, right_children, logits):
    if xs is None:
        xs = np.frombuffer(var_dict['X']).reshape(var_dict['X_shape'])

    current_node_per_row = np.zeros(xs.shape[0]).astype('int')
    values = np.empty(xs.shape[0])
    values.fill(np.nan)

    while np.isnan(values).any():

        row_idxs_at_leaf = np.argwhere(np.logical_and(right_children[current_node_per_row] == -1, np.isnan(values))).reshape(-1)
        row_idxs_at_branch = np.argwhere(right_children[current_node_per_row] != -1).reshape(-1)

        if row_idxs_at_leaf.shape[0] > 0:

            values[row_idxs_at_leaf] = logits[current_node_per_row[row_idxs_at_leaf]].reshape(-1)
            current_node_per_row[row_idxs_at_leaf] = -1

        if row_idxs_at_branch.shape[0] > 0:

            split_values_per_row = split_vals[current_node_per_row[row_idxs_at_branch]].astype('float64')
            split_features_per_row = split_feats[current_node_per_row[row_idxs_at_branch]].astype('int')
            feature_val_per_row = xs[row_idxs_at_branch, split_features_per_row].reshape(-1)

            branch_nodes = current_node_per_row[row_idxs_at_branch]
            current_node_per_row[row_idxs_at_branch] = np.where(feature_val_per_row < split_values_per_row,
                                                                right_children[branch_nodes].astype('int'),
                                                                (right_children[branch_nodes] + 1).astype('int'))

    return values


def __build_logit_func(n_trees, clss):

    def __logit_func(xs, serial, data_shape, pool=None):
        if serial:
            sum_of_leaf_values = np.zeros(xs.shape[0])
            for booster_index in range(clss, n_trees, n_classes):
                sum_of_leaf_values += __evaluate_tree(
                    xs, split_vals_dict[booster_index], split_feats_dict[booster_index],
                    right_children_dict[booster_index], logits_dict[booster_index])
        else:
            sum_of_leaf_values = np.sum(
                list(pool.starmap(__evaluate_tree,
                                  [(None, split_vals_dict[booster_index], split_feats_dict[booster_index],
                                    right_children_dict[booster_index], logits_dict[booster_index])
                                   for booster_index in range(clss, n_trees, n_classes)])), axis=0)
        return sum_of_leaf_values

    return __logit_func


def __init_worker(X, X_shape):
    var_dict['X'] = X
    var_dict['X_shape'] = X_shape

def __classify(rows, return_probabilities=False, force_serial=False):
    if force_serial:
        serial = True
    else:
        serial = default_to_serial
    if isinstance(rows, list):
        rows = np.array(rows)

    logits = [__build_logit_func(2, clss) for clss in range(n_classes)]

    if serial:
        o = np.array([logits[class_index](rows, True, rows.shape) for class_index in range(n_classes)]).T
    else:
        shared_arr = multiprocessing.RawArray('d', rows.shape[0] * rows.shape[1])
        shared_arr_np = np.frombuffer(shared_arr, dtype=rows.dtype).reshape(rows.shape)
        np.copyto(shared_arr_np, rows)

        procs = multiprocessing.cpu_count()
        pool = multiprocessing.Pool(processes=procs, initializer=__init_worker, initargs=(shared_arr, rows.shape))
        o = np.array([logits[class_index](None, False, rows.shape, pool) for class_index in range(n_classes)]).T

    if return_probabilities:

        argument = o[:, 0] - o[:, 1]
        p0 = 1.0 / (1.0 + np.exp(-argument)).reshape(-1, 1)
        p1 = 1.0 - p0
        output = np.concatenate((p0, p1), axis=1)

    else:
        output = np.argmax(o, axis=1)
    return output



def __validate_kwargs(kwargs):
    for key in kwargs:

        if key not in ['return_probabilities', 'force_serial']:
            raise PredictorError(f'{key} is not a keyword argument for Brainome\'s {classifier_type} predictor. Please see the documentation.', 4)


def __validate_data(row_or_arr, validate, row_num=None):
    if validate:
        expected_columns = len(important_idxs) + len(ignore_idxs) + 1
    else:
        expected_columns = len(important_idxs) + len(ignore_idxs)

    input_is_array = isinstance(row_or_arr, np.ndarray)
    n_cols = row_or_arr.shape[1] if input_is_array else len(row_or_arr)

    if n_cols != expected_columns:

        if row_num is None:
            err_str = f"Your data contains {n_cols} columns but {expected_columns} are required."
        else:
            err_str = f"At row {row_num}, your data contains {n_cols} columns but {expected_columns} are required."

        if validate:
            err_str += " The predictor's validate() method works on data that has the same columns in the same order as were present in the training CSV."
            err_str += " This includes the target column and features that are not used by the model but existed in the training CSV."
            if n_cols == 1 + len(important_idxs):
                err_str += f" We suggest confirming that the {len(ignore_idxs)} unused features are present in the data."
            elif n_cols == len(important_idxs):
                err_str += f" We suggest confirming that the {len(ignore_idxs)} unused features are present in the data as well as the target column. "
            elif n_cols == len(important_idxs) + len(ignore_idxs):
                err_str += " We suggest confirming that the target column present in the data. "
            err_str += " To make predictions, see the predictor's predict() method."
        else:
            err_str += " The predictor's predict() method works on data that has the same feature columns in the same relative order as were present in the training CSV."
            err_str += " This DOES NOT include the target column but DOES include features that are not used by the model but existed in the training CSV."
            if n_cols == 1 + len(important_idxs):
                err_str += f" We suggest confirming that the {len(ignore_idxs)} unused features are present in the data and that the target column is not present."
            elif n_cols == len(important_idxs):
                err_str += f" We suggest confirming that the {len(ignore_idxs)} unused features are present in the data."
            elif n_cols == 1 + len(important_idxs) + len(ignore_idxs):
                err_str += " We suggest confirming that the target column is not present."
            err_str += " To receive a performance summary, instead of make predictions, see the predictor's validate() method."

        raise PredictorError(err_str, 5)

    else:

        if not input_is_array:
            return row_or_arr


def __write_predictions(arr, header, headerless, trim, outfile=None):
    predictions = predict(arr)

    if not headerless:
        if trim:
            header = ','.join([x for i, x in enumerate(header) if i in important_idxs] + ['Prediction'])
        else:
            header = ','.join(header.tolist() + ['Prediction'])
        if outfile is None:
            print(header)
        else:
            print(header, file=outfile)

    for row, prediction in zip(arr, predictions):
        if trim:
            row = ['"' + field + '"' if ',' in field else field for i, field in enumerate(row) if i in important_idxs]
        else:
            row = ['"' + field + '"' if ',' in field else field for field in row]
        row.append(prediction)
        if outfile is None:
            print(','.join(row))
        else:
            print(','.join(row), file=outfile)


def load_data(csvfile, headerless, validate):
    """
    Parameters
    ----------
    csvfile : str
        The path to the CSV file containing the data.

    headerless : bool
        True if the CSV does not contain a header.

    validate : bool
        True if the data should be loaded to be used by the predictor's validate() method.
        False if the data should be loaded to be used by the predictor's predict() method.

    Returns
    -------
    arr : np.ndarray
        The data (observations and labels) found in the CSV without any header.

    data : np.ndarray or NoneType
        None if validate is False, otherwise the observations (data without the labels) found in the CSV.

    labels : np.ndarray or NoneType
        None if the validate is False, otherwise the labels found in the CSV.

    header : np.ndarray or NoneType
        None if the CSV is headerless, otherwise the header.
    """

    with open(csvfile, 'r', encoding='utf-8') as csvinput:
        arr = np.array([__validate_data(row, validate, row_num=i) for i, row in enumerate(csv.reader(csvinput)) if row != []], dtype=str)
    if headerless:
        header = None
    else:
        header = arr[0]
        arr = arr[1:]
    if validate:
        labels = arr[:, target_column]
        feature_columns = [i for i in range(arr.shape[1]) if i != target_column]
        data = arr[:, feature_columns]
    else:
        data, labels = None, None

    if validate and ignorelabels != []:
        idxs_to_keep = np.argwhere(np.logical_not(np.isin(labels, ignorelabels))).reshape(-1)
        labels = labels[idxs_to_keep]
        data = data[idxs_to_keep]

    return arr, data, labels, header


def predict(arr, remap=True, **kwargs):
    """
    Parameters
    ----------
    arr : list[list]
        An array of inputs to be cleaned by 'preprocess_and_clean_in_memory'. This
        should contain all the features that were present in the training data,
        regardless of whether or not they are used by the model, with the same
        relative order as in the training data. There should be no target column.


    remap : bool
        If True and 'return_probs' is False, remaps the output to the original class
        label. If 'return_probs' is True this instead adds a header indicating which
        original class label each column of output corresponds to.

    **kwargs :
        return_probabilities : bool
            If true, return class membership probabilities instead of classifications.

    **kwargs :
        force_serial : bool
            If true, model inference is done in serial rather than in parallel. This is
            useful if calling "predict" repeatedly inside a for-loop.

    Returns
    -------
    output : np.ndarray

        A numpy array of
            1. Class predictions if 'return_probabilities' is False.
            2. Class probabilities if 'return_probabilities' is True.

    """
    if not isinstance(arr, np.ndarray) and not isinstance(arr, list):
        raise PredictorError(f'Data must be provided to \'predict\' and \'validate\' as a list or np.ndarray, but an input of type {type(arr).__name__} was found.', 6)
    if isinstance(arr, list):
        arr = np.array(arr, dtype=str)

    kwargs = kwargs or {}
    __validate_kwargs(kwargs)
    __validate_data(arr, False)
    remove_bad_chars = lambda x: str(x).replace('"', '').replace(',', '').replace('(', '').replace(')', '').replace("'", '')
    arr = [[remove_bad_chars(field) for field in row] for row in arr]
    arr = __preprocess_and_clean_in_memory(arr)

    output = __classify(arr, **kwargs)

    if remap:
        if kwargs.get('return_probabilities'):
            header = np.array([__get_key(i, mapping) for i in range(output.shape[1])], dtype=str).reshape(1, -1)
            output = np.concatenate((header, output), axis=0)
        else:
            output = np.array([__get_key(prediction, mapping) for prediction in output])

    return output


def validate(arr, labels):
    """
    Parameters
    ----------
    cleanarr : np.ndarray
        An array of float values that has undergone each pre-
        prediction step.

    Returns
    -------
    count : int
        A count of the number of instances in cleanarr.

    correct_count : int
        A count of the number of correctly classified instances in
        cleanarr.

    numeachclass : dict
        A dictionary mapping each class to its number of instances.

    outputs : np.ndarray
        The output of the predictor's '__classify' method on cleanarr.
    """
    predictions = predict(arr)
    correct_count = int(np.sum(predictions.reshape(-1) == labels.reshape(-1)))
    count = predictions.shape[0]
    
    class_0, class_1 = __get_key(0, mapping), __get_key(1, mapping)
    num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0
    num_TP = int(np.sum(np.logical_and(predictions.reshape(-1) == class_1, labels.reshape(-1) == class_1)))
    num_TN = int(np.sum(np.logical_and(predictions.reshape(-1) == class_0, labels.reshape(-1) == class_0)))
    num_FN = int(np.sum(np.logical_and(predictions.reshape(-1) == class_0, labels.reshape(-1) == class_1)))
    num_FP = int(np.sum(np.logical_and(predictions.reshape(-1) == class_1, labels.reshape(-1) == class_0)))
    num_class_0 = int(np.sum(labels.reshape(-1) == class_0))
    num_class_1 = int(np.sum(labels.reshape(-1) == class_1))
    return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, predictions


def __main():
    parser = argparse.ArgumentParser(description='Predictor trained on ' + str(TRAINFILE))
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    parser.add_argument('-json', action="store_true", default=False, help="report measurements as json")
    parser.add_argument('-trim', action="store_true", help="If true, the prediction will not output ignored columns.")
    args = parser.parse_args()
    faulthandler.enable()

    arr, data, labels, header = load_data(csvfile=args.csvfile, headerless=args.headerless, validate=args.validate)

    if not args.validate:
        __write_predictions(arr, header, args.headerless, args.trim)
    else:

        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds = validate(data, labels)

        classcounts = np.bincount(np.array([mapping[label] for label in labels], dtype='int32')).reshape(-1)
        class_balance = (classcounts[np.argwhere(classcounts > 0)] / arr.shape[0]).reshape(-1).tolist()
        best_guess = round(100.0 * np.max(class_balance), 2)
        H = float(-1.0 * sum([class_balance[i] * math.log(class_balance[i]) / math.log(2) for i in range(len(class_balance))]))
        modelacc = int(float(correct_count * 10000) / count) / 100.0
        mtrx, stats = __confusion_matrix(np.array(labels).reshape(-1), np.array(preds).reshape(-1), args.json)

        if args.json:
            json_dict = {'instance_count': count,
                         'classifier_type': classifier_type,
                         'classes': n_classes,
                         'number_correct': correct_count,
                         'accuracy': {
                             'best_guess': (best_guess/100),
                             'improvement': (modelacc - best_guess)/100,
                              'model_accuracy': (modelacc/100),
                         },
                         'model_capacity': model_cap,
                         'generalization_ratio': int(float(correct_count * 100) / model_cap) / 100.0 * H,
                         'model_efficiency': int(100 * (modelacc - best_guess) / model_cap) / 100.0,
                         'shannon_entropy_of_labels': H,
                         'class_balance': class_balance,
                         'confusion_matrix': mtrx.tolist(),
                         'multiclass_stats': stats}

            print(json.dumps(json_dict))
        else:
            pad = lambda s, length, pad_right: str(s) + ' ' * max(0, length - len(str(s))) if pad_right else ' ' * max(0, length - len(str(s))) + str(s)
            labels = np.array(list(mapping.keys())).reshape(-1, 1)
            max_class_name_len = max([len(clss) for clss in mapping.keys()] + [7])

            max_TP_len = max([len(str(stats[key]['TP'])) for key in stats.keys()] + [2])
            max_FP_len = max([len(str(stats[key]['FP'])) for key in stats.keys()] + [2])
            max_TN_len = max([len(str(stats[key]['TN'])) for key in stats.keys()] + [2])
            max_FN_len = max([len(str(stats[key]['FN'])) for key in stats.keys()] + [2])

            cmat_template_1 = "    {} | {}"
            cmat_template_2 = "    {} | " + " {} " * n_classes
            acc_by_class_template_1 = "    {} | {}  {}  {}  {}  {}  {}  {}  {}  {}  {}"

            acc_by_class_lengths = [max_class_name_len, max_TP_len, max_FP_len, max_TN_len, max_FN_len, 7, 7, 7, 7, 7, 7]
            acc_by_class_header_fields = ['target', 'TP', 'FP', 'TN', 'FN', 'TPR', 'TNR', 'PPV', 'NPV', 'F1', 'TS']
            print("Classifier Type:                    Random Forest")

            print(f"System Type:                        {n_classes}-way classifier\n")

            print("Accuracy:")
            print("    Best-guess accuracy:            {:.2f}%".format(best_guess))
            print("    Model accuracy:                 {:.2f}%".format(modelacc) + " (" + str(int(correct_count)) + "/" + str(count) + " correct)")
            print("    Improvement over best guess:    {:.2f}%".format(modelacc - best_guess) + " (of possible " + str(round(100 - best_guess, 2)) + "%)\n")

            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(correct_count * 100) / model_cap) / 100.0 * H) + " bits/bit")

            if report_cmat:
                max_cmat_entry_len = len(str(int(np.max(mtrx))))
                mtrx = np.concatenate((labels, mtrx.astype('str')), axis=1).astype('str')
                max_pred_len = (mtrx.shape[1] - 1) * max_cmat_entry_len + n_classes * 2 - 1
                print("\nConfusion Matrix:\n")
                print(cmat_template_1.format(pad("Actual", max_class_name_len, False), "Predicted"))
                print(cmat_template_1.format("-" * max_class_name_len, "-" * max(max_pred_len, 9)))
                for row in mtrx:
                    print(cmat_template_2.format(
                        *[pad(field, max_class_name_len if i == 0 else max_cmat_entry_len, False) for i, field in enumerate(row)]))

            print("\nAccuracy by Class:\n")
            print(acc_by_class_template_1.format(
                *[pad(header_field, length, False) for i, (header_field, length) in enumerate(zip(acc_by_class_header_fields, acc_by_class_lengths))]))
            print(acc_by_class_template_1.format(
                *["-" * length for length in acc_by_class_lengths]))

            pct_format_string = "{:8.2%}"      # width = 8, decimals = 2
            for raw_class in mapping.keys():
                class_stats = stats[int(mapping[raw_class])]
                TP, FP, TN, FN = class_stats.get('TP', None), class_stats.get('FP', None), class_stats.get('TN', None), class_stats.get('FN', None)
                TPR = pct_format_string.format(class_stats['TPR']) if class_stats['TPR'] is not None else 'N/A'
                TNR = pct_format_string.format(class_stats['TNR']) if class_stats['TNR'] is not None else 'N/A'
                PPV = pct_format_string.format(class_stats['PPV']) if class_stats['PPV'] is not None else 'N/A'
                NPV = pct_format_string.format(class_stats['NPV']) if class_stats['NPV'] is not None else 'N/A'
                F1 = pct_format_string.format(class_stats['F1']) if class_stats['F1'] is not None else 'N/A'
                TS = pct_format_string.format(class_stats['TS']) if class_stats['TS'] is not None else 'N/A'
                line_fields = [raw_class, TP, FP, TN, FN, TPR, TNR, PPV, NPV, F1, TS]
                print(acc_by_class_template_1.format(
                    *[pad(field, length, False) for i, (field, length) in enumerate(zip(line_fields, acc_by_class_lengths))]))


if __name__ == "__main__":
    try:
        __main()
    except PredictorError as e:
        print(e, file=sys.stderr)
        sys.exit(e.code)
    except Exception as e:
        print(f"An unknown exception of type {type(e).__name__} occurred.", file=sys.stderr)
        sys.exit(-1)
