#!/usr/bin/env python3
#
# This code has been produced by an enterprise version of Brainome(tm) licensed to: andy Stevko.
# Portions of this code copyright (c) 2019-2022 by Brainome, Inc. All Rights Reserved.
# Distribution and use of this code or commercial use is permitted within the license terms
# set forth in a written contractual agreement between Brainome, Inc and brainome-user.
# Please contact support@brainome.ai with any questions.
# Use of predictions results at your own risk.
#
# Output of Brainome v1.8-120-prod.
# Invocation: brainome TRAIN_TEST_SPLITS/fri_c0_1000_50-clean-train.csv -f RF -y -split 70 -modelonly -q -o btc-runs/RF/fri_c0_1000_50.py -json btc-runs/RF/fri_c0_1000_50.json
# Total compiler execution time: 0:00:06.20. Finished on: Feb-26-2022 18:29:16.
# This source code requires Python 3.
#
"""

[01;1mPredictor:[0m                        btc-runs/RF/fri_c0_1000_50.py
    Classifier Type:              Random Forest
    System Type:                  Binary classifier
    Training / Validation Split:  70% : 30%
    Accuracy:
      Best-guess accuracy:        51.00%
      Training accuracy:         100.00% (489/489 correct)
      Validation Accuracy:        88.62% (187/211 correct)
      Combined Model Accuracy:    96.57% (676/700 correct)


    Model Capacity (MEC):         63    bits
    Generalization Ratio:          7.76 bits/bit
    Percent of Data Memorized:    25.78%
    Resilience to Noise:          -0.89 dB







    Training Confusion Matrix:
              Actual | Predicted
              ------ | ---------
                   1 |  240    0 
                   0 |    0  249 

    Validation Confusion Matrix:
              Actual | Predicted
              ------ | ---------
                   1 |   90   13 
                   0 |   11   97 

    Training Accuracy by Class:
         binaryClass |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS 
         ----------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------
                   1 |  240    0  249    0  100.00%  100.00%  100.00%  100.00%  100.00%  100.00%
                   0 |  249    0  240    0  100.00%  100.00%  100.00%  100.00%  100.00%  100.00%

    Validation Accuracy by Class:
         binaryClass |   TP   FP   TN   FN     TPR      TNR      PPV      NPV       F1       TS 
         ----------- | ---- ---- ---- ---- -------- -------- -------- -------- -------- --------
                   1 |   90   11   97   13   87.38%   89.81%   89.11%   88.18%   88.24%   78.95%
                   0 |   97   13   90   11   89.81%   87.38%   88.18%   89.11%   88.99%   80.17%


    Attribute Ranking:
                                      Feature | Relative Importance
                                          oz2 :   0.0982
                                          oz1 :   0.0805
                                          oz4 :   0.0769
                                         oz32 :   0.0766
                                         oz19 :   0.0600
                                         oz13 :   0.0459
                                          oz5 :   0.0421
                                         oz46 :   0.0370
                                         oz33 :   0.0331
                                          oz3 :   0.0307
                                         oz31 :   0.0271
                                         oz45 :   0.0247
                                         oz36 :   0.0238
                                          oz6 :   0.0229
                                         oz25 :   0.0222
                                         oz10 :   0.0221
                                         oz40 :   0.0215
                                         oz43 :   0.0210
                                         oz27 :   0.0193
                                         oz49 :   0.0173
                                         oz21 :   0.0169
                                         oz14 :   0.0134
                                         oz28 :   0.0129
                                         oz23 :   0.0127
                                         oz24 :   0.0116
                                         oz50 :   0.0115
                                         oz17 :   0.0113
                                          oz9 :   0.0108
                                         oz42 :   0.0107
                                         oz11 :   0.0088
                                         oz22 :   0.0075
                                          oz8 :   0.0074
                                         oz37 :   0.0071
                                         oz39 :   0.0070
                                         oz34 :   0.0067
                                         oz44 :   0.0063
                                         oz30 :   0.0057
                                         oz12 :   0.0049
                                         oz47 :   0.0042
                                         oz29 :   0.0042
                                         oz35 :   0.0035
                                         oz41 :   0.0031
                                         oz15 :   0.0029
                                         oz26 :   0.0024
                                         oz18 :   0.0022
                                         oz38 :   0.0009
                                         oz16 :   0.0007
         

"""

import sys
import math
import argparse
import csv
import binascii
import faulthandler
import json
try:
    import numpy as np  # For numpy see: http://numpy.org
except ImportError as e:
    print("This predictor requires the Numpy library. Please run 'python3 -m pip install numpy'.", file=sys.stderr)
    raise e
try:
    from scipy.sparse import coo_matrix
    report_cmat = True
except ImportError:
    print("Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix. Try 'python3 -m pip install scipy'.", file=sys.stderr)
    report_cmat = False
try:
    import multiprocessing
    var_dict = {}
    default_to_serial = False
except:
    default_to_serial = True

IOBUF = 100000000
sys.setrecursionlimit(1000000)
TRAINFILE = ['TRAIN_TEST_SPLITS/fri_c0_1000_50-clean-train.csv']
mapping = {'1': 0, '0': 1}
ignorelabels = []
ignorecolumns = []
target = ''
target_column = 50
important_idxs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]
ignore_idxs = []
classifier_type = 'RF'
num_attr = 50
n_classes = 2
model_cap = 63
logits_dict = {0: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125460014, 0.0, 0.0, 0.0, -0.593391895, 0.0, 0.0, 0.0, -0.0, -0.601954579, 0.543660045, -0.104550004, -0.423851371, 0.209100008, -0.358457178, 0.332100034, -0.572752237, 0.0330157913, 0.214200005, 0.538321316]), 1: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.125460014, 0.0, 0.0, 0.0, 0.593391895, 0.0, 0.0, 0.0, -0.0, 0.601954579, -0.543660045, 0.104550004, 0.423851371, -0.209100008, 0.358457178, -0.332100034, 0.572752237, -0.0330157913, -0.214200005, -0.538321316]), 2: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0310947541, -0.45351252, 0.0, 0.0, -0.435427368, 0.0421641916, -0.243183896, 0.347662836, -0.508564413, 0.240463167, -0.366464138, 0.275356948, -0.40144515, 0.46980986, 0.329015404, -0.161777616]), 3: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0310947392, 0.45351252, 0.0, 0.0, 0.435427368, -0.0421642065, 0.243183881, -0.347662836, 0.508564413, -0.240463153, 0.366464078, -0.275356948, 0.40144515, -0.4698098, -0.329015434, 0.161777556]), 4: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.190395743, 0.0, 0.0, 0.0, 0.0, 0.108355291, -0.315481931, -0.0515235439, 0.343625098, 0.394876182, -0.00717566768, -0.230516449, 0.212390184, -0.159879863, -0.721219063, 0.0531653091, -0.436014235, -0.410816401, 0.250221103]), 5: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.190395758, 0.0, 0.0, 0.0, 0.0, -0.108355284, 0.315481901, 0.0515235402, -0.343625039, -0.394876182, 0.00717564486, 0.230516478, -0.212390184, 0.159879893, 0.721219063, -0.0531653054, 0.436014235, 0.410816401, -0.250221103]), 6: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.263727546, 0.0, -0.370584995, 0.0622292385, -0.440505803, 0.0364791639, -0.215663299, 0.103135966, 0.0515895039, 0.356013507, -0.280475706, 0.244484395, -0.602076948, 0.146677479, -0.115495846, 0.342440039]), 7: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.263727546, 0.0, 0.370584995, -0.062229231, 0.440505803, -0.0364791714, 0.215663299, -0.103135981, -0.0515894964, -0.356013477, 0.280475706, -0.244484395, 0.602076888, -0.146677524, 0.115495779, -0.342440099]), 8: np.array([0.0, 0.0, 0.0, -0.163868532, 0.0, 0.0, 0.0, -0.0113490596, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0888706893, 0.411016971, -0.249166355, 0.264480829, -0.245330393, 0.128359377, 0.287595481, -0.214224011, 0.265323073, -0.157245144]), 9: np.array([0.0, 0.0, 0.0, 0.163868532, 0.0, 0.0, 0.0, 0.0113490438, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0888707265, -0.411016971, 0.249166369, -0.264480859, 0.245330393, -0.128359392, -0.287595481, 0.214224011, -0.265323102, 0.157245114]), 10: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.241656303, 0.0, -0.303226948, -0.0675527602, 0.0, 0.0, 0.241741493, -0.253551304, 0.235615388, -0.195764259, -0.0754010081, 0.330549121, 0.104455419, -0.179433331, -0.045778241, 0.272284836]), 11: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.241656318, 0.0, 0.303226948, 0.0675527304, 0.0, 0.0, -0.241741538, 0.253551275, -0.235615373, 0.195764288, 0.075400956, -0.330549121, -0.104455449, 0.179433331, 0.0457782485, -0.272284836]), 12: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.315441579, -0.0137076573, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.130211577, 0.26652509, 0.0334242024, -0.27627638, -0.307525754, 0.0136757093, 0.2765733, -0.0942457095, 0.224081248, -0.15151301, 0.203756645, -0.322263211]), 13: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.315441579, 0.0137076452, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.130211532, -0.26652509, -0.0334242359, 0.27627641, 0.307525754, -0.0136757623, -0.276573241, 0.0942457393, -0.224081248, 0.151512966, -0.20375663, 0.322263211]), 14: np.array([0.0, 0.0, 0.0, 0.0827598646, 0.0, 0.0, 0.0, -0.300308079, -0.0322718322, 0.0, 0.0, 0.0, -0.0909107625, 0.0531090945, -0.294899106, -0.091579549, 0.128485143, 0.249179944, -0.0153623065]), 15: np.array([0.0, 0.0, 0.0, -0.0827598572, 0.0, 0.0, 0.0, 0.300308079, 0.0322718285, 0.0, 0.0, 0.0, 0.0909108073, -0.0531091429, 0.294899106, 0.0915795565, -0.128485158, -0.249179915, 0.015362245]), 16: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.280172944, 0.0, 0.0, 0.0, 0.0, 0.0, -0.104920708, 0.240647763, -0.0261674114, -0.29355216, -0.176885456, 0.0975149199, -0.244824007, 0.0378647819, -0.161304146, 0.173547909, 0.251632571, -0.0303197745, -0.182257712, 0.127069354]), 17: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.280172944, 0.0, 0.0, 0.0, 0.0, 0.0, 0.104920655, -0.240647763, 0.0261674076, 0.29355219, 0.176885456, -0.0975149497, 0.244824022, -0.0378647447, 0.161304146, -0.173547894, -0.251632571, 0.0303197764, 0.182257697, -0.127069339]), 18: np.array([0.0, 0.0, 0.0, 0.0714332461, 0.0, 0.0, 0.0, -0.0211042799, -0.24334617, 0.0, 0.0, 0.0, 0.0, -0.00492598675, -0.286657542, -0.0825341716, 0.131222188, 0.065402247, -0.209170461, -0.0793394446, 0.158150271]), 19: np.array([0.0, 0.0, 0.0, -0.071433194, 0.0, 0.0, 0.0, 0.0211042669, 0.24334617, 0.0, 0.0, 0.0, 0.0, 0.00492598908, 0.286657542, 0.0825341642, -0.131222203, -0.0654022619, 0.209170476, 0.0793394297, -0.158150271]), 20: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.193483114, -0.0906165317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.302494764, -0.00362664252, -0.20401144, 0.116875358, -0.196629867, 0.0392284244, -0.0433859937, 0.237566978, 0.101963565, -0.177779898, 0.238097772, -0.075630784]), 21: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.193483204, 0.0906165689, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.302494764, 0.00362668047, 0.204011425, -0.11687541, 0.196629897, -0.0392284133, 0.0433860756, -0.237566978, -0.101963572, 0.177779868, -0.238097772, 0.0756307691]), 22: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.187834963, -0.0402090177, -0.0965837315, 0.0, 0.0, 0.0, 0.163240314, -0.0669882223, -0.18475315, 0.0369614549, 0.21923469, -0.0478480645, 0.180706277, -0.0557821728, -0.0191616546, -0.191989005]), 23: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.187834963, 0.0402090028, 0.0965837762, 0.0, 0.0, 0.0, -0.163240328, 0.0669882074, 0.184753105, -0.0369614735, -0.21923466, 0.0478480421, -0.180706218, 0.0557821952, 0.0191616267, 0.191988945]), 24: np.array([0.0, 0.0, 0.0, 0.0, 0.0547444709, 0.0, 0.0, 0.0, 0.0, -0.129382402, 0.0, -0.199813157, 0.112579517, -0.116076149, 0.115052566, -0.218321458, 0.0142288385, -0.0990902558, 0.147769928]), 25: np.array([0.0, 0.0, 0.0, 0.0, -0.0547444597, 0.0, 0.0, 0.0, 0.0, 0.129382402, 0.0, 0.199813157, -0.112579554, 0.116076112, -0.115052551, 0.218321428, -0.01422875, 0.0990901962, -0.147769883]), 26: np.array([0.0, 0.0, 0.0, 0.0266552661, 0.0, 0.0, 0.0, -0.0037655998, 0.0, 0.0323304869, 0.0, 0.0, 0.0, -0.235794082, -0.0589283146, -0.198675737, -0.0575541295, -0.019719651, 0.171114981, 0.0206347071, -0.187874347]), 27: np.array([0.0, 0.0, 0.0, -0.0266552251, 0.0, 0.0, 0.0, 0.00376562029, 0.0, -0.0323304869, 0.0, 0.0, 0.0, 0.235794097, 0.0589283444, 0.198675737, 0.0575540364, 0.0197196137, -0.171114981, -0.0206347033, 0.187874332]), 28: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0496536754, 0.0, 0.0, 0.0, 0.169145212, 0.175550133, 0.0199558511, -0.0848801285, 0.129959047, -0.182324603, 0.0273474716, 0.0745783746, -0.106747292]), 29: np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0496536531, 0.0, 0.0, 0.0, -0.169145226, -0.175550088, -0.0199558716, 0.0848800912, -0.129958913, 0.182324603, -0.0273475796, -0.0745783225, 0.106747255]), 30: np.array([0.0, 0.0, 0.0, -0.178806603, 0.0, 0.0, 0.0, 0.0, 0.0, -0.184011832, 0.0290768016, 0.0, -0.0774627253, -0.135224044, 0.0120134903, -0.0123165864, 0.127991483, -0.0176176298, 0.147970796]), 31: np.array([0.0, 0.0, 0.0, 0.178806558, 0.0, 0.0, 0.0, 0.0, 0.0, 0.184011832, -0.0290767718, 0.0, 0.0774626955, 0.13522397, -0.0120134465, 0.0123165539, -0.127991453, 0.0176175795, -0.147970781]), 32: np.array([0.0, 0.0, 0.0, 0.0, 0.047294829, 0.0, 0.0, 0.01697102, 0.0, 0.0, 0.0894401148, -0.043384552, 0.0, -0.2141978, -0.0385944843, -0.127213314, 0.035182409, 0.184130758, 0.025247423]), 33: np.array([0.0, 0.0, 0.0, 0.0, -0.0472947583, 0.0, 0.0, -0.0169711746, 0.0, 0.0, -0.0894400105, 0.0433844514, 0.0, 0.214197844, 0.0385945439, 0.127213329, -0.0351824537, -0.184130713, -0.0252474416]), 34: np.array([0.0, 0.0, 0.0, 0.0320405252, -0.147124425, 0.0, 0.0, 0.0, 0.0, -0.0593855046, 0.0, 0.00290925219, -0.164687037, 0.151446804, -0.0194712933, 0.188221976, -0.0222495496]), 35: np.array([0.0, 0.0, 0.0, -0.0320405513, 0.14712438, 0.0, 0.0, 0.0, 0.0, 0.0593854077, 0.0, -0.00290922285, 0.164687037, -0.151446804, 0.0194712225, -0.188221902, 0.0222496334])}
right_children_dict = {0: np.array([1, 3, 5, 7, 9, 11, 13, -1, 15, 17, 19, -1, 21, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 1: np.array([1, 3, 5, 7, 9, 11, 13, -1, 15, 17, 19, -1, 21, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 2: np.array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, -1, -1, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 3: np.array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, -1, -1, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 4: np.array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, -1, 21, 23, 25, 27, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 5: np.array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, -1, 21, 23, 25, 27, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 6: np.array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, -1, 27, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 7: np.array([1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, -1, 27, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 8: np.array([1, 3, 5, -1, 7, 9, 11, -1, 13, 15, 17, 19, 21, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 9: np.array([1, 3, 5, -1, 7, 9, 11, -1, 13, 15, 17, 19, 21, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 10: np.array([1, 3, 5, 7, 9, 11, 13, 15, 17, -1, 19, -1, -1, 21, 23, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 11: np.array([1, 3, 5, 7, 9, 11, 13, 15, 17, -1, 19, -1, -1, 21, 23, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 12: np.array([1, 3, 5, 7, 9, 11, 13, -1, -1, 15, 17, 19, 21, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 13: np.array([1, 3, 5, 7, 9, 11, 13, -1, -1, 15, 17, 19, 21, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 14: np.array([1, 3, 5, -1, 7, 9, 11, -1, -1, 13, 15, 17, -1, -1, -1, -1, -1, -1, -1]), 15: np.array([1, 3, 5, -1, 7, 9, 11, -1, -1, 13, 15, 17, -1, -1, -1, -1, -1, -1, -1]), 16: np.array([1, 3, 5, 7, 9, 11, 13, 15, 17, -1, 19, 21, 23, 25, 27, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 17: np.array([1, 3, 5, 7, 9, 11, 13, 15, 17, -1, 19, 21, 23, 25, 27, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 18: np.array([1, 3, 5, -1, 7, 9, 11, -1, -1, 13, 15, 17, 19, -1, -1, -1, -1, -1, -1, -1, -1]), 19: np.array([1, 3, 5, -1, 7, 9, 11, -1, -1, 13, 15, 17, 19, -1, -1, -1, -1, -1, -1, -1, -1]), 20: np.array([1, 3, 5, 7, 9, 11, 13, -1, -1, 15, 17, 19, 21, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 21: np.array([1, 3, 5, 7, 9, 11, 13, -1, -1, 15, 17, 19, 21, 23, 25, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 22: np.array([1, 3, 5, 7, 9, 11, 13, 15, 17, -1, -1, -1, 19, 21, 23, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 23: np.array([1, 3, 5, 7, 9, 11, 13, 15, 17, -1, -1, -1, 19, 21, 23, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 24: np.array([1, 3, 5, 7, -1, 9, 11, 13, 15, -1, 17, -1, -1, -1, -1, -1, -1, -1, -1]), 25: np.array([1, 3, 5, 7, -1, 9, 11, 13, 15, -1, 17, -1, -1, -1, -1, -1, -1, -1, -1]), 26: np.array([1, 3, 5, -1, 7, 9, 11, -1, 13, -1, 15, 17, 19, -1, -1, -1, -1, -1, -1, -1, -1]), 27: np.array([1, 3, 5, -1, 7, 9, 11, -1, 13, -1, 15, 17, 19, -1, -1, -1, -1, -1, -1, -1, -1]), 28: np.array([1, 3, 5, 7, 9, 11, -1, 13, 15, 17, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 29: np.array([1, 3, 5, 7, 9, 11, -1, 13, 15, 17, -1, -1, -1, -1, -1, -1, -1, -1, -1]), 30: np.array([1, 3, 5, -1, 7, 9, 11, 13, 15, -1, -1, 17, -1, -1, -1, -1, -1, -1, -1]), 31: np.array([1, 3, 5, -1, 7, 9, 11, 13, 15, -1, -1, 17, -1, -1, -1, -1, -1, -1, -1]), 32: np.array([1, 3, 5, 7, -1, 9, 11, -1, 13, 15, -1, -1, 17, -1, -1, -1, -1, -1, -1]), 33: np.array([1, 3, 5, 7, -1, 9, 11, -1, 13, 15, -1, -1, 17, -1, -1, -1, -1, -1, -1]), 34: np.array([1, 3, 5, -1, -1, 7, 9, 11, 13, -1, 15, -1, -1, -1, -1, -1, -1]), 35: np.array([1, 3, 5, -1, -1, 7, 9, 11, 13, -1, 15, -1, -1, -1, -1, -1, -1])}
split_feats_dict = {0: np.array([1, 3, 0, 45, 45, 3, 3, 0, 30, 31, 0, 0, 0, 31, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 1: np.array([1, 3, 0, 45, 45, 3, 3, 0, 30, 31, 0, 0, 0, 31, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 2: np.array([3, 0, 1, 2, 4, 45, 0, 35, 42, 2, 1, 0, 0, 44, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 3: np.array([3, 0, 1, 2, 4, 45, 0, 35, 42, 2, 1, 0, 0, 44, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 4: np.array([1, 3, 4, 2, 13, 1, 3, 2, 38, 7, 0, 0, 32, 24, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 5: np.array([1, 3, 4, 2, 13, 1, 3, 2, 38, 7, 0, 0, 32, 24, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 6: np.array([3, 0, 4, 1, 12, 9, 1, 2, 32, 3, 3, 0, 9, 0, 26, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 7: np.array([3, 0, 4, 1, 12, 9, 1, 2, 32, 3, 3, 0, 9, 0, 26, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 8: np.array([2, 1, 2, 0, 39, 3, 24, 0, 23, 39, 0, 44, 31, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 9: np.array([2, 1, 2, 0, 39, 3, 24, 0, 23, 39, 0, 44, 31, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 10: np.array([4, 23, 35, 2, 1, 33, 41, 5, 16, 0, 31, 0, 0, 13, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 11: np.array([4, 23, 35, 2, 1, 33, 41, 5, 16, 0, 31, 0, 0, 13, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 12: np.array([0, 3, 1, 29, 22, 0, 1, 0, 0, 48, 2, 3, 45, 5, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 13: np.array([0, 3, 1, 29, 22, 0, 1, 0, 0, 48, 2, 3, 45, 5, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 14: np.array([3, 2, 4, 0, 2, 0, 20, 0, 0, 21, 1, 16, 0, 0, 0, 0, 0, 0, 0]), 15: np.array([3, 2, 4, 0, 2, 0, 20, 0, 0, 21, 1, 16, 0, 0, 0, 0, 0, 0, 0]), 16: np.array([3, 49, 48, 22, 49, 43, 36, 35, 27, 0, 9, 11, 39, 27, 29, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 17: np.array([3, 49, 48, 22, 49, 43, 36, 35, 27, 0, 9, 11, 39, 27, 29, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 18: np.array([0, 21, 4, 0, 2, 10, 8, 0, 0, 9, 7, 27, 1, 0, 0, 0, 0, 0, 0, 0, 0]), 19: np.array([0, 21, 4, 0, 2, 10, 8, 0, 0, 9, 7, 27, 1, 0, 0, 0, 0, 0, 0, 0, 0]), 20: np.array([3, 33, 0, 24, 12, 1, 4, 0, 0, 40, 0, 27, 0, 44, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 21: np.array([3, 33, 0, 24, 12, 1, 4, 0, 0, 40, 0, 27, 0, 44, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 22: np.array([3, 2, 45, 2, 34, 26, 21, 28, 8, 0, 0, 0, 36, 46, 45, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 23: np.array([3, 2, 45, 2, 34, 26, 21, 28, 8, 0, 0, 0, 36, 46, 45, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 24: np.array([4, 23, 9, 16, 0, 3, 39, 49, 14, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0]), 25: np.array([4, 23, 9, 16, 0, 3, 39, 49, 14, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0]), 26: np.array([1, 14, 0, 0, 45, 25, 1, 0, 43, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 27: np.array([1, 14, 0, 0, 45, 25, 1, 0, 43, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 28: np.array([4, 2, 22, 2, 42, 45, 0, 44, 3, 34, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 29: np.array([4, 2, 22, 2, 42, 45, 0, 44, 3, 34, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 30: np.array([0, 3, 1, 0, 34, 13, 1, 44, 26, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0]), 31: np.array([0, 3, 1, 0, 34, 13, 1, 44, 26, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0]), 32: np.array([1, 20, 11, 11, 0, 23, 40, 0, 3, 49, 0, 0, 41, 0, 0, 0, 0, 0, 0]), 33: np.array([1, 20, 11, 11, 0, 23, 40, 0, 3, 49, 0, 0, 41, 0, 0, 0, 0, 0, 0]), 34: np.array([3, 33, 1, 0, 0, 0, 36, 21, 41, 0, 28, 0, 0, 0, 0, 0, 0]), 35: np.array([3, 33, 1, 0, 0, 0, 36, 21, 41, 0, 28, 0, 0, 0, 0, 0, 0])}
split_vals_dict = {0: np.array([-0.493788481, 0.146075994, -1.01019251, -1.66685557, -0.716032505, 0.140433997, -1.13336992, 0.0, -1.70142007, -0.121185005, 0.508478522, 0.0, -1.39277506, 0.199143499, -0.0990720019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 1: np.array([-0.493788481, 0.146075994, -1.01019251, -1.66685557, -0.716032505, 0.140433997, -1.13336992, 0.0, -1.70142007, -0.121185005, 0.508478522, 0.0, -1.39277506, 0.199143499, -0.0990720019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 2: np.array([0.132734507, -0.0294240005, -1.27489996, 1.11498201, -0.789173961, -0.728533983, -1.4808135, 1.56841946, 0.911872506, 1.06756449, -0.85965097, 0.0, 0.0, 0.340950489, 1.45915246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 3: np.array([0.132734507, -0.0294240005, -1.27489996, 1.11498201, -0.789173961, -0.728533983, -1.4808135, 1.56841946, 0.911872506, 1.06756449, -0.85965097, 0.0, 0.0, 0.340950489, 1.45915246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 4: np.array([-0.260726511, 1.43095756, -1.14271593, 1.49540246, 1.35308146, 1.04867351, -1.22031546, -1.2707535, -1.06427896, 0.408706009, 0.0, 0.541054964, 0.579743981, -0.449604511, -1.63276958, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 5: np.array([-0.260726511, 1.43095756, -1.14271593, 1.49540246, 1.35308146, 1.04867351, -1.22031546, -1.2707535, -1.06427896, 0.408706009, 0.0, 0.541054964, 0.579743981, -0.449604511, -1.63276958, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 6: np.array([0.598692, -0.0294240005, -0.745283008, 0.506269991, 0.788698494, 0.571254015, -1.47576141, 1.54107261, -1.31681204, -0.717221498, -1.12702, -0.994792461, 1.14433455, 0.0, -1.34384894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 7: np.array([0.598692, -0.0294240005, -0.745283008, 0.506269991, 0.788698494, 0.571254015, -1.47576141, 1.54107261, -1.31681204, -0.717221498, -1.12702, -0.994792461, 1.14433455, 0.0, -1.34384894, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 8: np.array([-1.33073699, -1.08261156, 0.917381525, 0.0, -1.11177301, 0.598692, -0.121132001, 0.0, -0.995228529, 1.38440299, -1.39277506, 1.63626802, -0.883892536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 9: np.array([-1.33073699, -1.08261156, 0.917381525, 0.0, -1.11177301, 0.598692, -0.121132001, 0.0, -0.995228529, 1.38440299, -1.39277506, 1.63626802, -0.883892536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 10: np.array([-0.275069475, 0.978366017, -1.48847103, 1.28793359, -0.468445003, 0.422353506, -0.435173005, -1.47298193, 1.15342855, 0.0, -0.64538002, 0.0, 0.0, 0.75674051, -1.03233194, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 11: np.array([-0.275069475, 0.978366017, -1.48847103, 1.28793359, -0.468445003, 0.422353506, -0.435173005, -1.47298193, 1.15342855, 0.0, -0.64538002, 0.0, 0.0, 0.75674051, -1.03233194, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 12: np.array([-0.582691014, 0.140433997, -0.493788481, 1.15427995, 0.00473899953, 0.508478522, 1.50460601, 0.0, 0.0, -0.144168496, -1.13951898, 1.25317001, -0.996729016, 1.57407999, -0.72902751, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 13: np.array([-0.582691014, 0.140433997, -0.493788481, 1.15427995, 0.00473899953, 0.508478522, 1.50460601, 0.0, 0.0, -0.144168496, -1.13951898, 1.25317001, -0.996729016, 1.57407999, -0.72902751, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 14: np.array([-1.22031546, -1.413872, 0.947533011, 0.0, 1.25406349, -1.033849, 1.14296603, 0.0, 0.0, -1.45093799, 0.0806525052, 1.40694749, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 15: np.array([-1.22031546, -1.413872, 0.947533011, 0.0, 1.25406349, -1.033849, 1.14296603, 0.0, 0.0, -1.45093799, 0.0806525052, 1.40694749, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 16: np.array([-0.0650620013, 0.628134012, -0.132385492, -1.17496443, 0.948979497, -0.1201455, 0.864238024, 0.506917477, -0.301661491, 0.0, 0.254615486, 1.155913, -0.970505476, 1.46470547, -0.1134165, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 17: np.array([-0.0650620013, 0.628134012, -0.132385492, -1.17496443, 0.948979497, -0.1201455, 0.864238024, 0.506917477, -0.301661491, 0.0, 0.254615486, 1.155913, -0.970505476, 1.46470547, -0.1134165, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 18: np.array([-1.033849, -1.45093799, -1.28757644, 0.0, -1.22430801, 0.349023998, -1.12351251, 0.0, 0.0, -0.553176522, -0.0977225006, -0.0688364953, -0.838465989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 19: np.array([-1.033849, -1.45093799, -1.28757644, 0.0, -1.22430801, 0.349023998, -1.12351251, 0.0, 0.0, -0.553176522, -0.0977225006, -0.0688364953, -0.838465989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 20: np.array([-0.709794521, -0.485999495, 0.245725006, -0.272698998, 0.79993701, 0.200529501, -1.28997099, 0.0, 0.0, 1.15048552, -0.0119749978, 1.02989197, -0.979631484, 0.570973516, 1.58124244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 21: np.array([-0.709794521, -0.485999495, 0.245725006, -0.272698998, 0.79993701, 0.200529501, -1.28997099, 0.0, 0.0, 1.15048552, -0.0119749978, 1.02989197, -0.979631484, 0.570973516, 1.58124244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 22: np.array([0.132734507, 1.34529448, 0.553168535, -1.06539297, 0.463336498, -1.34384894, 0.139446005, 0.147085994, 1.01571751, 0.0, 0.0, 0.0, 0.89645052, 0.593560517, 0.909527004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 23: np.array([0.132734507, 1.34529448, 0.553168535, -1.06539297, 0.463336498, -1.34384894, 0.139446005, 0.147085994, 1.01571751, 0.0, 0.0, 0.0, 0.89645052, 0.593560517, 0.909527004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 24: np.array([-0.534070969, 0.978366017, 1.05758643, -0.595412016, 0.0, -1.32363105, 0.856367469, 0.250945985, 0.828852475, 0.0, -1.42084908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 25: np.array([-0.534070969, 0.978366017, 1.05758643, -0.595412016, 0.0, -1.32363105, 0.856367469, 0.250945985, 0.828852475, 0.0, -1.42084908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 26: np.array([-0.493788481, -0.916777015, -0.777233481, 0.0, -0.721361995, -0.603909969, 1.50460601, 0.0, 0.926555037, 0.0, 0.140433997, -1.11155701, 1.05887651, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 27: np.array([-0.493788481, -0.916777015, -0.777233481, 0.0, -0.721361995, -0.603909969, 1.50460601, 0.0, 0.926555037, 0.0, 0.140433997, -1.11155701, 1.05887651, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 28: np.array([0.982236981, 0.98794198, 0.784074545, -0.850816965, 0.576700985, 0.65882051, 0.0, -0.477173001, 0.860620975, -0.333648503, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 29: np.array([0.982236981, 0.98794198, 0.784074545, -0.850816965, 0.576700985, 0.65882051, 0.0, -0.477173001, 0.860620975, -0.333648503, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 30: np.array([-0.358043015, -0.100182995, -1.00328803, 0.0, 0.339911491, 0.477104485, 1.50460601, 0.0922940001, -0.0905690044, 0.0, 0.0, -1.14260101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 31: np.array([-0.358043015, -0.100182995, -1.00328803, 0.0, 0.339911491, 0.477104485, 1.50460601, 0.0922940001, -0.0905690044, 0.0, 0.0, -1.14260101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 32: np.array([-0.260726511, 0.901678503, 0.0602620021, -0.985158503, 0.0, 0.804791987, -0.796944022, 0.0, 0.598692, 0.271375507, 0.0, 0.0, 0.48042351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 33: np.array([-0.260726511, 0.901678503, 0.0602620021, -0.985158503, 0.0, 0.804791987, -0.796944022, 0.0, 0.598692, 0.271375507, 0.0, 0.0, 0.48042351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 34: np.array([-0.983474493, -0.614154994, 0.179226995, 0.0, 0.0, 0.508478522, -1.21500397, -0.933761477, 0.0706700012, 0.0, 0.902152002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 35: np.array([-0.983474493, -0.614154994, 0.179226995, 0.0, 0.0, 0.508478522, -1.21500397, -0.933761477, 0.0706700012, 0.0, 0.902152002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])}


class PredictorError(Exception):

    def __init__(self, msg, code):
        self.msg = msg
        self.code = code

    def __str__(self):
        return self.msg

def __convert(cell):
    value = str(cell)
    try:
        result = int(value)
        return result
    except ValueError:
        try:
            result = float(value)
            if math.isnan(result):
                raise PredictorError('NaN value found. Aborting.', code=1)
            return result
        except ValueError:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            return result
        except Exception as e:
            raise e


def __get_key(val, dictionary):
    if dictionary == {}:
        return val
    for key, value in dictionary.items():
        if val == value:
            return key
    if val not in dictionary.values():
        raise PredictorError(f"Label {val} key does not exist", code=2)


def __confusion_matrix(y_true, y_pred, json):
    stats = {}
    labels = np.array(list(mapping.keys()))
    sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
    for class_i in range(n_classes):
        class_i_label = __get_key(class_i, mapping)
        stats[int(class_i)] = {}
        class_i_indices = np.argwhere(y_true == class_i_label)
        not_class_i_indices = np.argwhere(y_true != class_i_label)
        # None represents N/A in this case
        stats[int(class_i)]['TP'] = TP = int(np.sum(y_pred[class_i_indices] == class_i_label)) if class_i_indices.size > 0 else None
        stats[int(class_i)]['FN'] = FN = int(np.sum(y_pred[class_i_indices] != class_i_label)) if class_i_indices.size > 0 else None
        stats[int(class_i)]['TN'] = TN = int(np.sum(y_pred[not_class_i_indices] != class_i_label)) if not_class_i_indices.size > 0 else None
        stats[int(class_i)]['FP'] = FP = int(np.sum(y_pred[not_class_i_indices] == class_i_label)) if not_class_i_indices.size > 0 else None
        if TP is None or FN is None or (TP + FN == 0):
            stats[int(class_i)]['TPR'] = None
        else:
            stats[int(class_i)]['TPR'] = (TP / (TP + FN))
        if TN is None or FP is None or (TN + FP == 0):
            stats[int(class_i)]['TNR'] = None
        else:
            stats[int(class_i)]['TNR'] = (TN / (TN + FP))
        if TP is None or FP is None or (TP + FP == 0):
            stats[int(class_i)]['PPV'] = None
        else:
            stats[int(class_i)]['PPV'] = (TP / (TP + FP))
        if TN is None or FN is None or (TN + FN == 0):
            stats[int(class_i)]['NPV'] = None
        else:
            stats[int(class_i)]['NPV'] = (TN / (TN + FN))
        if TP is None or FP is None or FN is None or (TP + FP + FN == 0):
            stats[int(class_i)]['F1'] = None
        else:
            stats[int(class_i)]['F1'] = ((2 * TP) / (2 * TP + FP + FN))
        if TP is None or FP is None or FN is None or (TP + FP + FN == 0):
            stats[int(class_i)]['TS'] = None
        else:
            stats[int(class_i)]['TS'] = (TP / (TP + FP + FN))

    if not report_cmat:
        return np.array([]), stats

    label_to_ind = {label: i for i, label in enumerate(labels)}
    y_pred = np.array([label_to_ind.get(x, n_classes + 1) for x in y_pred])
    y_true = np.array([label_to_ind.get(x, n_classes + 1) for x in y_true])

    ind = np.logical_and(y_pred < n_classes, y_true < n_classes)
    y_pred = y_pred[ind]
    y_true = y_true[ind]
    sample_weight = sample_weight[ind]

    cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_classes, n_classes), dtype=np.int64).toarray()
    with np.errstate(all='ignore'):
        cm = np.nan_to_num(cm)

    return cm, stats


def __preprocess_and_clean_in_memory(arr):
    clean_arr = np.zeros((len(arr), len(important_idxs)))
    for i, row in enumerate(arr):
        try:
            row_used_cols_only = [row[i] for i in important_idxs]
        except IndexError:
            error_str = f"The input has shape ({len(arr)}, {len(row)}) but the expected shape is (*, {len(ignorecolumns) + len(important_idxs)})."
            if len(arr) == num_attr and len(arr[0]) != num_attr:
                error_str += "\n\nNote: You may have passed an input directly to 'preprocess_and_clean_in_memory' or 'predict_in_memory' "
                error_str += "rather than as an element of a list. Make sure that even single instances "
                error_str += "are enclosed in a list. Example: predict_in_memory(0) is invalid but "
                error_str += "predict_in_memory([0]) is valid."
            raise PredictorError(error_str, 3)
        clean_arr[i] = [float(__convert(field)) for field in row_used_cols_only]
    return clean_arr


def __evaluate_tree(xs, split_vals, split_feats, right_children, logits):
    if xs is None:
        xs = np.frombuffer(var_dict['X']).reshape(var_dict['X_shape'])

    current_node_per_row = np.zeros(xs.shape[0]).astype('int')
    values = np.empty(xs.shape[0])
    values.fill(np.nan)

    while np.isnan(values).any():

        row_idxs_at_leaf = np.argwhere(np.logical_and(right_children[current_node_per_row] == -1, np.isnan(values))).reshape(-1)
        row_idxs_at_branch = np.argwhere(right_children[current_node_per_row] != -1).reshape(-1)

        if row_idxs_at_leaf.shape[0] > 0:

            values[row_idxs_at_leaf] = logits[current_node_per_row[row_idxs_at_leaf]].reshape(-1)
            current_node_per_row[row_idxs_at_leaf] = -1

        if row_idxs_at_branch.shape[0] > 0:

            split_values_per_row = split_vals[current_node_per_row[row_idxs_at_branch]].astype('float64')
            split_features_per_row = split_feats[current_node_per_row[row_idxs_at_branch]].astype('int')
            feature_val_per_row = xs[row_idxs_at_branch, split_features_per_row].reshape(-1)

            branch_nodes = current_node_per_row[row_idxs_at_branch]
            current_node_per_row[row_idxs_at_branch] = np.where(feature_val_per_row < split_values_per_row,
                                                                right_children[branch_nodes].astype('int'),
                                                                (right_children[branch_nodes] + 1).astype('int'))

    return values


def __build_logit_func(n_trees, clss):

    def __logit_func(xs, serial, data_shape, pool=None):
        if serial:
            sum_of_leaf_values = np.zeros(xs.shape[0])
            for booster_index in range(clss, n_trees, n_classes):
                sum_of_leaf_values += __evaluate_tree(
                    xs, split_vals_dict[booster_index], split_feats_dict[booster_index],
                    right_children_dict[booster_index], logits_dict[booster_index])
        else:
            sum_of_leaf_values = np.sum(
                list(pool.starmap(__evaluate_tree,
                                  [(None, split_vals_dict[booster_index], split_feats_dict[booster_index],
                                    right_children_dict[booster_index], logits_dict[booster_index])
                                   for booster_index in range(clss, n_trees, n_classes)])), axis=0)
        return sum_of_leaf_values

    return __logit_func


def __init_worker(X, X_shape):
    var_dict['X'] = X
    var_dict['X_shape'] = X_shape

def __classify(rows, return_probabilities=False, force_serial=False):
    if force_serial:
        serial = True
    else:
        serial = default_to_serial
    if isinstance(rows, list):
        rows = np.array(rows)

    logits = [__build_logit_func(36, clss) for clss in range(n_classes)]

    if serial:
        o = np.array([logits[class_index](rows, True, rows.shape) for class_index in range(n_classes)]).T
    else:
        shared_arr = multiprocessing.RawArray('d', rows.shape[0] * rows.shape[1])
        shared_arr_np = np.frombuffer(shared_arr, dtype=rows.dtype).reshape(rows.shape)
        np.copyto(shared_arr_np, rows)

        procs = multiprocessing.cpu_count()
        pool = multiprocessing.Pool(processes=procs, initializer=__init_worker, initargs=(shared_arr, rows.shape))
        o = np.array([logits[class_index](None, False, rows.shape, pool) for class_index in range(n_classes)]).T

    if return_probabilities:

        argument = o[:, 0] - o[:, 1]
        p0 = 1.0 / (1.0 + np.exp(-argument)).reshape(-1, 1)
        p1 = 1.0 - p0
        output = np.concatenate((p0, p1), axis=1)

    else:
        output = np.argmax(o, axis=1)
    return output



def __validate_kwargs(kwargs):
    for key in kwargs:

        if key not in ['return_probabilities', 'force_serial']:
            raise PredictorError(f'{key} is not a keyword argument for Brainome\'s {classifier_type} predictor. Please see the documentation.', 4)


def __validate_data(row_or_arr, validate, row_num=None):
    if validate:
        expected_columns = len(important_idxs) + len(ignore_idxs) + 1
    else:
        expected_columns = len(important_idxs) + len(ignore_idxs)

    input_is_array = isinstance(row_or_arr, np.ndarray)
    n_cols = row_or_arr.shape[1] if input_is_array else len(row_or_arr)

    if n_cols != expected_columns:

        if row_num is None:
            err_str = f"Your data contains {n_cols} columns but {expected_columns} are required."
        else:
            err_str = f"At row {row_num}, your data contains {n_cols} columns but {expected_columns} are required."

        if validate:
            err_str += " The predictor's validate() method works on data that has the same columns in the same order as were present in the training CSV."
            err_str += " This includes the target column and features that are not used by the model but existed in the training CSV."
            if n_cols == 1 + len(important_idxs):
                err_str += f" We suggest confirming that the {len(ignore_idxs)} unused features are present in the data."
            elif n_cols == len(important_idxs):
                err_str += f" We suggest confirming that the {len(ignore_idxs)} unused features are present in the data as well as the target column. "
            elif n_cols == len(important_idxs) + len(ignore_idxs):
                err_str += " We suggest confirming that the target column present in the data. "
            err_str += " To make predictions, see the predictor's predict() method."
        else:
            err_str += " The predictor's predict() method works on data that has the same feature columns in the same relative order as were present in the training CSV."
            err_str += " This DOES NOT include the target column but DOES include features that are not used by the model but existed in the training CSV."
            if n_cols == 1 + len(important_idxs):
                err_str += f" We suggest confirming that the {len(ignore_idxs)} unused features are present in the data and that the target column is not present."
            elif n_cols == len(important_idxs):
                err_str += f" We suggest confirming that the {len(ignore_idxs)} unused features are present in the data."
            elif n_cols == 1 + len(important_idxs) + len(ignore_idxs):
                err_str += " We suggest confirming that the target column is not present."
            err_str += " To receive a performance summary, instead of make predictions, see the predictor's validate() method."

        raise PredictorError(err_str, 5)

    else:

        if not input_is_array:
            return row_or_arr


def __write_predictions(arr, header, headerless, trim, outfile=None):
    predictions = predict(arr)

    if not headerless:
        if trim:
            header = ','.join([x for i, x in enumerate(header) if i in important_idxs] + ['Prediction'])
        else:
            header = ','.join(header.tolist() + ['Prediction'])
        if outfile is None:
            print(header)
        else:
            print(header, file=outfile)

    for row, prediction in zip(arr, predictions):
        if trim:
            row = ['"' + field + '"' if ',' in field else field for i, field in enumerate(row) if i in important_idxs]
        else:
            row = ['"' + field + '"' if ',' in field else field for field in row]
        row.append(prediction)
        if outfile is None:
            print(','.join(row))
        else:
            print(','.join(row), file=outfile)


def load_data(csvfile, headerless, validate):
    """
    Parameters
    ----------
    csvfile : str
        The path to the CSV file containing the data.

    headerless : bool
        True if the CSV does not contain a header.

    validate : bool
        True if the data should be loaded to be used by the predictor's validate() method.
        False if the data should be loaded to be used by the predictor's predict() method.

    Returns
    -------
    arr : np.ndarray
        The data (observations and labels) found in the CSV without any header.

    data : np.ndarray or NoneType
        None if validate is False, otherwise the observations (data without the labels) found in the CSV.

    labels : np.ndarray or NoneType
        None if the validate is False, otherwise the labels found in the CSV.

    header : np.ndarray or NoneType
        None if the CSV is headerless, otherwise the header.
    """

    with open(csvfile, 'r', encoding='utf-8') as csvinput:
        arr = np.array([__validate_data(row, validate, row_num=i) for i, row in enumerate(csv.reader(csvinput)) if row != []], dtype=str)
    if headerless:
        header = None
    else:
        header = arr[0]
        arr = arr[1:]
    if validate:
        labels = arr[:, target_column]
        feature_columns = [i for i in range(arr.shape[1]) if i != target_column]
        data = arr[:, feature_columns]
    else:
        data, labels = None, None

    if validate and ignorelabels != []:
        idxs_to_keep = np.argwhere(np.logical_not(np.isin(labels, ignorelabels))).reshape(-1)
        labels = labels[idxs_to_keep]
        data = data[idxs_to_keep]

    return arr, data, labels, header


def predict(arr, remap=True, **kwargs):
    """
    Parameters
    ----------
    arr : list[list]
        An array of inputs to be cleaned by 'preprocess_and_clean_in_memory'. This
        should contain all the features that were present in the training data,
        regardless of whether or not they are used by the model, with the same
        relative order as in the training data. There should be no target column.


    remap : bool
        If True and 'return_probs' is False, remaps the output to the original class
        label. If 'return_probs' is True this instead adds a header indicating which
        original class label each column of output corresponds to.

    **kwargs :
        return_probabilities : bool
            If true, return class membership probabilities instead of classifications.

    **kwargs :
        force_serial : bool
            If true, model inference is done in serial rather than in parallel. This is
            useful if calling "predict" repeatedly inside a for-loop.

    Returns
    -------
    output : np.ndarray

        A numpy array of
            1. Class predictions if 'return_probabilities' is False.
            2. Class probabilities if 'return_probabilities' is True.

    """
    if not isinstance(arr, np.ndarray) and not isinstance(arr, list):
        raise PredictorError(f'Data must be provided to \'predict\' and \'validate\' as a list or np.ndarray, but an input of type {type(arr).__name__} was found.', 6)
    if isinstance(arr, list):
        arr = np.array(arr, dtype=str)

    kwargs = kwargs or {}
    __validate_kwargs(kwargs)
    __validate_data(arr, False)
    remove_bad_chars = lambda x: str(x).replace('"', '').replace(',', '').replace('(', '').replace(')', '').replace("'", '')
    arr = [[remove_bad_chars(field) for field in row] for row in arr]
    arr = __preprocess_and_clean_in_memory(arr)

    output = __classify(arr, **kwargs)

    if remap:
        if kwargs.get('return_probabilities'):
            header = np.array([__get_key(i, mapping) for i in range(output.shape[1])], dtype=str).reshape(1, -1)
            output = np.concatenate((header, output), axis=0)
        else:
            output = np.array([__get_key(prediction, mapping) for prediction in output])

    return output


def validate(arr, labels):
    """
    Parameters
    ----------
    cleanarr : np.ndarray
        An array of float values that has undergone each pre-
        prediction step.

    Returns
    -------
    count : int
        A count of the number of instances in cleanarr.

    correct_count : int
        A count of the number of correctly classified instances in
        cleanarr.

    numeachclass : dict
        A dictionary mapping each class to its number of instances.

    outputs : np.ndarray
        The output of the predictor's '__classify' method on cleanarr.
    """
    predictions = predict(arr)
    correct_count = int(np.sum(predictions.reshape(-1) == labels.reshape(-1)))
    count = predictions.shape[0]
    
    class_0, class_1 = __get_key(0, mapping), __get_key(1, mapping)
    num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0
    num_TP = int(np.sum(np.logical_and(predictions.reshape(-1) == class_1, labels.reshape(-1) == class_1)))
    num_TN = int(np.sum(np.logical_and(predictions.reshape(-1) == class_0, labels.reshape(-1) == class_0)))
    num_FN = int(np.sum(np.logical_and(predictions.reshape(-1) == class_0, labels.reshape(-1) == class_1)))
    num_FP = int(np.sum(np.logical_and(predictions.reshape(-1) == class_1, labels.reshape(-1) == class_0)))
    num_class_0 = int(np.sum(labels.reshape(-1) == class_0))
    num_class_1 = int(np.sum(labels.reshape(-1) == class_1))
    return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, predictions


def __main():
    parser = argparse.ArgumentParser(description='Predictor trained on ' + str(TRAINFILE))
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    parser.add_argument('-json', action="store_true", default=False, help="report measurements as json")
    parser.add_argument('-trim', action="store_true", help="If true, the prediction will not output ignored columns.")
    args = parser.parse_args()
    faulthandler.enable()

    arr, data, labels, header = load_data(csvfile=args.csvfile, headerless=args.headerless, validate=args.validate)

    if not args.validate:
        __write_predictions(arr, header, args.headerless, args.trim)
    else:

        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds = validate(data, labels)

        classcounts = np.bincount(np.array([mapping[label] for label in labels], dtype='int32')).reshape(-1)
        class_balance = (classcounts[np.argwhere(classcounts > 0)] / arr.shape[0]).reshape(-1).tolist()
        best_guess = round(100.0 * np.max(class_balance), 2)
        H = float(-1.0 * sum([class_balance[i] * math.log(class_balance[i]) / math.log(2) for i in range(len(class_balance))]))
        modelacc = int(float(correct_count * 10000) / count) / 100.0
        mtrx, stats = __confusion_matrix(np.array(labels).reshape(-1), np.array(preds).reshape(-1), args.json)

        if args.json:
            json_dict = {'instance_count': count,
                         'classifier_type': classifier_type,
                         'classes': n_classes,
                         'number_correct': correct_count,
                         'accuracy': {
                             'best_guess': (best_guess/100),
                             'improvement': (modelacc - best_guess)/100,
                              'model_accuracy': (modelacc/100),
                         },
                         'model_capacity': model_cap,
                         'generalization_ratio': int(float(correct_count * 100) / model_cap) / 100.0 * H,
                         'model_efficiency': int(100 * (modelacc - best_guess) / model_cap) / 100.0,
                         'shannon_entropy_of_labels': H,
                         'class_balance': class_balance,
                         'confusion_matrix': mtrx.tolist(),
                         'multiclass_stats': stats}

            print(json.dumps(json_dict))
        else:
            pad = lambda s, length, pad_right: str(s) + ' ' * max(0, length - len(str(s))) if pad_right else ' ' * max(0, length - len(str(s))) + str(s)
            labels = np.array(list(mapping.keys())).reshape(-1, 1)
            max_class_name_len = max([len(clss) for clss in mapping.keys()] + [7])

            max_TP_len = max([len(str(stats[key]['TP'])) for key in stats.keys()] + [2])
            max_FP_len = max([len(str(stats[key]['FP'])) for key in stats.keys()] + [2])
            max_TN_len = max([len(str(stats[key]['TN'])) for key in stats.keys()] + [2])
            max_FN_len = max([len(str(stats[key]['FN'])) for key in stats.keys()] + [2])

            cmat_template_1 = "    {} | {}"
            cmat_template_2 = "    {} | " + " {} " * n_classes
            acc_by_class_template_1 = "    {} | {}  {}  {}  {}  {}  {}  {}  {}  {}  {}"

            acc_by_class_lengths = [max_class_name_len, max_TP_len, max_FP_len, max_TN_len, max_FN_len, 7, 7, 7, 7, 7, 7]
            acc_by_class_header_fields = ['target', 'TP', 'FP', 'TN', 'FN', 'TPR', 'TNR', 'PPV', 'NPV', 'F1', 'TS']
            print("Classifier Type:                    Random Forest")

            print(f"System Type:                        {n_classes}-way classifier\n")

            print("Accuracy:")
            print("    Best-guess accuracy:            {:.2f}%".format(best_guess))
            print("    Model accuracy:                 {:.2f}%".format(modelacc) + " (" + str(int(correct_count)) + "/" + str(count) + " correct)")
            print("    Improvement over best guess:    {:.2f}%".format(modelacc - best_guess) + " (of possible " + str(round(100 - best_guess, 2)) + "%)\n")

            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(correct_count * 100) / model_cap) / 100.0 * H) + " bits/bit")

            if report_cmat:
                max_cmat_entry_len = len(str(int(np.max(mtrx))))
                mtrx = np.concatenate((labels, mtrx.astype('str')), axis=1).astype('str')
                max_pred_len = (mtrx.shape[1] - 1) * max_cmat_entry_len + n_classes * 2 - 1
                print("\nConfusion Matrix:\n")
                print(cmat_template_1.format(pad("Actual", max_class_name_len, False), "Predicted"))
                print(cmat_template_1.format("-" * max_class_name_len, "-" * max(max_pred_len, 9)))
                for row in mtrx:
                    print(cmat_template_2.format(
                        *[pad(field, max_class_name_len if i == 0 else max_cmat_entry_len, False) for i, field in enumerate(row)]))

            print("\nAccuracy by Class:\n")
            print(acc_by_class_template_1.format(
                *[pad(header_field, length, False) for i, (header_field, length) in enumerate(zip(acc_by_class_header_fields, acc_by_class_lengths))]))
            print(acc_by_class_template_1.format(
                *["-" * length for length in acc_by_class_lengths]))

            pct_format_string = "{:8.2%}"      # width = 8, decimals = 2
            for raw_class in mapping.keys():
                class_stats = stats[int(mapping[raw_class])]
                TP, FP, TN, FN = class_stats.get('TP', None), class_stats.get('FP', None), class_stats.get('TN', None), class_stats.get('FN', None)
                TPR = pct_format_string.format(class_stats['TPR']) if class_stats['TPR'] is not None else 'N/A'
                TNR = pct_format_string.format(class_stats['TNR']) if class_stats['TNR'] is not None else 'N/A'
                PPV = pct_format_string.format(class_stats['PPV']) if class_stats['PPV'] is not None else 'N/A'
                NPV = pct_format_string.format(class_stats['NPV']) if class_stats['NPV'] is not None else 'N/A'
                F1 = pct_format_string.format(class_stats['F1']) if class_stats['F1'] is not None else 'N/A'
                TS = pct_format_string.format(class_stats['TS']) if class_stats['TS'] is not None else 'N/A'
                line_fields = [raw_class, TP, FP, TN, FN, TPR, TNR, PPV, NPV, F1, TS]
                print(acc_by_class_template_1.format(
                    *[pad(field, length, False) for i, (field, length) in enumerate(zip(line_fields, acc_by_class_lengths))]))


if __name__ == "__main__":
    try:
        __main()
    except PredictorError as e:
        print(e, file=sys.stderr)
        sys.exit(e.code)
    except Exception as e:
        print(f"An unknown exception of type {type(e).__name__} occurred.", file=sys.stderr)
        sys.exit(-1)
