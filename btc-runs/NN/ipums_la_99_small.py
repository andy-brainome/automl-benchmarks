#!/usr/bin/env python3
#
# This code has been produced by an enterprise version of Brainome(tm) licensed to: andy Stevko.
# Portions of this code copyright (c) 2019-2022 by Brainome, Inc. All Rights Reserved.
# Distribution and use of this code or commercial use is permitted within the license terms
# set forth in a written contractual agreement between Brainome, Inc and brainome-user.
# Please contact support@brainome.ai with any questions.
# Use of predictions results at your own risk.
#
# Output of Brainome v1.8-120-prod.
# Invocation: brainome TRAIN_TEST_SPLITS/kdd_ipums_la_99-small-clean-train.csv -f NN -y -split 70 -modelonly -q -o btc-runs/NN/ipums_la_99_small.py -json btc-runs/NN/ipums_la_99_small.json
# Total compiler execution time: 0:00:37.04. Finished on: Feb-26-2022 18:42:57.
# This source code requires Python 3.
#
"""

[01;1mPredictor:[0m                        btc-runs/NN/ipums_la_99_small.py
    Classifier Type:              Neural Network
    System Type:                  Binary classifier
    Training / Validation Split:  70% : 30%
    Accuracy:
      Best-guess accuracy:        93.57%
      Training accuracy:          93.58% (4054/4332 correct)
      Validation Accuracy:        93.43% (1736/1858 correct)
      Combined Model Accuracy:    93.53% (5790/6190 correct)


    Model Capacity (MEC):        117    bits
    Generalization Ratio:         11.91 bits/bit
    Percent of Data Memorized:    24.03%
    Resilience to Noise:          -1.54 dB







    Training Confusion Matrix:
              Actual | Predicted
              ------ | ---------
                   0 |  4054     0 
                   1 |   278     0 

    Validation Confusion Matrix:
              Actual | Predicted
              ------ | ---------
                   0 |  1734     4 
                   1 |   118     2 

    Training Accuracy by Class:
         binaryClass |    TP    FP    TN    FN     TPR      TNR      PPV      NPV       F1       TS 
         ----------- | ----- ----- ----- ----- -------- -------- -------- -------- -------- --------
                   0 |  4054   278     0     0  100.00%    0.00%   93.58%      N/A   96.68%   93.58%
                   1 |     0     0  4054   278    0.00%  100.00%      N/A   93.58%    0.00%    0.00%

    Validation Accuracy by Class:
         binaryClass |    TP    FP    TN    FN     TPR      TNR      PPV      NPV       F1       TS 
         ----------- | ----- ----- ----- ----- -------- -------- -------- -------- -------- --------
                   0 |  1734   118     2     4   99.77%    1.67%   93.63%   33.33%   96.60%   93.43%
                   1 |     2     4  1734   118    1.67%   99.77%   33.33%   93.63%    3.17%    1.61%




"""

import sys
import math
import argparse
import csv
import binascii
import faulthandler
import json
try:
    import numpy as np  # For numpy see: http://numpy.org
except ImportError as e:
    print("This predictor requires the Numpy library. Please run 'python3 -m pip install numpy'.", file=sys.stderr)
    raise e
try:
    from scipy.sparse import coo_matrix
    report_cmat = True
except ImportError:
    print("Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix. Try 'python3 -m pip install scipy'.", file=sys.stderr)
    report_cmat = False

IOBUF = 100000000
sys.setrecursionlimit(1000000)
TRAINFILE = ['TRAIN_TEST_SPLITS/kdd_ipums_la_99-small-clean-train.csv']
mapping = {'0': 0, '1': 1}
ignorelabels = []
ignorecolumns = []
list_of_cols_to_normalize = [1, 2, 3, 4, 5, 7, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 55]
column_mappings = [{1119304034.0: 0, 2146395540.0: 1, 3154319154.0: 2}, {1335132677.0: 0, 1795721447.0: 1, 4155630070.0: 2}, {4127462978.0: 0, 4163141499.0: 1}, {430339600.0: 0, 1684325040.0: 1, 2063764093.0: 2}, {5000.0: 0, 12500.0: 1, 17500.0: 2, 22500.0: 3, 27500.0: 4, 32500.0: 5, 37500.0: 6, 42499.0: 7, 47500.0: 8, 52500.0: 9, 57500.0: 10, 62500.0: 11, 67500.0: 12, 72500.0: 13, 77500.0: 14, 85000.0: 15, 95000.0: 16, 112500.0: 17, 137500.0: 18, 162500.0: 19, 187500.0: 20, 225000.0: 21, 275000.0: 22, 350000.0: 23, 400000.0: 24, 999999.0: 25}, {0.0: 0, 1.0: 1, 50.0: 2, 150.0: 3, 160.0: 4, 200.0: 5, 250.0: 6, 300.0: 7, 350.0: 8, 400.0: 9, 440.0: 10, 500.0: 11, 600.0: 12, 627.0: 13, 630.0: 14, 640.0: 15, 680.0: 16, 691.0: 17, 693.0: 18, 699.0: 19, 700.0: 20, 720.0: 21, 752.0: 22, 790.0: 23, 806.0: 24, 900.0: 25, 940.0: 26, 1000.0: 27, 1200.0: 28, 1285.0: 29, 1305.0: 30, 1310.0: 31, 1404.0: 32, 1450.0: 33, 1500.0: 34, 1595.0: 35, 1635.0: 36, 1689.0: 37, 1700.0: 38, 1800.0: 39, 1940.0: 40, 1946.0: 41, 2000.0: 42, 2005.0: 43, 2100.0: 44, 2160.0: 45, 2214.0: 46, 2229.0: 47, 2240.0: 48, 2300.0: 49, 2350.0: 50, 2400.0: 51, 2500.0: 52, 2625.0: 53, 2642.0: 54, 2650.0: 55, 2691.0: 56, 2800.0: 57, 2845.0: 58, 3000.0: 59, 3084.0: 60, 3129.0: 61, 3200.0: 62, 3212.0: 63, 3231.0: 64, 3300.0: 65, 3423.0: 66, 3431.0: 67, 3511.0: 68, 3588.0: 69, 3600.0: 70, 3648.0: 71, 3664.0: 72, 3700.0: 73, 3744.0: 74, 3780.0: 75, 3820.0: 76, 3824.0: 77, 3900.0: 78, 3982.0: 79, 3983.0: 80, 4000.0: 81, 4002.0: 82, 4008.0: 83, 4075.0: 84, 4120.0: 85, 4200.0: 86, 4224.0: 87, 4273.0: 88, 4480.0: 89, 4500.0: 90, 4507.0: 91, 4510.0: 92, 4700.0: 93, 4704.0: 94, 4800.0: 95, 4913.0: 96, 4944.0: 97, 4968.0: 98, 5000.0: 99, 5112.0: 100, 5134.0: 101, 5196.0: 102, 5200.0: 103, 5244.0: 104, 5250.0: 105, 5338.0: 106, 5400.0: 107, 5424.0: 108, 5500.0: 109, 5588.0: 110, 5600.0: 111, 5652.0: 112, 5700.0: 113, 5760.0: 114, 5800.0: 115, 5820.0: 116, 5864.0: 117, 5880.0: 118, 5920.0: 119, 6000.0: 120, 6009.0: 121, 6084.0: 122, 6096.0: 123, 6120.0: 124, 6200.0: 125, 6240.0: 126, 6276.0: 127, 6400.0: 128, 6414.0: 129, 6440.0: 130, 6442.0: 131, 6480.0: 132, 6500.0: 133, 6600.0: 134, 6640.0: 135, 6646.0: 136, 6704.0: 137, 6720.0: 138, 6744.0: 139, 6816.0: 140, 6840.0: 141, 6884.0: 142, 6910.0: 143, 6920.0: 144, 7000.0: 145, 7017.0: 146, 7032.0: 147, 7044.0: 148, 7060.0: 149, 7092.0: 150, 7100.0: 151, 7149.0: 152, 7152.0: 153, 7187.0: 154, 7195.0: 155, 7200.0: 156, 7224.0: 157, 7232.0: 158, 7249.0: 159, 7255.0: 160, 7267.0: 161, 7320.0: 162, 7378.0: 163, 7400.0: 164, 7440.0: 165, 7464.0: 166, 7479.0: 167, 7488.0: 168, 7492.0: 169, 7500.0: 170, 7525.0: 171, 7548.0: 172, 7560.0: 173, 7650.0: 174, 7700.0: 175, 7735.0: 176, 7740.0: 177, 7761.0: 178, 7764.0: 179, 7800.0: 180, 7822.0: 181, 7840.0: 182, 7850.0: 183, 7880.0: 184, 7884.0: 185, 7944.0: 186, 7956.0: 187, 7968.0: 188, 7997.0: 189, 8000.0: 190, 8016.0: 191, 8040.0: 192, 8100.0: 193, 8120.0: 194, 8134.0: 195, 8160.0: 196, 8180.0: 197, 8196.0: 198, 8200.0: 199, 8208.0: 200, 8291.0: 201, 8300.0: 202, 8328.0: 203, 8376.0: 204, 8391.0: 205, 8400.0: 206, 8425.0: 207, 8492.0: 208, 8500.0: 209, 8544.0: 210, 8558.0: 211, 8636.0: 212, 8640.0: 213, 8653.0: 214, 8664.0: 215, 8679.0: 216, 8700.0: 217, 8800.0: 218, 8808.0: 219, 8832.0: 220, 8840.0: 221, 8844.0: 222, 8900.0: 223, 8904.0: 224, 8928.0: 225, 8964.0: 226, 8988.0: 227, 9000.0: 228, 9010.0: 229, 9030.0: 230, 9075.0: 231, 9100.0: 232, 9120.0: 233, 9135.0: 234, 9170.0: 235, 9180.0: 236, 9228.0: 237, 9279.0: 238, 9358.0: 239, 9360.0: 240, 9394.0: 241, 9400.0: 242, 9404.0: 243, 9424.0: 244, 9460.0: 245, 9466.0: 246, 9500.0: 247, 9520.0: 248, 9577.0: 249, 9584.0: 250, 9600.0: 251, 9668.0: 252, 9700.0: 253, 9775.0: 254, 9800.0: 255, 9810.0: 256, 9887.0: 257, 9888.0: 258, 9890.0: 259, 9900.0: 260, 10000.0: 261, 10022.0: 262, 10080.0: 263, 10159.0: 264, 10195.0: 265, 10200.0: 266, 10211.0: 267, 10276.0: 268, 10296.0: 269, 10298.0: 270, 10385.0: 271, 10392.0: 272, 10394.0: 273, 10400.0: 274, 10500.0: 275, 10569.0: 276, 10600.0: 277, 10620.0: 278, 10650.0: 279, 10666.0: 280, 10680.0: 281, 10702.0: 282, 10720.0: 283, 10725.0: 284, 10800.0: 285, 10840.0: 286, 10850.0: 287, 10860.0: 288, 10916.0: 289, 10950.0: 290, 10980.0: 291, 11000.0: 292, 11068.0: 293, 11090.0: 294, 11150.0: 295, 11189.0: 296, 11213.0: 297, 11235.0: 298, 11250.0: 299, 11268.0: 300, 11270.0: 301, 11286.0: 302, 11294.0: 303, 11300.0: 304, 11371.0: 305, 11376.0: 306, 11440.0: 307, 11520.0: 308, 11537.0: 309, 11580.0: 310, 11600.0: 311, 11670.0: 312, 11697.0: 313, 11711.0: 314, 11712.0: 315, 11759.0: 316, 11785.0: 317, 11801.0: 318, 11923.0: 319, 12000.0: 320, 12012.0: 321, 12032.0: 322, 12037.0: 323, 12048.0: 324, 12060.0: 325, 12100.0: 326, 12132.0: 327, 12172.0: 328, 12200.0: 329, 12264.0: 330, 12280.0: 331, 12361.0: 332, 12367.0: 333, 12432.0: 334, 12437.0: 335, 12461.0: 336, 12500.0: 337, 12508.0: 338, 12513.0: 339, 12530.0: 340, 12600.0: 341, 12662.0: 342, 12683.0: 343, 12746.0: 344, 12815.0: 345, 12856.0: 346, 12864.0: 347, 13000.0: 348, 13005.0: 349, 13070.0: 350, 13092.0: 351, 13100.0: 352, 13115.0: 353, 13126.0: 354, 13221.0: 355, 13236.0: 356, 13272.0: 357, 13500.0: 358, 13533.0: 359, 13547.0: 360, 13550.0: 361, 13560.0: 362, 13600.0: 363, 13605.0: 364, 13620.0: 365, 13621.0: 366, 13653.0: 367, 13800.0: 368, 13824.0: 369, 13853.0: 370, 13876.0: 371, 13880.0: 372, 13900.0: 373, 13920.0: 374, 13960.0: 375, 14000.0: 376, 14004.0: 377, 14012.0: 378, 14056.0: 379, 14100.0: 380, 14121.0: 381, 14128.0: 382, 14150.0: 383, 14160.0: 384, 14170.0: 385, 14200.0: 386, 14300.0: 387, 14315.0: 388, 14358.0: 389, 14382.0: 390, 14400.0: 391, 14500.0: 392, 14560.0: 393, 14600.0: 394, 14628.0: 395, 14650.0: 396, 14700.0: 397, 14772.0: 398, 14777.0: 399, 14800.0: 400, 14803.0: 401, 14850.0: 402, 14916.0: 403, 14970.0: 404, 15000.0: 405, 15072.0: 406, 15080.0: 407, 15090.0: 408, 15112.0: 409, 15120.0: 410, 15132.0: 411, 15150.0: 412, 15153.0: 413, 15189.0: 414, 15200.0: 415, 15210.0: 416, 15250.0: 417, 15289.0: 418, 15309.0: 419, 15328.0: 420, 15360.0: 421, 15384.0: 422, 15399.0: 423, 15400.0: 424, 15405.0: 425, 15422.0: 426, 15450.0: 427, 15478.0: 428, 15500.0: 429, 15509.0: 430, 15525.0: 431, 15600.0: 432, 15620.0: 433, 15680.0: 434, 15698.0: 435, 15700.0: 436, 15730.0: 437, 15749.0: 438, 15750.0: 439, 15785.0: 440, 15846.0: 441, 15900.0: 442, 15912.0: 443, 15953.0: 444, 15955.0: 445, 15966.0: 446, 15972.0: 447, 16000.0: 448, 16034.0: 449, 16079.0: 450, 16090.0: 451, 16095.0: 452, 16127.0: 453, 16138.0: 454, 16145.0: 455, 16182.0: 456, 16280.0: 457, 16289.0: 458, 16355.0: 459, 16365.0: 460, 16400.0: 461, 16407.0: 462, 16469.0: 463, 16500.0: 464, 16526.0: 465, 16529.0: 466, 16560.0: 467, 16576.0: 468, 16587.0: 469, 16638.0: 470, 16640.0: 471, 16698.0: 472, 16736.0: 473, 16760.0: 474, 16788.0: 475, 16800.0: 476, 16876.0: 477, 16884.0: 478, 17000.0: 479, 17011.0: 480, 17022.0: 481, 17040.0: 482, 17060.0: 483, 17160.0: 484, 17180.0: 485, 17184.0: 486, 17189.0: 487, 17196.0: 488, 17200.0: 489, 17201.0: 490, 17240.0: 491, 17300.0: 492, 17328.0: 493, 17430.0: 494, 17500.0: 495, 17512.0: 496, 17517.0: 497, 17532.0: 498, 17568.0: 499, 17572.0: 500, 17630.0: 501, 17670.0: 502, 17717.0: 503, 17789.0: 504, 17800.0: 505, 17815.0: 506, 17840.0: 507, 17865.0: 508, 17867.0: 509, 17876.0: 510, 17892.0: 511, 17896.0: 512, 17928.0: 513, 18000.0: 514, 18028.0: 515, 18122.0: 516, 18150.0: 517, 18200.0: 518, 18209.0: 519, 18270.0: 520, 18294.0: 521, 18318.0: 522, 18352.0: 523, 18361.0: 524, 18382.0: 525, 18400.0: 526, 18480.0: 527, 18490.0: 528, 18500.0: 529, 18540.0: 530, 18550.0: 531, 18581.0: 532, 18600.0: 533, 18640.0: 534, 18683.0: 535, 18692.0: 536, 18700.0: 537, 18720.0: 538, 18730.0: 539, 18800.0: 540, 18900.0: 541, 18911.0: 542, 18960.0: 543, 19000.0: 544, 19100.0: 545, 19101.0: 546, 19125.0: 547, 19178.0: 548, 19189.0: 549, 19195.0: 550, 19200.0: 551, 19211.0: 552, 19212.0: 553, 19312.0: 554, 19359.0: 555, 19360.0: 556, 19385.0: 557, 19390.0: 558, 19440.0: 559, 19500.0: 560, 19510.0: 561, 19523.0: 562, 19543.0: 563, 19570.0: 564, 19590.0: 565, 19611.0: 566, 19624.0: 567, 19644.0: 568, 19672.0: 569, 19700.0: 570, 19752.0: 571, 19794.0: 572, 19800.0: 573, 19876.0: 574, 19900.0: 575, 20000.0: 576, 20001.0: 577, 20020.0: 578, 20030.0: 579, 20034.0: 580, 20050.0: 581, 20100.0: 582, 20111.0: 583, 20145.0: 584, 20200.0: 585, 20240.0: 586, 20248.0: 587, 20337.0: 588, 20400.0: 589, 20500.0: 590, 20583.0: 591, 20677.0: 592, 20699.0: 593, 20700.0: 594, 20725.0: 595, 20734.0: 596, 20796.0: 597, 20800.0: 598, 20820.0: 599, 20904.0: 600, 20971.0: 601, 21000.0: 602, 21010.0: 603, 21120.0: 604, 21156.0: 605, 21166.0: 606, 21200.0: 607, 21235.0: 608, 21260.0: 609, 21271.0: 610, 21340.0: 611, 21352.0: 612, 21492.0: 613, 21504.0: 614, 21520.0: 615, 21522.0: 616, 21600.0: 617, 21620.0: 618, 21767.0: 619, 21800.0: 620, 21820.0: 621, 21840.0: 622, 21900.0: 623, 22000.0: 624, 22012.0: 625, 22018.0: 626, 22025.0: 627, 22047.0: 628, 22080.0: 629, 22096.0: 630, 22114.0: 631, 22163.0: 632, 22199.0: 633, 22200.0: 634, 22212.0: 635, 22223.0: 636, 22230.0: 637, 22292.0: 638, 22340.0: 639, 22354.0: 640, 22387.0: 641, 22425.0: 642, 22460.0: 643, 22474.0: 644, 22500.0: 645, 22510.0: 646, 22534.0: 647, 22580.0: 648, 22584.0: 649, 22691.0: 650, 22700.0: 651, 22712.0: 652, 22749.0: 653, 22756.0: 654, 22762.0: 655, 22763.0: 656, 22765.0: 657, 22800.0: 658, 22824.0: 659, 22892.0: 660, 22976.0: 661, 23000.0: 662, 23008.0: 663, 23102.0: 664, 23137.0: 665, 23148.0: 666, 23157.0: 667, 23162.0: 668, 23200.0: 669, 23212.0: 670, 23222.0: 671, 23250.0: 672, 23299.0: 673, 23300.0: 674, 23340.0: 675, 23400.0: 676, 23403.0: 677, 23420.0: 678, 23444.0: 679, 23500.0: 680, 23509.0: 681, 23603.0: 682, 23604.0: 683, 23623.0: 684, 23630.0: 685, 23648.0: 686, 23678.0: 687, 23699.0: 688, 23700.0: 689, 23725.0: 690, 23733.0: 691, 23744.0: 692, 23756.0: 693, 23760.0: 694, 23775.0: 695, 23800.0: 696, 23804.0: 697, 23824.0: 698, 23832.0: 699, 23876.0: 700, 23912.0: 701, 24000.0: 702, 24026.0: 703, 24042.0: 704, 24062.0: 705, 24100.0: 706, 24109.0: 707, 24174.0: 708, 24200.0: 709, 24215.0: 710, 24325.0: 711, 24355.0: 712, 24357.0: 713, 24400.0: 714, 24473.0: 715, 24480.0: 716, 24500.0: 717, 24504.0: 718, 24600.0: 719, 24700.0: 720, 24761.0: 721, 24800.0: 722, 24820.0: 723, 24826.0: 724, 24875.0: 725, 24893.0: 726, 25000.0: 727, 25024.0: 728, 25065.0: 729, 25100.0: 730, 25107.0: 731, 25110.0: 732, 25125.0: 733, 25126.0: 734, 25159.0: 735, 25180.0: 736, 25189.0: 737, 25200.0: 738, 25207.0: 739, 25240.0: 740, 25242.0: 741, 25325.0: 742, 25331.0: 743, 25353.0: 744, 25400.0: 745, 25416.0: 746, 25461.0: 747, 25476.0: 748, 25480.0: 749, 25484.0: 750, 25500.0: 751, 25512.0: 752, 25523.0: 753, 25529.0: 754, 25536.0: 755, 25560.0: 756, 25600.0: 757, 25620.0: 758, 25632.0: 759, 25633.0: 760, 25646.0: 761, 25664.0: 762, 25700.0: 763, 25797.0: 764, 25800.0: 765, 25838.0: 766, 25920.0: 767, 25947.0: 768, 25950.0: 769, 25993.0: 770, 26000.0: 771, 26018.0: 772, 26066.0: 773, 26084.0: 774, 26090.0: 775, 26097.0: 776, 26100.0: 777, 26120.0: 778, 26152.0: 779, 26158.0: 780, 26200.0: 781, 26207.0: 782, 26241.0: 783, 26280.0: 784, 26302.0: 785, 26339.0: 786, 26360.0: 787, 26400.0: 788, 26418.0: 789, 26435.0: 790, 26461.0: 791, 26500.0: 792, 26512.0: 793, 26594.0: 794, 26600.0: 795, 26620.0: 796, 26626.0: 797, 26627.0: 798, 26648.0: 799, 26650.0: 800, 26665.0: 801, 26800.0: 802, 26812.0: 803, 26914.0: 804, 26998.0: 805, 27000.0: 806, 27060.0: 807, 27120.0: 808, 27132.0: 809, 27189.0: 810, 27200.0: 811, 27224.0: 812, 27225.0: 813, 27280.0: 814, 27300.0: 815, 27327.0: 816, 27350.0: 817, 27420.0: 818, 27429.0: 819, 27500.0: 820, 27560.0: 821, 27594.0: 822, 27600.0: 823, 27700.0: 824, 27750.0: 825, 27760.0: 826, 27769.0: 827, 27800.0: 828, 27803.0: 829, 27841.0: 830, 27847.0: 831, 27899.0: 832, 27908.0: 833, 27912.0: 834, 27932.0: 835, 27935.0: 836, 27960.0: 837, 27978.0: 838, 28000.0: 839, 28015.0: 840, 28024.0: 841, 28026.0: 842, 28051.0: 843, 28085.0: 844, 28086.0: 845, 28100.0: 846, 28125.0: 847, 28164.0: 848, 28200.0: 849, 28232.0: 850, 28240.0: 851, 28268.0: 852, 28285.0: 853, 28300.0: 854, 28322.0: 855, 28365.0: 856, 28368.0: 857, 28384.0: 858, 28394.0: 859, 28400.0: 860, 28472.0: 861, 28478.0: 862, 28500.0: 863, 28512.0: 864, 28561.0: 865, 28600.0: 866, 28641.0: 867, 28684.0: 868, 28700.0: 869, 28720.0: 870, 28770.0: 871, 28775.0: 872, 28830.0: 873, 28890.0: 874, 28900.0: 875, 28915.0: 876, 29000.0: 877, 29023.0: 878, 29049.0: 879, 29100.0: 880, 29130.0: 881, 29166.0: 882, 29190.0: 883, 29283.0: 884, 29304.0: 885, 29400.0: 886, 29449.0: 887, 29500.0: 888, 29560.0: 889, 29578.0: 890, 29600.0: 891, 29616.0: 892, 29681.0: 893, 29698.0: 894, 29700.0: 895, 29719.0: 896, 29728.0: 897, 29757.0: 898, 29759.0: 899, 29761.0: 900, 29775.0: 901, 29800.0: 902, 29808.0: 903, 29817.0: 904, 29900.0: 905, 29976.0: 906, 30000.0: 907, 30001.0: 908, 30028.0: 909, 30033.0: 910, 30067.0: 911, 30088.0: 912, 30120.0: 913, 30160.0: 914, 30189.0: 915, 30229.0: 916, 30248.0: 917, 30260.0: 918, 30300.0: 919, 30320.0: 920, 30370.0: 921, 30378.0: 922, 30400.0: 923, 30429.0: 924, 30440.0: 925, 30500.0: 926, 30528.0: 927, 30600.0: 928, 30617.0: 929, 30640.0: 930, 30671.0: 931, 30781.0: 932, 30788.0: 933, 30800.0: 934, 30870.0: 935, 30888.0: 936, 31000.0: 937, 31011.0: 938, 31039.0: 939, 31057.0: 940, 31067.0: 941, 31070.0: 942, 31100.0: 943, 31111.0: 944, 31150.0: 945, 31176.0: 946, 31262.0: 947, 31293.0: 948, 31300.0: 949, 31317.0: 950, 31400.0: 951, 31418.0: 952, 31445.0: 953, 31500.0: 954, 31520.0: 955, 31537.0: 956, 31542.0: 957, 31543.0: 958, 31544.0: 959, 31560.0: 960, 31564.0: 961, 31575.0: 962, 31712.0: 963, 31750.0: 964, 31850.0: 965, 31886.0: 966, 32000.0: 967, 32035.0: 968, 32060.0: 969, 32100.0: 970, 32116.0: 971, 32200.0: 972, 32240.0: 973, 32318.0: 974, 32400.0: 975, 32401.0: 976, 32404.0: 977, 32423.0: 978, 32440.0: 979, 32474.0: 980, 32476.0: 981, 32500.0: 982, 32513.0: 983, 32530.0: 984, 32575.0: 985, 32600.0: 986, 32620.0: 987, 32640.0: 988, 32733.0: 989, 32800.0: 990, 32801.0: 991, 32808.0: 992, 32830.0: 993, 32831.0: 994, 32870.0: 995, 32900.0: 996, 32920.0: 997, 32930.0: 998, 33000.0: 999, 33003.0: 1000, 33068.0: 1001, 33088.0: 1002, 33106.0: 1003, 33124.0: 1004, 33267.0: 1005, 33288.0: 1006, 33300.0: 1007, 33316.0: 1008, 33408.0: 1009, 33451.0: 1010, 33454.0: 1011, 33500.0: 1012, 33570.0: 1013, 33600.0: 1014, 33800.0: 1015, 33858.0: 1016, 33880.0: 1017, 33935.0: 1018, 33968.0: 1019, 33989.0: 1020, 33998.0: 1021, 34000.0: 1022, 34058.0: 1023, 34106.0: 1024, 34130.0: 1025, 34155.0: 1026, 34185.0: 1027, 34189.0: 1028, 34194.0: 1029, 34220.0: 1030, 34233.0: 1031, 34360.0: 1032, 34362.0: 1033, 34400.0: 1034, 34420.0: 1035, 34440.0: 1036, 34449.0: 1037, 34467.0: 1038, 34500.0: 1039, 34502.0: 1040, 34520.0: 1041, 34607.0: 1042, 34652.0: 1043, 34670.0: 1044, 34700.0: 1045, 34711.0: 1046, 34714.0: 1047, 34760.0: 1048, 34783.0: 1049, 34797.0: 1050, 34800.0: 1051, 34883.0: 1052, 34900.0: 1053, 34976.0: 1054, 35000.0: 1055, 35050.0: 1056, 35078.0: 1057, 35090.0: 1058, 35100.0: 1059, 35149.0: 1060, 35150.0: 1061, 35198.0: 1062, 35207.0: 1063, 35230.0: 1064, 35247.0: 1065, 35282.0: 1066, 35292.0: 1067, 35293.0: 1068, 35300.0: 1069, 35320.0: 1070, 35336.0: 1071, 35437.0: 1072, 35457.0: 1073, 35500.0: 1074, 35502.0: 1075, 35528.0: 1076, 35592.0: 1077, 35600.0: 1078, 35618.0: 1079, 35693.0: 1080, 35720.0: 1081, 35752.0: 1082, 35760.0: 1083, 35800.0: 1084, 35850.0: 1085, 35888.0: 1086, 35940.0: 1087, 35980.0: 1088, 36000.0: 1089, 36034.0: 1090, 36050.0: 1091, 36075.0: 1092, 36078.0: 1093, 36100.0: 1094, 36140.0: 1095, 36170.0: 1096, 36199.0: 1097, 36200.0: 1098, 36202.0: 1099, 36365.0: 1100, 36386.0: 1101, 36423.0: 1102, 36464.0: 1103, 36479.0: 1104, 36480.0: 1105, 36500.0: 1106, 36543.0: 1107, 36547.0: 1108, 36565.0: 1109, 36580.0: 1110, 36662.0: 1111, 36696.0: 1112, 36700.0: 1113, 36706.0: 1114, 36734.0: 1115, 36761.0: 1116, 36796.0: 1117, 36800.0: 1118, 36806.0: 1119, 36843.0: 1120, 36850.0: 1121, 36864.0: 1122, 37000.0: 1123, 37098.0: 1124, 37118.0: 1125, 37200.0: 1126, 37240.0: 1127, 37268.0: 1128, 37366.0: 1129, 37380.0: 1130, 37397.0: 1131, 37431.0: 1132, 37500.0: 1133, 37509.0: 1134, 37588.0: 1135, 37600.0: 1136, 37612.0: 1137, 37683.0: 1138, 37700.0: 1139, 37731.0: 1140, 37769.0: 1141, 37844.0: 1142, 37900.0: 1143, 38000.0: 1144, 38008.0: 1145, 38118.0: 1146, 38160.0: 1147, 38200.0: 1148, 38214.0: 1149, 38298.0: 1150, 38335.0: 1151, 38400.0: 1152, 38442.0: 1153, 38500.0: 1154, 38550.0: 1155, 38574.0: 1156, 38590.0: 1157, 38600.0: 1158, 38613.0: 1159, 38620.0: 1160, 38687.0: 1161, 38759.0: 1162, 38800.0: 1163, 38900.0: 1164, 38922.0: 1165, 38942.0: 1166, 38950.0: 1167, 38976.0: 1168, 38990.0: 1169, 39000.0: 1170, 39020.0: 1171, 39052.0: 1172, 39075.0: 1173, 39200.0: 1174, 39300.0: 1175, 39320.0: 1176, 39361.0: 1177, 39400.0: 1178, 39449.0: 1179, 39476.0: 1180, 39494.0: 1181, 39500.0: 1182, 39522.0: 1183, 39549.0: 1184, 39600.0: 1185, 39611.0: 1186, 39625.0: 1187, 39700.0: 1188, 39762.0: 1189, 39766.0: 1190, 39839.0: 1191, 39951.0: 1192, 39956.0: 1193, 40000.0: 1194, 40001.0: 1195, 40090.0: 1196, 40091.0: 1197, 40100.0: 1198, 40138.0: 1199, 40149.0: 1200, 40163.0: 1201, 40245.0: 1202, 40248.0: 1203, 40272.0: 1204, 40284.0: 1205, 40350.0: 1206, 40400.0: 1207, 40442.0: 1208, 40469.0: 1209, 40500.0: 1210, 40596.0: 1211, 40600.0: 1212, 40624.0: 1213, 40700.0: 1214, 40727.0: 1215, 40730.0: 1216, 40800.0: 1217, 40912.0: 1218, 40920.0: 1219, 40994.0: 1220, 40995.0: 1221, 41000.0: 1222, 41120.0: 1223, 41154.0: 1224, 41200.0: 1225, 41383.0: 1226, 41400.0: 1227, 41420.0: 1228, 41450.0: 1229, 41489.0: 1230, 41626.0: 1231, 41682.0: 1232, 41700.0: 1233, 41707.0: 1234, 41800.0: 1235, 42000.0: 1236, 42040.0: 1237, 42063.0: 1238, 42097.0: 1239, 42140.0: 1240, 42200.0: 1241, 42240.0: 1242, 42250.0: 1243, 42389.0: 1244, 42400.0: 1245, 42403.0: 1246, 42475.0: 1247, 42476.0: 1248, 42500.0: 1249, 42600.0: 1250, 42605.0: 1251, 42648.0: 1252, 42686.0: 1253, 42738.0: 1254, 42800.0: 1255, 42834.0: 1256, 42883.0: 1257, 42904.0: 1258, 42910.0: 1259, 43000.0: 1260, 43038.0: 1261, 43046.0: 1262, 43105.0: 1263, 43200.0: 1264, 43300.0: 1265, 43327.0: 1266, 43400.0: 1267, 43500.0: 1268, 43533.0: 1269, 43561.0: 1270, 43608.0: 1271, 43676.0: 1272, 43690.0: 1273, 43709.0: 1274, 43728.0: 1275, 43765.0: 1276, 43800.0: 1277, 43868.0: 1278, 43900.0: 1279, 44000.0: 1280, 44035.0: 1281, 44100.0: 1282, 44118.0: 1283, 44132.0: 1284, 44150.0: 1285, 44193.0: 1286, 44220.0: 1287, 44339.0: 1288, 44400.0: 1289, 44440.0: 1290, 44472.0: 1291, 44500.0: 1292, 44600.0: 1293, 44641.0: 1294, 44800.0: 1295, 44805.0: 1296, 44822.0: 1297, 44900.0: 1298, 44904.0: 1299, 44960.0: 1300, 45000.0: 1301, 45014.0: 1302, 45020.0: 1303, 45100.0: 1304, 45122.0: 1305, 45133.0: 1306, 45200.0: 1307, 45250.0: 1308, 45272.0: 1309, 45393.0: 1310, 45400.0: 1311, 45444.0: 1312, 45500.0: 1313, 45507.0: 1314, 45550.0: 1315, 45567.0: 1316, 45595.0: 1317, 45609.0: 1318, 45648.0: 1319, 45655.0: 1320, 45749.0: 1321, 45750.0: 1322, 45760.0: 1323, 45795.0: 1324, 45900.0: 1325, 46000.0: 1326, 46008.0: 1327, 46100.0: 1328, 46200.0: 1329, 46234.0: 1330, 46280.0: 1331, 46297.0: 1332, 46300.0: 1333, 46341.0: 1334, 46349.0: 1335, 46378.0: 1336, 46384.0: 1337, 46431.0: 1338, 46500.0: 1339, 46540.0: 1340, 46573.0: 1341, 46576.0: 1342, 46588.0: 1343, 46641.0: 1344, 46650.0: 1345, 46720.0: 1346, 46731.0: 1347, 46742.0: 1348, 46800.0: 1349, 46820.0: 1350, 46878.0: 1351, 46966.0: 1352, 47000.0: 1353, 47048.0: 1354, 47103.0: 1355, 47189.0: 1356, 47269.0: 1357, 47350.0: 1358, 47371.0: 1359, 47500.0: 1360, 47540.0: 1361, 47550.0: 1362, 47584.0: 1363, 47600.0: 1364, 47672.0: 1365, 47693.0: 1366, 47695.0: 1367, 47704.0: 1368, 47708.0: 1369, 47800.0: 1370, 47840.0: 1371, 47852.0: 1372, 47900.0: 1373, 47901.0: 1374, 47947.0: 1375, 47958.0: 1376, 47970.0: 1377, 47972.0: 1378, 48000.0: 1379, 48110.0: 1380, 48134.0: 1381, 48200.0: 1382, 48224.0: 1383, 48247.0: 1384, 48272.0: 1385, 48373.0: 1386, 48400.0: 1387, 48440.0: 1388, 48441.0: 1389, 48459.0: 1390, 48520.0: 1391, 48550.0: 1392, 48556.0: 1393, 48600.0: 1394, 48615.0: 1395, 48620.0: 1396, 48700.0: 1397, 48716.0: 1398, 48800.0: 1399, 48856.0: 1400, 48882.0: 1401, 48900.0: 1402, 48993.0: 1403, 49000.0: 1404, 49050.0: 1405, 49100.0: 1406, 49200.0: 1407, 49350.0: 1408, 49369.0: 1409, 49500.0: 1410, 49531.0: 1411, 49548.0: 1412, 49560.0: 1413, 49565.0: 1414, 49600.0: 1415, 49699.0: 1416, 49800.0: 1417, 49857.0: 1418, 49900.0: 1419, 49912.0: 1420, 49935.0: 1421, 49943.0: 1422, 49994.0: 1423, 50000.0: 1424, 50049.0: 1425, 50062.0: 1426, 50085.0: 1427, 50100.0: 1428, 50160.0: 1429, 50202.0: 1430, 50300.0: 1431, 50306.0: 1432, 50338.0: 1433, 50339.0: 1434, 50347.0: 1435, 50360.0: 1436, 50397.0: 1437, 50400.0: 1438, 50484.0: 1439, 50500.0: 1440, 50515.0: 1441, 50531.0: 1442, 50540.0: 1443, 50690.0: 1444, 50715.0: 1445, 50720.0: 1446, 50763.0: 1447, 50800.0: 1448, 50908.0: 1449, 50960.0: 1450, 50988.0: 1451, 51000.0: 1452, 51052.0: 1453, 51080.0: 1454, 51250.0: 1455, 51269.0: 1456, 51358.0: 1457, 51390.0: 1458, 51400.0: 1459, 51450.0: 1460, 51532.0: 1461, 51546.0: 1462, 51600.0: 1463, 51670.0: 1464, 51726.0: 1465, 51749.0: 1466, 51882.0: 1467, 51943.0: 1468, 52000.0: 1469, 52040.0: 1470, 52086.0: 1471, 52092.0: 1472, 52168.0: 1473, 52200.0: 1474, 52233.0: 1475, 52328.0: 1476, 52410.0: 1477, 52500.0: 1478, 52512.0: 1479, 52591.0: 1480, 52598.0: 1481, 52602.0: 1482, 52630.0: 1483, 52632.0: 1484, 52710.0: 1485, 52744.0: 1486, 52900.0: 1487, 52950.0: 1488, 53000.0: 1489, 53050.0: 1490, 53140.0: 1491, 53158.0: 1492, 53222.0: 1493, 53243.0: 1494, 53300.0: 1495, 53310.0: 1496, 53340.0: 1497, 53500.0: 1498, 53532.0: 1499, 53571.0: 1500, 53600.0: 1501, 53820.0: 1502, 53920.0: 1503, 53922.0: 1504, 53976.0: 1505, 53979.0: 1506, 54000.0: 1507, 54047.0: 1508, 54117.0: 1509, 54197.0: 1510, 54222.0: 1511, 54300.0: 1512, 54307.0: 1513, 54332.0: 1514, 54333.0: 1515, 54334.0: 1516, 54400.0: 1517, 54600.0: 1518, 54635.0: 1519, 54648.0: 1520, 54698.0: 1521, 54714.0: 1522, 54719.0: 1523, 54748.0: 1524, 54750.0: 1525, 54831.0: 1526, 54900.0: 1527, 54950.0: 1528, 54962.0: 1529, 54989.0: 1530, 55000.0: 1531, 55009.0: 1532, 55069.0: 1533, 55152.0: 1534, 55200.0: 1535, 55300.0: 1536, 55320.0: 1537, 55385.0: 1538, 55440.0: 1539, 55450.0: 1540, 55570.0: 1541, 55590.0: 1542, 55600.0: 1543, 55691.0: 1544, 55756.0: 1545, 55800.0: 1546, 55960.0: 1547, 55967.0: 1548, 56000.0: 1549, 56030.0: 1550, 56090.0: 1551, 56107.0: 1552, 56184.0: 1553, 56200.0: 1554, 56300.0: 1555, 56350.0: 1556, 56380.0: 1557, 56385.0: 1558, 56400.0: 1559, 56508.0: 1560, 56600.0: 1561, 56641.0: 1562, 56700.0: 1563, 56702.0: 1564, 56740.0: 1565, 56789.0: 1566, 56799.0: 1567, 56800.0: 1568, 56826.0: 1569, 56829.0: 1570, 56894.0: 1571, 56921.0: 1572, 56940.0: 1573, 56965.0: 1574, 57000.0: 1575, 57012.0: 1576, 57044.0: 1577, 57090.0: 1578, 57100.0: 1579, 57181.0: 1580, 57188.0: 1581, 57200.0: 1582, 57418.0: 1583, 57500.0: 1584, 57510.0: 1585, 57566.0: 1586, 57587.0: 1587, 57600.0: 1588, 57671.0: 1589, 57746.0: 1590, 57800.0: 1591, 57840.0: 1592, 57890.0: 1593, 57911.0: 1594, 57936.0: 1595, 57971.0: 1596, 57973.0: 1597, 57985.0: 1598, 58000.0: 1599, 58280.0: 1600, 58300.0: 1601, 58330.0: 1602, 58359.0: 1603, 58379.0: 1604, 58400.0: 1605, 58425.0: 1606, 58500.0: 1607, 58600.0: 1608, 58603.0: 1609, 58649.0: 1610, 58750.0: 1611, 58753.0: 1612, 58761.0: 1613, 58819.0: 1614, 58900.0: 1615, 58963.0: 1616, 58966.0: 1617, 59000.0: 1618, 59020.0: 1619, 59100.0: 1620, 59161.0: 1621, 59186.0: 1622, 59204.0: 1623, 59495.0: 1624, 59500.0: 1625, 59547.0: 1626, 59600.0: 1627, 59672.0: 1628, 59709.0: 1629, 59750.0: 1630, 59776.0: 1631, 59800.0: 1632, 60000.0: 1633, 60100.0: 1634, 60137.0: 1635, 60200.0: 1636, 60250.0: 1637, 60256.0: 1638, 60327.0: 1639, 60500.0: 1640, 60528.0: 1641, 60580.0: 1642, 60600.0: 1643, 60631.0: 1644, 60650.0: 1645, 60683.0: 1646, 60700.0: 1647, 60760.0: 1648, 60817.0: 1649, 60980.0: 1650, 61000.0: 1651, 61030.0: 1652, 61100.0: 1653, 61138.0: 1654, 61159.0: 1655, 61165.0: 1656, 61200.0: 1657, 61234.0: 1658, 61300.0: 1659, 61342.0: 1660, 61355.0: 1661, 61401.0: 1662, 61431.0: 1663, 61438.0: 1664, 61526.0: 1665, 61545.0: 1666, 61672.0: 1667, 61700.0: 1668, 61760.0: 1669, 61844.0: 1670, 61906.0: 1671, 62000.0: 1672, 62200.0: 1673, 62320.0: 1674, 62400.0: 1675, 62454.0: 1676, 62500.0: 1677, 62585.0: 1678, 62640.0: 1679, 62731.0: 1680, 62742.0: 1681, 62794.0: 1682, 62800.0: 1683, 62900.0: 1684, 62999.0: 1685, 63000.0: 1686, 63014.0: 1687, 63060.0: 1688, 63100.0: 1689, 63350.0: 1690, 63498.0: 1691, 63511.0: 1692, 63515.0: 1693, 63603.0: 1694, 63673.0: 1695, 63803.0: 1696, 63878.0: 1697, 64000.0: 1698, 64036.0: 1699, 64085.0: 1700, 64136.0: 1701, 64169.0: 1702, 64200.0: 1703, 64265.0: 1704, 64373.0: 1705, 64400.0: 1706, 64600.0: 1707, 64613.0: 1708, 64696.0: 1709, 64700.0: 1710, 64724.0: 1711, 64731.0: 1712, 64733.0: 1713, 64800.0: 1714, 64816.0: 1715, 64900.0: 1716, 64958.0: 1717, 65000.0: 1718, 65080.0: 1719, 65200.0: 1720, 65239.0: 1721, 65300.0: 1722, 65400.0: 1723, 65488.0: 1724, 65500.0: 1725, 65600.0: 1726, 65624.0: 1727, 65679.0: 1728, 65710.0: 1729, 65748.0: 1730, 65750.0: 1731, 65772.0: 1732, 65879.0: 1733, 66000.0: 1734, 66100.0: 1735, 66152.0: 1736, 66257.0: 1737, 66500.0: 1738, 66603.0: 1739, 66643.0: 1740, 66655.0: 1741, 66700.0: 1742, 66775.0: 1743, 66796.0: 1744, 66857.0: 1745, 66882.0: 1746, 66916.0: 1747, 67000.0: 1748, 67020.0: 1749, 67034.0: 1750, 67111.0: 1751, 67134.0: 1752, 67173.0: 1753, 67216.0: 1754, 67293.0: 1755, 67361.0: 1756, 67364.0: 1757, 67398.0: 1758, 67513.0: 1759, 67555.0: 1760, 67897.0: 1761, 68000.0: 1762, 68020.0: 1763, 68095.0: 1764, 68107.0: 1765, 68151.0: 1766, 68160.0: 1767, 68198.0: 1768, 68203.0: 1769, 68215.0: 1770, 68224.0: 1771, 68323.0: 1772, 68500.0: 1773, 68519.0: 1774, 68540.0: 1775, 68600.0: 1776, 69000.0: 1777, 69113.0: 1778, 69115.0: 1779, 69228.0: 1780, 69316.0: 1781, 69382.0: 1782, 69500.0: 1783, 69530.0: 1784, 69603.0: 1785, 69643.0: 1786, 69700.0: 1787, 70000.0: 1788, 70043.0: 1789, 70090.0: 1790, 70126.0: 1791, 70146.0: 1792, 70165.0: 1793, 70189.0: 1794, 70284.0: 1795, 70350.0: 1796, 70700.0: 1797, 70735.0: 1798, 70750.0: 1799, 70900.0: 1800, 71000.0: 1801, 71204.0: 1802, 71300.0: 1803, 71461.0: 1804, 71501.0: 1805, 71603.0: 1806, 71796.0: 1807, 71850.0: 1808, 71876.0: 1809, 71900.0: 1810, 72000.0: 1811, 72303.0: 1812, 72400.0: 1813, 72500.0: 1814, 72650.0: 1815, 72778.0: 1816, 72820.0: 1817, 72904.0: 1818, 72932.0: 1819, 72993.0: 1820, 73000.0: 1821, 73002.0: 1822, 73200.0: 1823, 73450.0: 1824, 73590.0: 1825, 73673.0: 1826, 73687.0: 1827, 73689.0: 1828, 73792.0: 1829, 73966.0: 1830, 74000.0: 1831, 74193.0: 1832, 74216.0: 1833, 74259.0: 1834, 74300.0: 1835, 74405.0: 1836, 74466.0: 1837, 74595.0: 1838, 74600.0: 1839, 74828.0: 1840, 74886.0: 1841, 75000.0: 1842, 75048.0: 1843, 75189.0: 1844, 75395.0: 1845, 75500.0: 1846, 75579.0: 1847, 75600.0: 1848, 75746.0: 1849, 76000.0: 1850, 76152.0: 1851, 76200.0: 1852, 76368.0: 1853, 76400.0: 1854, 76428.0: 1855, 76900.0: 1856, 77000.0: 1857, 77029.0: 1858, 77030.0: 1859, 77040.0: 1860, 77076.0: 1861, 77200.0: 1862, 77250.0: 1863, 77400.0: 1864, 77500.0: 1865, 77512.0: 1866, 77525.0: 1867, 77731.0: 1868, 77783.0: 1869, 78000.0: 1870, 78005.0: 1871, 78050.0: 1872, 78100.0: 1873, 78200.0: 1874, 78339.0: 1875, 78355.0: 1876, 78374.0: 1877, 78403.0: 1878, 78659.0: 1879, 78728.0: 1880, 78780.0: 1881, 78800.0: 1882, 78830.0: 1883, 78950.0: 1884, 79000.0: 1885, 79010.0: 1886, 79287.0: 1887, 79600.0: 1888, 79699.0: 1889, 79762.0: 1890, 79879.0: 1891, 79885.0: 1892, 79900.0: 1893, 80000.0: 1894, 80051.0: 1895, 80116.0: 1896, 80150.0: 1897, 80166.0: 1898, 80300.0: 1899, 80400.0: 1900, 80600.0: 1901, 80752.0: 1902, 80760.0: 1903, 80800.0: 1904, 80960.0: 1905, 81000.0: 1906, 81085.0: 1907, 81200.0: 1908, 81500.0: 1909, 81610.0: 1910, 81800.0: 1911, 82000.0: 1912, 82150.0: 1913, 82200.0: 1914, 82600.0: 1915, 82603.0: 1916, 82670.0: 1917, 82800.0: 1918, 83000.0: 1919, 83003.0: 1920, 83025.0: 1921, 83029.0: 1922, 83043.0: 1923, 83130.0: 1924, 83300.0: 1925, 83391.0: 1926, 83400.0: 1927, 83556.0: 1928, 83600.0: 1929, 83607.0: 1930, 84000.0: 1931, 84100.0: 1932, 84200.0: 1933, 84400.0: 1934, 84500.0: 1935, 84544.0: 1936, 84600.0: 1937, 84843.0: 1938, 84855.0: 1939, 84892.0: 1940, 85000.0: 1941, 85021.0: 1942, 85042.0: 1943, 85194.0: 1944, 85500.0: 1945, 85550.0: 1946, 85612.0: 1947, 85749.0: 1948, 85800.0: 1949, 85859.0: 1950, 85975.0: 1951, 86000.0: 1952, 86042.0: 1953, 86176.0: 1954, 86238.0: 1955, 86273.0: 1956, 86377.0: 1957, 86398.0: 1958, 86500.0: 1959, 86608.0: 1960, 86730.0: 1961, 86800.0: 1962, 86851.0: 1963, 87000.0: 1964, 87100.0: 1965, 87200.0: 1966, 87300.0: 1967, 87645.0: 1968, 87980.0: 1969, 88000.0: 1970, 88200.0: 1971, 88241.0: 1972, 88253.0: 1973, 88397.0: 1974, 88405.0: 1975, 88500.0: 1976, 88800.0: 1977, 88986.0: 1978, 89000.0: 1979, 89063.0: 1980, 89096.0: 1981, 89447.0: 1982, 89590.0: 1983, 89600.0: 1984, 89800.0: 1985, 90000.0: 1986, 90168.0: 1987, 90193.0: 1988, 90200.0: 1989, 90300.0: 1990, 90430.0: 1991, 90500.0: 1992, 90600.0: 1993, 90866.0: 1994, 90929.0: 1995, 91000.0: 1996, 91196.0: 1997, 91290.0: 1998, 91447.0: 1999, 91593.0: 2000, 91873.0: 2001, 91916.0: 2002, 92000.0: 2003, 92006.0: 2004, 92664.0: 2005, 93000.0: 2006, 93347.0: 2007, 93406.0: 2008, 93501.0: 2009, 93600.0: 2010, 94000.0: 2011, 94128.0: 2012, 94440.0: 2013, 94892.0: 2014, 94958.0: 2015, 95000.0: 2016, 95400.0: 2017, 95760.0: 2018, 95800.0: 2019, 95821.0: 2020, 95934.0: 2021, 96000.0: 2022, 96054.0: 2023, 96228.0: 2024, 96334.0: 2025, 96380.0: 2026, 96509.0: 2027, 96732.0: 2028, 97000.0: 2029, 97070.0: 2030, 97200.0: 2031, 97400.0: 2032, 97720.0: 2033, 98000.0: 2034, 98603.0: 2035, 98674.0: 2036, 99000.0: 2037, 99003.0: 2038, 99506.0: 2039, 99700.0: 2040, 100000.0: 2041, 100035.0: 2042, 100320.0: 2043, 100400.0: 2044, 101000.0: 2045, 101200.0: 2046, 101491.0: 2047, 101540.0: 2048, 101600.0: 2049, 101603.0: 2050, 102000.0: 2051, 102019.0: 2052, 102100.0: 2053, 102125.0: 2054, 103000.0: 2055, 103265.0: 2056, 103603.0: 2057, 103917.0: 2058, 104000.0: 2059, 104145.0: 2060, 104200.0: 2061, 104603.0: 2062, 105000.0: 2063, 105200.0: 2064, 105401.0: 2065, 105563.0: 2066, 105708.0: 2067, 106000.0: 2068, 106600.0: 2069, 106603.0: 2070, 107000.0: 2071, 107200.0: 2072, 107300.0: 2073, 107800.0: 2074, 108000.0: 2075, 108500.0: 2076, 108776.0: 2077, 108841.0: 2078, 109100.0: 2079, 109600.0: 2080, 109700.0: 2081, 109893.0: 2082, 110000.0: 2083, 110150.0: 2084, 110700.0: 2085, 111000.0: 2086, 111400.0: 2087, 111513.0: 2088, 111750.0: 2089, 111810.0: 2090, 112000.0: 2091, 112500.0: 2092, 112617.0: 2093, 112870.0: 2094, 113000.0: 2095, 113036.0: 2096, 113241.0: 2097, 114012.0: 2098, 114248.0: 2099, 114500.0: 2100, 114580.0: 2101, 114700.0: 2102, 115000.0: 2103, 115250.0: 2104, 115400.0: 2105, 116000.0: 2106, 116012.0: 2107, 116600.0: 2108, 116649.0: 2109, 116931.0: 2110, 117000.0: 2111, 117252.0: 2112, 118000.0: 2113, 118032.0: 2114, 118086.0: 2115, 118450.0: 2116, 118603.0: 2117, 118674.0: 2118, 118900.0: 2119, 119000.0: 2120, 119300.0: 2121, 119584.0: 2122, 119703.0: 2123, 120000.0: 2124, 120001.0: 2125, 120300.0: 2126, 122000.0: 2127, 122200.0: 2128, 123515.0: 2129, 123722.0: 2130, 123800.0: 2131, 124005.0: 2132, 124103.0: 2133, 124200.0: 2134, 124315.0: 2135, 124506.0: 2136, 124615.0: 2137, 125000.0: 2138, 125515.0: 2139, 125600.0: 2140, 126261.0: 2141, 126300.0: 2142, 126351.0: 2143, 126800.0: 2144, 126816.0: 2145, 127198.0: 2146, 127250.0: 2147, 127974.0: 2148, 128515.0: 2149, 128603.0: 2150, 128702.0: 2151, 129000.0: 2152, 129791.0: 2153, 130000.0: 2154, 130603.0: 2155, 132000.0: 2156, 132806.0: 2157, 133500.0: 2158, 133756.0: 2159, 135000.0: 2160, 135471.0: 2161, 135500.0: 2162, 135715.0: 2163, 136676.0: 2164, 136708.0: 2165, 137000.0: 2166, 138350.0: 2167, 138831.0: 2168, 138959.0: 2169, 139300.0: 2170, 140020.0: 2171, 141000.0: 2172, 141300.0: 2173, 141691.0: 2174, 143000.0: 2175, 143515.0: 2176, 143816.0: 2177, 143840.0: 2178, 146403.0: 2179, 146832.0: 2180, 147731.0: 2181, 150000.0: 2182, 150440.0: 2183, 150458.0: 2184, 150800.0: 2185, 151500.0: 2186, 153000.0: 2187, 153515.0: 2188, 155515.0: 2189, 156327.0: 2190, 156755.0: 2191, 157206.0: 2192, 158500.0: 2193, 158603.0: 2194, 159779.0: 2195, 160201.0: 2196, 160300.0: 2197, 163000.0: 2198, 165815.0: 2199, 166822.0: 2200, 167000.0: 2201, 167348.0: 2202, 168000.0: 2203, 169331.0: 2204, 169411.0: 2205, 172746.0: 2206, 175215.0: 2207, 178515.0: 2208, 179015.0: 2209, 180000.0: 2210, 181115.0: 2211, 183515.0: 2212, 184400.0: 2213, 185015.0: 2214, 188000.0: 2215, 189115.0: 2216, 190603.0: 2217, 191103.0: 2218, 195516.0: 2219, 196184.0: 2220, 196716.0: 2221, 197116.0: 2222, 197395.0: 2223, 198516.0: 2224, 198616.0: 2225, 199815.0: 2226, 201016.0: 2227, 201316.0: 2228, 202516.0: 2229, 205016.0: 2230, 205049.0: 2231, 205516.0: 2232, 205916.0: 2233, 208516.0: 2234, 209500.0: 2235, 210516.0: 2236, 210966.0: 2237, 211686.0: 2238, 212076.0: 2239, 212284.0: 2240, 213516.0: 2241, 214716.0: 2242, 215516.0: 2243, 216087.0: 2244, 216716.0: 2245, 219616.0: 2246, 220516.0: 2247, 222000.0: 2248, 222839.0: 2249, 223216.0: 2250, 223515.0: 2251, 225000.0: 2252, 225516.0: 2253, 226316.0: 2254, 227125.0: 2255, 227716.0: 2256, 230060.0: 2257, 232646.0: 2258, 235409.0: 2259, 235516.0: 2260, 238516.0: 2261, 240516.0: 2262, 242603.0: 2263, 247030.0: 2264, 254119.0: 2265, 254757.0: 2266, 255516.0: 2267, 256334.0: 2268, 260000.0: 2269, 260463.0: 2270, 260516.0: 2271, 260816.0: 2272, 263768.0: 2273, 264000.0: 2274, 265000.0: 2275, 266516.0: 2276, 273308.0: 2277, 274119.0: 2278, 279516.0: 2279, 294119.0: 2280, 294516.0: 2281, 295842.0: 2282, 296516.0: 2283, 318309.0: 2284, 320709.0: 2285, 322603.0: 2286, 324516.0: 2287, 326031.0: 2288, 333831.0: 2289, 343618.0: 2290, 356231.0: 2291, 384031.0: 2292, 391032.0: 2293, 395116.0: 2294, 999999.0: 2295, 65959.0: 2296, 846.0: 2297, 28340.0: 2298, 53295.0: 2299, 84300.0: 2300, 29079.0: 2301, 47300.0: 2302, 2018.0: 2303, 211000.0: 2304, 63620.0: 2305, 40164.0: 2306, 26816.0: 2307, 119046.0: 2308, 77007.0: 2309, 49450.0: 2310, 80950.0: 2311, 194000.0: 2312, 17820.0: 2313, 117350.0: 2314, 7908.0: 2315, 95603.0: 2316, 22882.0: 2317, 56861.0: 2318, 19600.0: 2319, 142000.0: 2320, 75980.0: 2321, 16600.0: 2322, 58135.0: 2323, 67600.0: 2324, 28130.0: 2325, 13772.0: 2326, 13468.0: 2327, 13885.0: 2328, 29450.0: 2329, 11950.0: 2330, 124500.0: 2331, 37800.0: 2332, 6445.0: 2333, 85390.0: 2334, 18384.0: 2335, 110762.0: 2336, 29317.0: 2337, 32250.0: 2338, 15385.0: 2339, 28927.0: 2340, 31800.0: 2341, 14240.0: 2342, 8398.0: 2343, 13190.0: 2344, 127000.0: 2345, 210716.0: 2346, 66082.0: 2347, 11900.0: 2348, 55652.0: 2349, 13908.0: 2350, 116772.0: 2351, 14680.0: 2352, 200516.0: 2353, 36160.0: 2354, 12428.0: 2355, 34034.0: 2356, 8758.0: 2357, 16277.0: 2358, 45150.0: 2359, 30436.0: 2360, 51223.0: 2361, 23090.0: 2362, 83034.0: 2363, 34988.0: 2364, 56524.0: 2365, 25744.0: 2366, 37880.0: 2367, 47700.0: 2368, 3400.0: 2369, 13680.0: 2370, 55876.0: 2371, 19450.0: 2372, 21300.0: 2373, 101124.0: 2374, 37981.0: 2375, 28066.0: 2376, 39502.0: 2377, 39614.0: 2378, 33662.0: 2379, 99100.0: 2380, 66745.0: 2381, 21584.0: 2382, 18335.0: 2383, 262676.0: 2384, 55868.0: 2385, 5787.0: 2386, 13325.0: 2387, 19501.0: 2388, 27977.0: 2389, 108200.0: 2390, 72800.0: 2391, 15839.0: 2392, 35400.0: 2393, 23100.0: 2394, 242117.0: 2395, 94820.0: 2396, 82140.0: 2397, 28876.0: 2398, 12268.0: 2399, 132940.0: 2400, 15029.0: 2401, 47260.0: 2402, 70500.0: 2403, 199000.0: 2404, 58700.0: 2405, 64147.0: 2406, 7244.0: 2407, 10158.0: 2408, 8721.0: 2409, 49491.0: 2410, 70684.0: 2411, 54430.0: 2412, 97600.0: 2413, 67283.0: 2414, 25760.0: 2415, 162283.0: 2416, 12518.0: 2417, 71500.0: 2418, 13746.0: 2419, 33978.0: 2420, 57930.0: 2421, 35135.0: 2422, 40088.0: 2423, 40930.0: 2424, 68800.0: 2425, 35105.0: 2426, 66764.0: 2427, 41600.0: 2428, 29164.0: 2429, 268881.0: 2430, 136000.0: 2431, 26050.0: 2432, 110040.0: 2433, 165302.0: 2434, 10968.0: 2435, 21597.0: 2436, 8002.0: 2437, 51991.0: 2438, 29411.0: 2439, 52400.0: 2440, 39354.0: 2441, 69320.0: 2442, 28393.0: 2443, 13624.0: 2444, 40240.0: 2445, 7668.0: 2446, 35340.0: 2447, 74500.0: 2448, 13523.0: 2449, 131726.0: 2450, 73600.0: 2451, 142600.0: 2452, 61974.0: 2453, 13240.0: 2454, 22383.0: 2455, 41077.0: 2456, 124102.0: 2457, 27400.0: 2458, 22181.0: 2459, 96667.0: 2460, 16693.0: 2461, 13315.0: 2462, 7920.0: 2463, 14048.0: 2464, 28296.0: 2465, 17112.0: 2466, 55367.0: 2467, 71700.0: 2468, 37171.0: 2469, 84603.0: 2470, 80484.0: 2471, 80213.0: 2472, 76500.0: 2473, 60300.0: 2474, 41941.0: 2475, 41500.0: 2476, 87302.0: 2477, 53188.0: 2478, 66864.0: 2479, 38280.0: 2480, 64159.0: 2481, 47200.0: 2482, 46400.0: 2483, 24622.0: 2484, 87994.0: 2485, 191890.0: 2486, 3500.0: 2487, 10315.0: 2488, 85030.0: 2489, 36780.0: 2490, 74160.0: 2491, 100732.0: 2492, 10264.0: 2493, 17424.0: 2494, 37400.0: 2495, 35648.0: 2496, 87341.0: 2497, 21104.0: 2498, 15535.0: 2499, 76950.0: 2500, 39824.0: 2501, 25964.0: 2502, 20840.0: 2503, 110990.0: 2504, 85103.0: 2505, 209116.0: 2506, 25805.0: 2507, 99013.0: 2508, 126617.0: 2509, 2858.0: 2510, 57601.0: 2511, 21341.0: 2512, 10940.0: 2513, 798.0: 2514, 3638.0: 2515, 63262.0: 2516, 15381.0: 2517, 42508.0: 2518, 81600.0: 2519, 14367.0: 2520, 115823.0: 2521, 63922.0: 2522, 16650.0: 2523, 33903.0: 2524, 10710.0: 2525, 22131.0: 2526, 22885.0: 2527, 67131.0: 2528, 67921.0: 2529, 38864.0: 2530, 61900.0: 2531, 35200.0: 2532, 57885.0: 2533, 71885.0: 2534, 30569.0: 2535, 27111.0: 2536, 12770.0: 2537, 12055.0: 2538, 63123.0: 2539, 2064.0: 2540, 60400.0: 2541, 322000.0: 2542, 44160.0: 2543, 31200.0: 2544, 197466.0: 2545, 12666.0: 2546, 93716.0: 2547, 62850.0: 2548, 78863.0: 2549, 21480.0: 2550, 59080.0: 2551, 51750.0: 2552, 14450.0: 2553, 12192.0: 2554, 37749.0: 2555, 7080.0: 2556, 34300.0: 2557, 96200.0: 2558, 76673.0: 2559, 19381.0: 2560, 49750.0: 2561, 66400.0: 2562, 3542.0: 2563, 24604.0: 2564, 22401.0: 2565, 26700.0: 2566, 59567.0: 2567, 53900.0: 2568, 133000.0: 2569, 24178.0: 2570, 21370.0: 2571, 31910.0: 2572, 6932.0: 2573, 57045.0: 2574, 51050.0: 2575, 102400.0: 2576, 46050.0: 2577, 63695.0: 2578, 16392.0: 2579, 66800.0: 2580, 29115.0: 2581, 638.0: 2582, 44437.0: 2583, 52465.0: 2584, 38311.0: 2585, 53910.0: 2586, 35968.0: 2587, 48100.0: 2588, 15717.0: 2589, 58814.0: 2590, 17640.0: 2591, 151320.0: 2592, 18908.0: 2593, 49553.0: 2594, 97650.0: 2595, 82500.0: 2596, 69917.0: 2597, 50600.0: 2598, 47995.0: 2599, 57553.0: 2600, 172596.0: 2601, 14361.0: 2602, 203000.0: 2603, 27886.0: 2604, 85220.0: 2605, 6450.0: 2606, 74681.0: 2607, 20376.0: 2608, 9941.0: 2609, 13546.0: 2610, 65428.0: 2611, 39105.0: 2612, 10788.0: 2613, 12800.0: 2614, 33468.0: 2615, 67800.0: 2616, 82258.0: 2617, 112600.0: 2618, 22413.0: 2619, 38845.0: 2620, 10056.0: 2621, 11400.0: 2622, 68665.0: 2623, 81400.0: 2624, 53060.0: 2625, 37674.0: 2626, 56765.0: 2627, 21400.0: 2628, 24425.0: 2629, 109250.0: 2630, 125520.0: 2631, 25275.0: 2632, 17924.0: 2633, 44030.0: 2634, 24850.0: 2635, 100350.0: 2636, 21138.0: 2637, 15708.0: 2638, 6876.0: 2639, 85200.0: 2640, 32285.0: 2641, 31397.0: 2642, 109500.0: 2643, 17256.0: 2644, 8688.0: 2645, 12379.0: 2646, 175903.0: 2647, 34600.0: 2648, 41176.0: 2649, 52015.0: 2650, 17962.0: 2651, 132334.0: 2652, 54250.0: 2653, 17696.0: 2654, 37345.0: 2655, 52696.0: 2656, 9644.0: 2657, 9572.0: 2658, 9986.0: 2659, 36701.0: 2660, 26900.0: 2661, 14268.0: 2662, 1468.0: 2663, 44200.0: 2664, 6993.0: 2665, 34815.0: 2666, 12592.0: 2667, 22770.0: 2668, 51230.0: 2669, 24450.0: 2670, 47653.0: 2671, 195000.0: 2672, 24723.0: 2673, 25780.0: 2674, 10750.0: 2675, 40320.0: 2676, 12156.0: 2677, 3924.0: 2678, 17400.0: 2679, 442721.0: 2680, 29365.0: 2681, 14112.0: 2682, 10681.0: 2683, 27676.0: 2684, 17280.0: 2685, 25412.0: 2686, 45148.0: 2687, 53540.0: 2688, 153603.0: 2689, 49185.0: 2690, 12175.0: 2691, 21147.0: 2692, 235603.0: 2693, 28465.0: 2694, 360748.0: 2695, 13478.0: 2696, 11001.0: 2697, 13827.0: 2698, 119400.0: 2699, 48120.0: 2700, 22884.0: 2701, 40297.0: 2702, 119850.0: 2703, 49300.0: 2704, 64603.0: 2705, 35560.0: 2706, 12748.0: 2707, 32050.0: 2708, 34008.0: 2709, 70984.0: 2710, 56710.0: 2711, 49776.0: 2712, 206516.0: 2713, 45206.0: 2714, 81484.0: 2715, 44920.0: 2716, 70920.0: 2717, 8004.0: 2718, 105600.0: 2719, 40020.0: 2720, 2456.0: 2721, 18300.0: 2722, 61961.0: 2723, 41316.0: 2724, 13364.0: 2725, 21700.0: 2726, 20359.0: 2727, 73781.0: 2728, 30386.0: 2729, 81464.0: 2730, 32180.0: 2731, 44929.0: 2732, 34716.0: 2733, 39369.0: 2734, 133515.0: 2735, 16954.0: 2736, 45001.0: 2737, 9200.0: 2738, 64436.0: 2739, 15127.0: 2740, 53380.0: 2741, 54274.0: 2742, 50629.0: 2743, 145515.0: 2744, 29001.0: 2745, 76197.0: 2746, 38566.0: 2747, 6824.0: 2748, 108100.0: 2749, 19224.0: 2750, 106010.0: 2751, 40181.0: 2752, 83711.0: 2753, 44176.0: 2754, 10542.0: 2755, 35900.0: 2756, 35625.0: 2757, 65173.0: 2758, 183600.0: 2759, 25068.0: 2760, 19241.0: 2761, 25040.0: 2762, 15430.0: 2763, 23112.0: 2764, 64560.0: 2765, 8338.0: 2766, 110691.0: 2767, 17700.0: 2768, 74840.0: 2769, 14597.0: 2770, 36090.0: 2771, 39820.0: 2772, 23501.0: 2773, 20498.0: 2774, 38106.0: 2775, 47329.0: 2776, 61618.0: 2777, 61400.0: 2778, 59240.0: 2779, 33882.0: 2780, 97316.0: 2781, 26044.0: 2782, 72802.0: 2783, 52600.0: 2784, 42736.0: 2785, 9864.0: 2786, 75973.0: 2787, 39428.0: 2788, 8228.0: 2789, 75121.0: 2790, 56992.0: 2791, 72341.0: 2792, 69120.0: 2793, 57914.0: 2794, 39328.0: 2795, 13822.0: 2796, 6828.0: 2797, 44446.0: 2798, 55100.0: 2799, 45650.0: 2800, 62756.0: 2801, 22253.0: 2802, 53720.0: 2803, 81196.0: 2804, 60900.0: 2805, 8760.0: 2806, 52800.0: 2807, 8244.0: 2808, 44623.0: 2809, 228464.0: 2810, 18284.0: 2811, 81973.0: 2812, 67124.0: 2813, 52428.0: 2814, 6960.0: 2815, 32096.0: 2816, 1022.0: 2817, 40080.0: 2818, 52366.0: 2819, 58517.0: 2820, 69620.0: 2821, 27144.0: 2822, 49194.0: 2823, 44780.0: 2824, 77741.0: 2825, 56295.0: 2826, 71503.0: 2827, 51463.0: 2828, 40828.0: 2829, 80250.0: 2830, 65700.0: 2831, 26257.0: 2832, 23758.0: 2833, 55883.0: 2834, 11088.0: 2835, 48332.0: 2836, 61800.0: 2837, 56472.0: 2838, 96500.0: 2839, 64100.0: 2840, 54100.0: 2841, 9550.0: 2842, 54450.0: 2843, 20268.0: 2844, 1564.0: 2845, 115500.0: 2846, 64500.0: 2847, 157000.0: 2848, 205442.0: 2849, 35034.0: 2850, 83874.0: 2851, 80430.0: 2852, 184955.0: 2853, 16730.0: 2854, 37450.0: 2855, 123000.0: 2856, 29984.0: 2857, 49780.0: 2858, 11280.0: 2859, 6616.0: 2860, 32463.0: 2861, 71786.0: 2862, 83063.0: 2863, 35464.0: 2864, 13152.0: 2865, 21488.0: 2866, 25891.0: 2867, 228516.0: 2868, 9610.0: 2869, 37656.0: 2870, 67700.0: 2871, 20980.0: 2872, 79500.0: 2873, 7932.0: 2874, 74677.0: 2875, 241516.0: 2876, 44270.0: 2877, 35705.0: 2878, 44456.0: 2879, 46517.0: 2880, 142985.0: 2881, 50025.0: 2882, 19300.0: 2883, 13568.0: 2884, 10907.0: 2885, 16564.0: 2886, 38750.0: 2887, 4952.0: 2888, 53132.0: 2889, 12304.0: 2890, 34401.0: 2891, 18100.0: 2892, 69286.0: 2893, 66820.0: 2894, 61403.0: 2895, 43570.0: 2896, 25962.0: 2897, 65034.0: 2898, 54643.0: 2899, 38700.0: 2900, 95767.0: 2901, 28766.0: 2902, 23888.0: 2903, 23685.0: 2904, 7034.0: 2905, 88929.0: 2906, 93280.0: 2907, 58586.0: 2908, 23680.0: 2909, 39818.0: 2910, 135974.0: 2911, 53276.0: 2912, 58090.0: 2913, 52684.0: 2914, 16608.0: 2915, 19654.0: 2916, 14432.0: 2917, 23910.0: 2918, 9880.0: 2919, 70268.0: 2920, 67955.0: 2921, 18051.0: 2922, 46150.0: 2923, 16895.0: 2924, 7176.0: 2925, 7524.0: 2926, 35060.0: 2927, 11852.0: 2928, 37236.0: 2929, 44428.0: 2930, 8812.0: 2931, 5374.0: 2932, 75001.0: 2933, 97702.0: 2934, 6460.0: 2935, 79478.0: 2936, 9248.0: 2937, 40250.0: 2938, 67140.0: 2939, 2700.0: 2940, 26560.0: 2941, 42532.0: 2942, 29950.0: 2943, 37974.0: 2944, 19750.0: 2945, 12912.0: 2946, 8540.0: 2947, 67056.0: 2948, 15266.0: 2949, 50989.0: 2950, 32550.0: 2951, 28764.0: 2952, 48146.0: 2953, 20060.0: 2954, 182118.0: 2955, 183815.0: 2956, 55500.0: 2957, 40101.0: 2958, 174000.0: 2959, 23510.0: 2960, 91099.0: 2961, 42581.0: 2962, 63522.0: 2963, 44044.0: 2964, 103327.0: 2965, 61264.0: 2966, 109401.0: 2967, 22400.0: 2968, 32630.0: 2969, 21689.0: 2970, 6612.0: 2971, 58212.0: 2972, 27876.0: 2973, 12659.0: 2974, 35612.0: 2975, 67095.0: 2976, 73410.0: 2977, 29780.0: 2978, 12400.0: 2979, 54800.0: 2980, 37370.0: 2981, 49405.0: 2982, 601.0: 2983, 13700.0: 2984, 120800.0: 2985, 25740.0: 2986, 46429.0: 2987, 40957.0: 2988, 233912.0: 2989, 23636.0: 2990, 94668.0: 2991, 30100.0: 2992, 28941.0: 2993, 136753.0: 2994, 22480.0: 2995, 56132.0: 2996, 44250.0: 2997, 34470.0: 2998, 65450.0: 2999, 59809.0: 3000, 37091.0: 3001, 18975.0: 3002, 44445.0: 3003, 41686.0: 3004, 18198.0: 3005, 101942.0: 3006, 27868.0: 3007, 42817.0: 3008, 15300.0: 3009, 79400.0: 3010, 275719.0: 3011, 12271.0: 3012, 24838.0: 3013, 12260.0: 3014, 167835.0: 3015, 45102.0: 3016, 95535.0: 3017, 4870.0: 3018, 35119.0: 3019}, {188962491.0: 0, 330961765.0: 1, 975332729.0: 2, 997252992.0: 3, 1560697901.0: 4, 1788211206.0: 5, 2001877100.0: 6, 2801713682.0: 7, 2991933799.0: 8, 3231784877.0: 9, 3401566521.0: 10, 3815930669.0: 11, 4052084249.0: 12}, {1306038106.0: 0, 4122955671.0: 1}, {621778706.0: 0, 701120191.0: 1, 874847880.0: 2, 1231373657.0: 3, 2198790816.0: 4, 3980514807.0: 5, 4091618104.0: 6}, {456659696.0: 0, 1492722811.0: 1, 2958897597.0: 2, 3375379397.0: 3, 3571393042.0: 4, 4084799460.0: 5}, {3.0: 0, 4.0: 1, 5.0: 2, 6.0: 3, 7.0: 4, 8.0: 5, 9.0: 6, 11.0: 7, 985693824.0: 8, 1684325040.0: 9, 1997186321.0: 10, 2211334012.0: 11, 2318493838.0: 12, 2785754252.0: 13}, {1684325040.0: 0, 2235108354.0: 1, 3055939076.0: 2}, {420184399.0: 0, 1684325040.0: 1, 1711720114.0: 2, 1846325721.0: 3, 2156067110.0: 4, 2664106125.0: 5, 3497807896.0: 6, 3827287123.0: 7, 3936362660.0: 8, 4144357475.0: 9}, {1242644209.0: 0, 1333853783.0: 1, 1684325040.0: 2, 2659261300.0: 3}, {1684325040.0: 0, 1739384082.0: 1, 3555600225.0: 2, 4030401829.0: 3}, {1684325040.0: 0, 3791798082.0: 1, 4033051765.0: 2}, {1684325040.0: 0, 2234561500.0: 1, 2985724231.0: 2}, {659930115.0: 0, 1072804360.0: 1, 1626161905.0: 2, 1684325040.0: 3, 1868697974.0: 4, 2082048417.0: 5, 2082607685.0: 6}, {667704912.0: 0, 743457664.0: 1, 888425867.0: 2, 1119183711.0: 3, 1431963742.0: 4, 1684325040.0: 5, 2099697422.0: 6, 2687694036.0: 7, 3014575764.0: 8}, {36103881.0: 0, 333207776.0: 1, 1645465596.0: 2, 1684325040.0: 3, 2414546412.0: 4, 2759995889.0: 5, 3990127257.0: 6, 4182777155.0: 7}, {1303016265.0: 0, 1684325040.0: 1, 4063104189.0: 2}, {-9279.0: 0, -7000.0: 1, -2005.0: 2, -2000.0: 3, -650.0: 4, -200.0: 5, -160.0: 6, 0.0: 7, 1.0: 8, 10.0: 9, 13.0: 10, 40.0: 11, 60.0: 12, 90.0: 13, 100.0: 14, 150.0: 15, 160.0: 16, 199.0: 17, 200.0: 18, 224.0: 19, 229.0: 20, 300.0: 21, 325.0: 22, 340.0: 23, 341.0: 24, 368.0: 25, 379.0: 26, 385.0: 27, 390.0: 28, 400.0: 29, 441.0: 30, 495.0: 31, 500.0: 32, 507.0: 33, 560.0: 34, 570.0: 35, 600.0: 36, 620.0: 37, 630.0: 38, 640.0: 39, 651.0: 40, 653.0: 41, 660.0: 42, 699.0: 43, 700.0: 44, 720.0: 45, 750.0: 46, 759.0: 47, 761.0: 48, 790.0: 49, 800.0: 50, 818.0: 51, 864.0: 52, 900.0: 53, 915.0: 54, 928.0: 55, 963.0: 56, 995.0: 57, 1000.0: 58, 1025.0: 59, 1029.0: 60, 1091.0: 61, 1100.0: 62, 1120.0: 63, 1125.0: 64, 1135.0: 65, 1138.0: 66, 1200.0: 67, 1280.0: 68, 1300.0: 69, 1320.0: 70, 1350.0: 71, 1369.0: 72, 1400.0: 73, 1406.0: 74, 1484.0: 75, 1488.0: 76, 1500.0: 77, 1590.0: 78, 1600.0: 79, 1656.0: 80, 1664.0: 81, 1680.0: 82, 1685.0: 83, 1700.0: 84, 1709.0: 85, 1720.0: 86, 1740.0: 87, 1750.0: 88, 1800.0: 89, 1863.0: 90, 1900.0: 91, 1920.0: 92, 1980.0: 93, 1992.0: 94, 2000.0: 95, 2003.0: 96, 2050.0: 97, 2100.0: 98, 2200.0: 99, 2262.0: 100, 2280.0: 101, 2300.0: 102, 2335.0: 103, 2340.0: 104, 2356.0: 105, 2400.0: 106, 2460.0: 107, 2500.0: 108, 2501.0: 109, 2571.0: 110, 2580.0: 111, 2600.0: 112, 2617.0: 113, 2620.0: 114, 2628.0: 115, 2687.0: 116, 2700.0: 117, 2750.0: 118, 2769.0: 119, 2800.0: 120, 2834.0: 121, 2880.0: 122, 2900.0: 123, 2916.0: 124, 2976.0: 125, 2981.0: 126, 3000.0: 127, 3010.0: 128, 3030.0: 129, 3084.0: 130, 3100.0: 131, 3112.0: 132, 3120.0: 133, 3150.0: 134, 3155.0: 135, 3200.0: 136, 3231.0: 137, 3264.0: 138, 3300.0: 139, 3336.0: 140, 3360.0: 141, 3370.0: 142, 3400.0: 143, 3406.0: 144, 3480.0: 145, 3500.0: 146, 3511.0: 147, 3534.0: 148, 3588.0: 149, 3599.0: 150, 3600.0: 151, 3642.0: 152, 3649.0: 153, 3650.0: 154, 3700.0: 155, 3741.0: 156, 3744.0: 157, 3777.0: 158, 3780.0: 159, 3800.0: 160, 3807.0: 161, 3820.0: 162, 3876.0: 163, 3880.0: 164, 3885.0: 165, 3900.0: 166, 3982.0: 167, 4000.0: 168, 4002.0: 169, 4008.0: 170, 4018.0: 171, 4075.0: 172, 4080.0: 173, 4091.0: 174, 4092.0: 175, 4100.0: 176, 4190.0: 177, 4195.0: 178, 4200.0: 179, 4210.0: 180, 4220.0: 181, 4295.0: 182, 4296.0: 183, 4300.0: 184, 4311.0: 185, 4320.0: 186, 4337.0: 187, 4361.0: 188, 4380.0: 189, 4400.0: 190, 4416.0: 191, 4417.0: 192, 4420.0: 193, 4440.0: 194, 4464.0: 195, 4475.0: 196, 4500.0: 197, 4522.0: 198, 4556.0: 199, 4560.0: 200, 4600.0: 201, 4608.0: 202, 4629.0: 203, 4690.0: 204, 4700.0: 205, 4719.0: 206, 4797.0: 207, 4800.0: 208, 4851.0: 209, 4876.0: 210, 4900.0: 211, 4946.0: 212, 4971.0: 213, 5000.0: 214, 5010.0: 215, 5088.0: 216, 5100.0: 217, 5118.0: 218, 5134.0: 219, 5184.0: 220, 5196.0: 221, 5200.0: 222, 5217.0: 223, 5244.0: 224, 5290.0: 225, 5300.0: 226, 5316.0: 227, 5364.0: 228, 5398.0: 229, 5400.0: 230, 5424.0: 231, 5500.0: 232, 5508.0: 233, 5558.0: 234, 5600.0: 235, 5652.0: 236, 5664.0: 237, 5670.0: 238, 5676.0: 239, 5688.0: 240, 5700.0: 241, 5721.0: 242, 5760.0: 243, 5817.0: 244, 5820.0: 245, 5822.0: 246, 5880.0: 247, 5900.0: 248, 5990.0: 249, 5998.0: 250, 6000.0: 251, 6009.0: 252, 6010.0: 253, 6012.0: 254, 6020.0: 255, 6021.0: 256, 6030.0: 257, 6039.0: 258, 6048.0: 259, 6063.0: 260, 6084.0: 261, 6125.0: 262, 6140.0: 263, 6154.0: 264, 6180.0: 265, 6192.0: 266, 6200.0: 267, 6225.0: 268, 6240.0: 269, 6257.0: 270, 6262.0: 271, 6276.0: 272, 6282.0: 273, 6356.0: 274, 6360.0: 275, 6369.0: 276, 6400.0: 277, 6414.0: 278, 6440.0: 279, 6480.0: 280, 6500.0: 281, 6516.0: 282, 6578.0: 283, 6600.0: 284, 6630.0: 285, 6634.0: 286, 6640.0: 287, 6646.0: 288, 6664.0: 289, 6696.0: 290, 6698.0: 291, 6700.0: 292, 6706.0: 293, 6710.0: 294, 6720.0: 295, 6744.0: 296, 6785.0: 297, 6816.0: 298, 6833.0: 299, 6834.0: 300, 6840.0: 301, 6884.0: 302, 6888.0: 303, 6900.0: 304, 6908.0: 305, 6910.0: 306, 6960.0: 307, 6994.0: 308, 6996.0: 309, 7000.0: 310, 7002.0: 311, 7044.0: 312, 7060.0: 313, 7072.0: 314, 7073.0: 315, 7092.0: 316, 7116.0: 317, 7187.0: 318, 7195.0: 319, 7200.0: 320, 7212.0: 321, 7224.0: 322, 7240.0: 323, 7255.0: 324, 7272.0: 325, 7318.0: 326, 7320.0: 327, 7360.0: 328, 7374.0: 329, 7378.0: 330, 7380.0: 331, 7400.0: 332, 7440.0: 333, 7464.0: 334, 7488.0: 335, 7492.0: 336, 7500.0: 337, 7509.0: 338, 7512.0: 339, 7548.0: 340, 7560.0: 341, 7590.0: 342, 7600.0: 343, 7608.0: 344, 7650.0: 345, 7668.0: 346, 7684.0: 347, 7728.0: 348, 7740.0: 349, 7752.0: 350, 7764.0: 351, 7800.0: 352, 7865.0: 353, 7884.0: 354, 7920.0: 355, 7944.0: 356, 7956.0: 357, 8000.0: 358, 8001.0: 359, 8011.0: 360, 8016.0: 361, 8030.0: 362, 8040.0: 363, 8056.0: 364, 8060.0: 365, 8100.0: 366, 8106.0: 367, 8118.0: 368, 8120.0: 369, 8136.0: 370, 8146.0: 371, 8150.0: 372, 8160.0: 373, 8162.0: 374, 8180.0: 375, 8200.0: 376, 8208.0: 377, 8232.0: 378, 8240.0: 379, 8260.0: 380, 8280.0: 381, 8328.0: 382, 8362.0: 383, 8375.0: 384, 8376.0: 385, 8400.0: 386, 8425.0: 387, 8448.0: 388, 8492.0: 389, 8496.0: 390, 8500.0: 391, 8508.0: 392, 8550.0: 393, 8558.0: 394, 8583.0: 395, 8600.0: 396, 8636.0: 397, 8640.0: 398, 8679.0: 399, 8683.0: 400, 8700.0: 401, 8748.0: 402, 8750.0: 403, 8772.0: 404, 8795.0: 405, 8796.0: 406, 8800.0: 407, 8808.0: 408, 8838.0: 409, 8840.0: 410, 8918.0: 411, 8937.0: 412, 8964.0: 413, 8988.0: 414, 9000.0: 415, 9010.0: 416, 9032.0: 417, 9075.0: 418, 9100.0: 419, 9107.0: 420, 9120.0: 421, 9156.0: 422, 9180.0: 423, 9200.0: 424, 9300.0: 425, 9303.0: 426, 9336.0: 427, 9360.0: 428, 9368.0: 429, 9380.0: 430, 9394.0: 431, 9400.0: 432, 9444.0: 433, 9449.0: 434, 9500.0: 435, 9520.0: 436, 9600.0: 437, 9685.0: 438, 9688.0: 439, 9700.0: 440, 9732.0: 441, 9775.0: 442, 9800.0: 443, 9840.0: 444, 9880.0: 445, 9888.0: 446, 9900.0: 447, 9929.0: 448, 9947.0: 449, 10000.0: 450, 10007.0: 451, 10063.0: 452, 10080.0: 453, 10142.0: 454, 10183.0: 455, 10185.0: 456, 10200.0: 457, 10211.0: 458, 10385.0: 459, 10394.0: 460, 10400.0: 461, 10401.0: 462, 10424.0: 463, 10479.0: 464, 10500.0: 465, 10523.0: 466, 10560.0: 467, 10700.0: 468, 10790.0: 469, 10800.0: 470, 10840.0: 471, 10850.0: 472, 10890.0: 473, 10916.0: 474, 10980.0: 475, 10982.0: 476, 11000.0: 477, 11009.0: 478, 11034.0: 479, 11200.0: 480, 11220.0: 481, 11221.0: 482, 11235.0: 483, 11250.0: 484, 11267.0: 485, 11268.0: 486, 11270.0: 487, 11300.0: 488, 11375.0: 489, 11412.0: 490, 11440.0: 491, 11476.0: 492, 11500.0: 493, 11520.0: 494, 11530.0: 495, 11670.0: 496, 11673.0: 497, 11711.0: 498, 11726.0: 499, 11727.0: 500, 11759.0: 501, 11800.0: 502, 11900.0: 503, 11901.0: 504, 11936.0: 505, 12000.0: 506, 12012.0: 507, 12066.0: 508, 12102.0: 509, 12200.0: 510, 12367.0: 511, 12391.0: 512, 12397.0: 513, 12408.0: 514, 12422.0: 515, 12461.0: 516, 12472.0: 517, 12491.0: 518, 12500.0: 519, 12507.0: 520, 12513.0: 521, 12530.0: 522, 12552.0: 523, 12574.0: 524, 12600.0: 525, 12618.0: 526, 12772.0: 527, 12777.0: 528, 12779.0: 529, 12800.0: 530, 12815.0: 531, 12920.0: 532, 12928.0: 533, 12986.0: 534, 13000.0: 535, 13005.0: 536, 13115.0: 537, 13169.0: 538, 13194.0: 539, 13236.0: 540, 13260.0: 541, 13272.0: 542, 13282.0: 543, 13294.0: 544, 13400.0: 545, 13487.0: 546, 13499.0: 547, 13500.0: 548, 13517.0: 549, 13521.0: 550, 13547.0: 551, 13600.0: 552, 13620.0: 553, 13698.0: 554, 13757.0: 555, 13788.0: 556, 13795.0: 557, 13800.0: 558, 13820.0: 559, 13841.0: 560, 13915.0: 561, 14000.0: 562, 14071.0: 563, 14100.0: 564, 14150.0: 565, 14200.0: 566, 14232.0: 567, 14233.0: 568, 14247.0: 569, 14315.0: 570, 14382.0: 571, 14400.0: 572, 14435.0: 573, 14500.0: 574, 14536.0: 575, 14571.0: 576, 14600.0: 577, 14637.0: 578, 14692.0: 579, 14700.0: 580, 14745.0: 581, 14777.0: 582, 14788.0: 583, 14798.0: 584, 14800.0: 585, 14916.0: 586, 14976.0: 587, 14985.0: 588, 15000.0: 589, 15150.0: 590, 15153.0: 591, 15189.0: 592, 15200.0: 593, 15220.0: 594, 15248.0: 595, 15250.0: 596, 15422.0: 597, 15450.0: 598, 15472.0: 599, 15478.0: 600, 15500.0: 601, 15537.0: 602, 15600.0: 603, 15618.0: 604, 15677.0: 605, 15690.0: 606, 15816.0: 607, 15900.0: 608, 16000.0: 609, 16035.0: 610, 16090.0: 611, 16100.0: 612, 16145.0: 613, 16146.0: 614, 16150.0: 615, 16182.0: 616, 16219.0: 617, 16262.0: 618, 16320.0: 619, 16352.0: 620, 16365.0: 621, 16387.0: 622, 16400.0: 623, 16401.0: 624, 16407.0: 625, 16464.0: 626, 16469.0: 627, 16500.0: 628, 16550.0: 629, 16560.0: 630, 16577.0: 631, 16638.0: 632, 16641.0: 633, 16700.0: 634, 16739.0: 635, 16800.0: 636, 16816.0: 637, 16884.0: 638, 16928.0: 639, 16958.0: 640, 16980.0: 641, 17000.0: 642, 17060.0: 643, 17065.0: 644, 17100.0: 645, 17200.0: 646, 17240.0: 647, 17376.0: 648, 17400.0: 649, 17420.0: 650, 17500.0: 651, 17521.0: 652, 17568.0: 653, 17729.0: 654, 17789.0: 655, 17820.0: 656, 17930.0: 657, 17990.0: 658, 18000.0: 659, 18048.0: 660, 18100.0: 661, 18122.0: 662, 18200.0: 663, 18209.0: 664, 18274.0: 665, 18310.0: 666, 18361.0: 667, 18392.0: 668, 18414.0: 669, 18474.0: 670, 18500.0: 671, 18552.0: 672, 18560.0: 673, 18581.0: 674, 18600.0: 675, 18633.0: 676, 18678.0: 677, 18692.0: 678, 18708.0: 679, 18775.0: 680, 18783.0: 681, 18800.0: 682, 18928.0: 683, 18985.0: 684, 19000.0: 685, 19020.0: 686, 19031.0: 687, 19101.0: 688, 19125.0: 689, 19189.0: 690, 19200.0: 691, 19325.0: 692, 19334.0: 693, 19360.0: 694, 19500.0: 695, 19510.0: 696, 19523.0: 697, 19530.0: 698, 19537.0: 699, 19600.0: 700, 19611.0: 701, 19625.0: 702, 19644.0: 703, 19672.0: 704, 19700.0: 705, 19749.0: 706, 19936.0: 707, 20000.0: 708, 20001.0: 709, 20030.0: 710, 20034.0: 711, 20100.0: 712, 20166.0: 713, 20200.0: 714, 20208.0: 715, 20326.0: 716, 20479.0: 717, 20500.0: 718, 20677.0: 719, 20699.0: 720, 20800.0: 721, 20812.0: 722, 20840.0: 723, 20971.0: 724, 21000.0: 725, 21100.0: 726, 21340.0: 727, 21465.0: 728, 21468.0: 729, 21500.0: 730, 21504.0: 731, 21600.0: 732, 21690.0: 733, 21768.0: 734, 21776.0: 735, 21850.0: 736, 21939.0: 737, 21967.0: 738, 21982.0: 739, 22000.0: 740, 22012.0: 741, 22025.0: 742, 22050.0: 743, 22075.0: 744, 22100.0: 745, 22300.0: 746, 22324.0: 747, 22365.0: 748, 22403.0: 749, 22411.0: 750, 22425.0: 751, 22493.0: 752, 22500.0: 753, 22534.0: 754, 22741.0: 755, 22917.0: 756, 23000.0: 757, 23090.0: 758, 23137.0: 759, 23157.0: 760, 23162.0: 761, 23163.0: 762, 23200.0: 763, 23222.0: 764, 23300.0: 765, 23403.0: 766, 23430.0: 767, 23500.0: 768, 23509.0: 769, 23600.0: 770, 23642.0: 771, 23662.0: 772, 23664.0: 773, 23700.0: 774, 23725.0: 775, 23800.0: 776, 23824.0: 777, 23860.0: 778, 23948.0: 779, 24000.0: 780, 24033.0: 781, 24060.0: 782, 24100.0: 783, 24105.0: 784, 24174.0: 785, 24215.0: 786, 24260.0: 787, 24332.0: 788, 24334.0: 789, 24500.0: 790, 24504.0: 791, 24600.0: 792, 24722.0: 793, 24800.0: 794, 24803.0: 795, 24893.0: 796, 24939.0: 797, 24944.0: 798, 24976.0: 799, 24980.0: 800, 25000.0: 801, 25065.0: 802, 25100.0: 803, 25108.0: 804, 25242.0: 805, 25280.0: 806, 25400.0: 807, 25404.0: 808, 25405.0: 809, 25416.0: 810, 25440.0: 811, 25500.0: 812, 25600.0: 813, 25620.0: 814, 25680.0: 815, 25700.0: 816, 25701.0: 817, 25828.0: 818, 25950.0: 819, 25993.0: 820, 26000.0: 821, 26015.0: 822, 26050.0: 823, 26100.0: 824, 26124.0: 825, 26144.0: 826, 26225.0: 827, 26240.0: 828, 26241.0: 829, 26400.0: 830, 26461.0: 831, 26500.0: 832, 26512.0: 833, 26584.0: 834, 26655.0: 835, 26699.0: 836, 26742.0: 837, 26865.0: 838, 27000.0: 839, 27030.0: 840, 27060.0: 841, 27090.0: 842, 27142.0: 843, 27227.0: 844, 27400.0: 845, 27445.0: 846, 27500.0: 847, 27600.0: 848, 27772.0: 849, 27899.0: 850, 27961.0: 851, 27978.0: 852, 28000.0: 853, 28026.0: 854, 28164.0: 855, 28232.0: 856, 28400.0: 857, 28470.0: 858, 28481.0: 859, 28500.0: 860, 28566.0: 861, 28600.0: 862, 28641.0: 863, 28700.0: 864, 28800.0: 865, 28830.0: 866, 28850.0: 867, 28857.0: 868, 28915.0: 869, 29000.0: 870, 29050.0: 871, 29052.0: 872, 29185.0: 873, 29300.0: 874, 29304.0: 875, 29454.0: 876, 29595.0: 877, 29600.0: 878, 29689.0: 879, 29700.0: 880, 29711.0: 881, 29719.0: 882, 29759.0: 883, 29775.0: 884, 29800.0: 885, 29817.0: 886, 30000.0: 887, 30050.0: 888, 30133.0: 889, 30140.0: 890, 30200.0: 891, 30248.0: 892, 30485.0: 893, 30500.0: 894, 30600.0: 895, 30989.0: 896, 31000.0: 897, 31043.0: 898, 31057.0: 899, 31100.0: 900, 31200.0: 901, 31256.0: 902, 31300.0: 903, 31416.0: 904, 31500.0: 905, 31537.0: 906, 31568.0: 907, 31712.0: 908, 31886.0: 909, 31921.0: 910, 32000.0: 911, 32040.0: 912, 32100.0: 913, 32199.0: 914, 32400.0: 915, 32478.0: 916, 32500.0: 917, 32513.0: 918, 32565.0: 919, 32680.0: 920, 32700.0: 921, 32733.0: 922, 32747.0: 923, 32870.0: 924, 32922.0: 925, 32926.0: 926, 33000.0: 927, 33280.0: 928, 33400.0: 929, 33408.0: 930, 33446.0: 931, 33500.0: 932, 33600.0: 933, 33700.0: 934, 33798.0: 935, 33858.0: 936, 34000.0: 937, 34020.0: 938, 34022.0: 939, 34106.0: 940, 34130.0: 941, 34194.0: 942, 34500.0: 943, 34652.0: 944, 34711.0: 945, 35000.0: 946, 35090.0: 947, 35149.0: 948, 35249.0: 949, 35303.0: 950, 35336.0: 951, 35400.0: 952, 35500.0: 953, 35502.0: 954, 35752.0: 955, 35756.0: 956, 35980.0: 957, 36000.0: 958, 36250.0: 959, 36282.0: 960, 36355.0: 961, 36436.0: 962, 36479.0: 963, 36525.0: 964, 36580.0: 965, 36617.0: 966, 36800.0: 967, 36806.0: 968, 36990.0: 969, 37000.0: 970, 37200.0: 971, 37209.0: 972, 37268.0: 973, 37279.0: 974, 37400.0: 975, 37500.0: 976, 37534.0: 977, 37600.0: 978, 37731.0: 979, 37900.0: 980, 38000.0: 981, 38002.0: 982, 38008.0: 983, 38100.0: 984, 38169.0: 985, 38300.0: 986, 38400.0: 987, 38447.0: 988, 38475.0: 989, 38600.0: 990, 38700.0: 991, 38819.0: 992, 39000.0: 993, 39020.0: 994, 39038.0: 995, 39075.0: 996, 39110.0: 997, 39184.0: 998, 39287.0: 999, 39400.0: 1000, 39449.0: 1001, 39491.0: 1002, 39494.0: 1003, 39500.0: 1004, 40000.0: 1005, 40200.0: 1006, 40272.0: 1007, 40273.0: 1008, 40284.0: 1009, 40300.0: 1010, 40500.0: 1011, 40700.0: 1012, 40790.0: 1013, 40900.0: 1014, 41000.0: 1015, 41063.0: 1016, 41200.0: 1017, 41400.0: 1018, 41480.0: 1019, 41550.0: 1020, 41600.0: 1021, 41639.0: 1022, 41906.0: 1023, 42000.0: 1024, 42010.0: 1025, 42120.0: 1026, 42200.0: 1027, 42389.0: 1028, 42654.0: 1029, 43000.0: 1030, 43300.0: 1031, 43358.0: 1032, 43400.0: 1033, 43500.0: 1034, 43504.0: 1035, 43709.0: 1036, 43800.0: 1037, 44000.0: 1038, 44440.0: 1039, 44773.0: 1040, 44800.0: 1041, 45000.0: 1042, 45122.0: 1043, 45133.0: 1044, 45150.0: 1045, 45176.0: 1046, 45200.0: 1047, 45220.0: 1048, 45338.0: 1049, 45400.0: 1050, 45500.0: 1051, 45590.0: 1052, 45600.0: 1053, 45700.0: 1054, 46000.0: 1055, 46125.0: 1056, 46138.0: 1057, 46431.0: 1058, 46500.0: 1059, 46796.0: 1060, 46800.0: 1061, 46823.0: 1062, 46878.0: 1063, 46950.0: 1064, 47000.0: 1065, 47096.0: 1066, 47130.0: 1067, 47500.0: 1068, 47512.0: 1069, 47583.0: 1070, 47661.0: 1071, 47800.0: 1072, 47901.0: 1073, 47958.0: 1074, 47991.0: 1075, 48000.0: 1076, 48134.0: 1077, 48200.0: 1078, 48224.0: 1079, 48247.0: 1080, 48256.0: 1081, 48373.0: 1082, 48459.0: 1083, 48520.0: 1084, 48612.0: 1085, 48700.0: 1086, 48800.0: 1087, 48840.0: 1088, 48842.0: 1089, 49000.0: 1090, 49624.0: 1091, 50000.0: 1092, 50030.0: 1093, 50100.0: 1094, 50164.0: 1095, 50397.0: 1096, 50400.0: 1097, 50500.0: 1098, 50720.0: 1099, 51000.0: 1100, 51178.0: 1101, 51342.0: 1102, 51400.0: 1103, 51471.0: 1104, 51546.0: 1105, 51670.0: 1106, 51690.0: 1107, 51698.0: 1108, 52000.0: 1109, 52010.0: 1110, 52035.0: 1111, 52500.0: 1112, 52830.0: 1113, 52871.0: 1114, 53000.0: 1115, 53558.0: 1116, 54000.0: 1117, 54222.0: 1118, 54333.0: 1119, 54600.0: 1120, 54831.0: 1121, 55000.0: 1122, 55100.0: 1123, 55200.0: 1124, 55600.0: 1125, 55700.0: 1126, 55892.0: 1127, 55967.0: 1128, 56000.0: 1129, 56692.0: 1130, 56965.0: 1131, 57000.0: 1132, 57341.0: 1133, 57635.0: 1134, 58000.0: 1135, 58068.0: 1136, 58396.0: 1137, 58603.0: 1138, 58649.0: 1139, 58761.0: 1140, 58781.0: 1141, 58900.0: 1142, 59000.0: 1143, 59025.0: 1144, 59500.0: 1145, 59951.0: 1146, 60000.0: 1147, 60100.0: 1148, 60114.0: 1149, 60500.0: 1150, 61000.0: 1151, 62000.0: 1152, 62203.0: 1153, 62731.0: 1154, 63000.0: 1155, 63300.0: 1156, 63500.0: 1157, 63603.0: 1158, 63750.0: 1159, 64000.0: 1160, 64200.0: 1161, 64450.0: 1162, 64600.0: 1163, 64603.0: 1164, 64900.0: 1165, 65000.0: 1166, 65110.0: 1167, 65188.0: 1168, 65202.0: 1169, 65300.0: 1170, 65400.0: 1171, 65550.0: 1172, 66000.0: 1173, 66257.0: 1174, 66603.0: 1175, 66643.0: 1176, 66655.0: 1177, 66700.0: 1178, 66853.0: 1179, 67000.0: 1180, 67812.0: 1181, 68000.0: 1182, 68203.0: 1183, 68603.0: 1184, 69020.0: 1185, 69643.0: 1186, 70000.0: 1187, 70600.0: 1188, 72000.0: 1189, 73603.0: 1190, 73687.0: 1191, 74000.0: 1192, 74598.0: 1193, 75000.0: 1194, 75327.0: 1195, 76000.0: 1196, 76500.0: 1197, 76900.0: 1198, 77000.0: 1199, 77400.0: 1200, 77500.0: 1201, 77601.0: 1202, 78036.0: 1203, 78603.0: 1204, 78659.0: 1205, 78714.0: 1206, 79885.0: 1207, 80000.0: 1208, 80400.0: 1209, 80600.0: 1210, 82150.0: 1211, 83130.0: 1212, 83400.0: 1213, 84100.0: 1214, 85843.0: 1215, 87000.0: 1216, 87300.0: 1217, 88431.0: 1218, 89000.0: 1219, 89300.0: 1220, 89703.0: 1221, 90000.0: 1222, 90300.0: 1223, 92000.0: 1224, 93000.0: 1225, 94288.0: 1226, 94500.0: 1227, 94603.0: 1228, 95000.0: 1229, 96000.0: 1230, 99000.0: 1231, 99500.0: 1232, 100000.0: 1233, 101000.0: 1234, 104603.0: 1235, 105000.0: 1236, 106603.0: 1237, 107000.0: 1238, 108400.0: 1239, 109000.0: 1240, 110150.0: 1241, 111000.0: 1242, 115000.0: 1243, 118603.0: 1244, 120000.0: 1245, 120001.0: 1246, 123515.0: 1247, 123815.0: 1248, 124000.0: 1249, 124015.0: 1250, 124315.0: 1251, 125000.0: 1252, 126000.0: 1253, 128603.0: 1254, 128702.0: 1255, 130000.0: 1256, 132500.0: 1257, 133200.0: 1258, 133515.0: 1259, 139300.0: 1260, 140000.0: 1261, 143515.0: 1262, 150000.0: 1263, 152815.0: 1264, 160300.0: 1265, 173515.0: 1266, 195516.0: 1267, 197516.0: 1268, 199316.0: 1269, 205516.0: 1270, 212076.0: 1271, 213516.0: 1272, 220516.0: 1273, 225000.0: 1274, 230516.0: 1275, 254119.0: 1276, 256516.0: 1277, 263768.0: 1278, 279516.0: 1279, 56300.0: 1280, 49450.0: 1281, 138000.0: 1282, 54327.0: 1283, 70603.0: 1284, 38298.0: 1285, 36700.0: 1286, 42785.0: 1287, 67600.0: 1288, 35050.0: 1289, 13772.0: 1290, 13468.0: 1291, 30189.0: 1292, 5950.0: 1293, 32250.0: 1294, 19798.0: 1295, 29400.0: 1296, 8300.0: 1297, 203516.0: 1298, 48644.0: 1299, 2568.0: 1300, 53764.0: 1301, 200516.0: 1302, 4030.0: 1303, 428.0: 1304, 34034.0: 1305, 8758.0: 1306, 8256.0: 1307, 24352.0: 1308, 3530.0: 1309, 34988.0: 1310, 21800.0: 1311, 25744.0: 1312, 16089.0: 1313, 34400.0: 1314, 101124.0: 1315, 58700.0: 1316, 5502.0: 1317, 36360.0: 1318, 94000.0: 1319, 250.0: 1320, 14160.0: 1321, 18335.0: 1322, 24200.0: 1323, 45075.0: 1324, 42600.0: 1325, 13325.0: 1326, 42500.0: 1327, 2525.0: 1328, 215516.0: 1329, 12643.0: 1330, 32800.0: 1331, 4846.0: 1332, 8268.0: 1333, 32200.0: 1334, 34140.0: 1335, 8074.0: 1336, 22200.0: 1337, 70500.0: 1338, 11150.0: 1339, 4129.0: 1340, 12526.0: 1341, 36078.0: 1342, 8516.0: 1343, 51930.0: 1344, 89600.0: 1345, 7283.0: 1346, 18900.0: 1347, 12518.0: 1348, 13746.0: 1349, 40088.0: 1350, 8196.0: 1351, 29164.0: 1352, 3980.0: 1353, 206726.0: 1354, 809.0: 1355, 21597.0: 1356, 8002.0: 1357, 2991.0: 1358, 22400.0: 1359, 2550.0: 1360, 6364.0: 1361, 62903.0: 1362, 52200.0: 1363, 14822.0: 1364, 3040.0: 1365, 260.0: 1366, 4208.0: 1367, 3226.0: 1368, 22181.0: 1369, 4435.0: 1370, 2278.0: 1371, 12048.0: 1372, 26196.0: 1373, 1847.0: 1374, 5271.0: 1375, 17300.0: 1376, 84000.0: 1377, 22484.0: 1378, 62471.0: 1379, 20724.0: 1380, 31864.0: 1381, 313119.0: 1382, 18343.0: 1383, 10315.0: 1384, 4395.0: 1385, 2508.0: 1386, 8064.0: 1387, 22800.0: 1388, 30835.0: 1389, 7120.0: 1390, 21975.0: 1391, 21104.0: 1392, 20373.0: 1393, 33100.0: 1394, 964.0: 1395, 15840.0: 1396, 5940.0: 1397, 25805.0: 1398, 80913.0: 1399, 1063.0: 1400, 28973.0: 1401, 21341.0: 1402, 3638.0: 1403, 42300.0: 1404, 4312.0: 1405, 40600.0: 1406, 35900.0: 1407, 14622.0: 1408, 22131.0: 1409, 67131.0: 1410, 5159.0: 1411, 18700.0: 1412, 14146.0: 1413, 27469.0: 1414, 12770.0: 1415, 16784.0: 1416, 2064.0: 1417, 130603.0: 1418, 716.0: 1419, 28760.0: 1420, 36400.0: 1421, 196966.0: 1422, 14450.0: 1423, 26082.0: 1424, 5448.0: 1425, 2544.0: 1426, 3542.0: 1427, 10570.0: 1428, 13100.0: 1429, 69000.0: 1430, 82000.0: 1431, 20748.0: 1432, 6932.0: 1433, 5250.0: 1434, 17020.0: 1435, 638.0: 1436, 47721.0: 1437, 21523.0: 1438, 62500.0: 1439, 11947.0: 1440, 18908.0: 1441, 28128.0: 1442, 96050.0: 1443, 20953.0: 1444, 14361.0: 1445, 103000.0: 1446, 6450.0: 1447, 74681.0: 1448, 3340.0: 1449, 10330.0: 1450, 48400.0: 1451, 6800.0: 1452, 17274.0: 1453, 10056.0: 1454, 26560.0: 1455, 11932.0: 1456, 6016.0: 1457, 24425.0: 1458, 35300.0: 1459, 10873.0: 1460, 3859.0: 1461, 24850.0: 1462, 15826.0: 1463, 85200.0: 1464, 17256.0: 1465, 75500.0: 1466, 133603.0: 1467, 55166.0: 1468, 615.0: 1469, 1825.0: 1470, 52700.0: 1471, 350.0: 1472, 2696.0: 1473, 9644.0: 1474, 14820.0: 1475, 9986.0: 1476, 9612.0: 1477, 2951.0: 1478, 9015.0: 1479, 51230.0: 1480, 21250.0: 1481, 15416.0: 1482, 27700.0: 1483, 18090.0: 1484, 24016.0: 1485, 22320.0: 1486, 4944.0: 1487, 3924.0: 1488, 29365.0: 1489, 4152.0: 1490, 10681.0: 1491, 27676.0: 1492, 83603.0: 1493, 87500.0: 1494, 12175.0: 1495, 3276.0: 1496, 132000.0: 1497, 20465.0: 1498, 13478.0: 1499, 6685.0: 1500, 34008.0: 1501, 14467.0: 1502, 13970.0: 1503, 41576.0: 1504, 206516.0: 1505, 45206.0: 1506, 8620.0: 1507, 29120.0: 1508, 8004.0: 1509, 105600.0: 1510, 2456.0: 1511, 798.0: 1512, 7438.0: 1513, 17316.0: 1514, 5694.0: 1515, 21700.0: 1516, 6011.0: 1517, 11820.0: 1518, 19464.0: 1519, 15929.0: 1520, 20532.0: 1521, 4140.0: 1522, 5980.0: 1523, 54274.0: 1524, 10047.0: 1525, 31443.0: 1526, 3240.0: 1527, 1010.0: 1528, 15621.0: 1529, 39112.0: 1530, 3618.0: 1531, 5771.0: 1532, 102000.0: 1533, 19241.0: 1534, 19520.0: 1535, 17700.0: 1536, 25800.0: 1537, 22387.0: 1538, 3541.0: 1539, 14440.0: 1540, 7757.0: 1541, 10300.0: 1542, 5536.0: 1543, 36735.0: 1544, 42460.0: 1545, 18356.0: 1546, 9576.0: 1547, 59100.0: 1548, 23050.0: 1549, 3752.0: 1550, 69120.0: 1551, 34360.0: 1552, 21328.0: 1553, 9186.0: 1554, 2268.0: 1555, 38150.0: 1556, 6828.0: 1557, 62756.0: 1558, 37720.0: 1559, 20044.0: 1560, 40800.0: 1561, 6864.0: 1562, 44623.0: 1563, 220123.0: 1564, 15124.0: 1565, 12540.0: 1566, 52366.0: 1567, 5234.0: 1568, 16200.0: 1569, 11371.0: 1570, 25983.0: 1571, 32828.0: 1572, 15289.0: 1573, 10482.0: 1574, 11088.0: 1575, 9550.0: 1576, 54450.0: 1577, 18360.0: 1578, 29900.0: 1579, 38374.0: 1580, 47320.0: 1581, 170.0: 1582, 26200.0: 1583, 28575.0: 1584, 46887.0: 1585, 35464.0: 1586, 6576.0: 1587, 13064.0: 1588, 13476.0: 1589, 7932.0: 1590, 43900.0: 1591, 19671.0: 1592, 33728.0: 1593, 20210.0: 1594, 19040.0: 1595, 19300.0: 1596, 13568.0: 1597, 10907.0: 1598, 180.0: 1599, 52328.0: 1600, 23450.0: 1601, 16221.0: 1602, 53132.0: 1603, 20400.0: 1604, 14030.0: 1605, 18400.0: 1606, 18300.0: 1607, 6560.0: 1608, 5103.0: 1609, 520.0: 1610, 43353.0: 1611, 7962.0: 1612, 6300.0: 1613, 37164.0: 1614, 6852.0: 1615, 20240.0: 1616, 98603.0: 1617, 1440.0: 1618, 694.0: 1619, 6362.0: 1620, 85000.0: 1621, 840.0: 1622, 280.0: 1623, 19818.0: 1624, 64531.0: 1625, 4776.0: 1626, 4294.0: 1627, 38479.0: 1628, 19654.0: 1629, 7336.0: 1630, 19800.0: 1631, 23997.0: 1632, 8025.0: 1633, 13051.0: 1634, 31150.0: 1635, 19769.0: 1636, 16084.0: 1637, 7524.0: 1638, 2950.0: 1639, 8812.0: 1640, 5374.0: 1641, 5872.0: 1642, 74500.0: 1643, 9080.0: 1644, 2030.0: 1645, 9248.0: 1646, 27960.0: 1647, 3946.0: 1648, 14560.0: 1649, 42532.0: 1650, 33800.0: 1651, 19888.0: 1652, 19650.0: 1653, 411.0: 1654, 20825.0: 1655, 15266.0: 1656, 31742.0: 1657, 1798.0: 1658, 8664.0: 1659, 38546.0: 1660, 34376.0: 1661, 182118.0: 1662, 40001.0: 1663, 6446.0: 1664, 30390.0: 1665, 25861.0: 1666, 23897.0: 1667, 72517.0: 1668, 36132.0: 1669, 12536.0: 1670, 6612.0: 1671, 19055.0: 1672, 3092.0: 1673, 27876.0: 1674, 18010.0: 1675, 1086.0: 1676, 12400.0: 1677, 13700.0: 1678, 120800.0: 1679, 18880.0: 1680, 973.0: 1681, 23636.0: 1682, 41014.0: 1683, 28941.0: 1684, 17680.0: 1685, 16985.0: 1686, 37091.0: 1687, 18975.0: 1688, 42620.0: 1689, 15846.0: 1690, 167.0: 1691, 31039.0: 1692, 12271.0: 1693, 3718.0: 1694, 12260.0: 1695, -2500.0: 1696, 166429.0: 1697, 17802.0: 1698, 3140.0: 1699, 34159.0: 1700}, {0.0: 0, 40.0: 1, 90.0: 2, 100.0: 3, 160.0: 4, 200.0: 5, 224.0: 6, 229.0: 7, 300.0: 8, 325.0: 9, 340.0: 10, 385.0: 11, 390.0: 12, 400.0: 13, 441.0: 14, 480.0: 15, 495.0: 16, 500.0: 17, 507.0: 18, 537.0: 19, 559.0: 20, 570.0: 21, 576.0: 22, 600.0: 23, 630.0: 24, 640.0: 25, 651.0: 26, 653.0: 27, 660.0: 28, 700.0: 29, 711.0: 30, 720.0: 31, 750.0: 32, 759.0: 33, 780.0: 34, 800.0: 35, 818.0: 36, 857.0: 37, 900.0: 38, 915.0: 39, 928.0: 40, 995.0: 41, 1000.0: 42, 1029.0: 43, 1091.0: 44, 1100.0: 45, 1125.0: 46, 1135.0: 47, 1138.0: 48, 1200.0: 49, 1280.0: 50, 1369.0: 51, 1400.0: 52, 1406.0: 53, 1433.0: 54, 1463.0: 55, 1484.0: 56, 1488.0: 57, 1500.0: 58, 1548.0: 59, 1600.0: 60, 1656.0: 61, 1685.0: 62, 1700.0: 63, 1709.0: 64, 1720.0: 65, 1740.0: 66, 1750.0: 67, 1800.0: 68, 1900.0: 69, 1979.0: 70, 2000.0: 71, 2100.0: 72, 2190.0: 73, 2200.0: 74, 2262.0: 75, 2310.0: 76, 2335.0: 77, 2356.0: 78, 2400.0: 79, 2500.0: 80, 2571.0: 81, 2580.0: 82, 2605.0: 83, 2617.0: 84, 2620.0: 85, 2657.0: 86, 2769.0: 87, 2800.0: 88, 2834.0: 89, 3000.0: 90, 3030.0: 91, 3120.0: 92, 3150.0: 93, 3200.0: 94, 3215.0: 95, 3273.0: 96, 3336.0: 97, 3360.0: 98, 3400.0: 99, 3500.0: 100, 3513.0: 101, 3527.0: 102, 3588.0: 103, 3600.0: 104, 3700.0: 105, 3741.0: 106, 3761.0: 107, 3800.0: 108, 3807.0: 109, 3880.0: 110, 3885.0: 111, 4000.0: 112, 4033.0: 113, 4080.0: 114, 4095.0: 115, 4100.0: 116, 4176.0: 117, 4200.0: 118, 4210.0: 119, 4300.0: 120, 4314.0: 121, 4320.0: 122, 4400.0: 123, 4420.0: 124, 4475.0: 125, 4500.0: 126, 4522.0: 127, 4560.0: 128, 4606.0: 129, 4608.0: 130, 4662.0: 131, 4727.0: 132, 4800.0: 133, 4851.0: 134, 4876.0: 135, 4900.0: 136, 5000.0: 137, 5100.0: 138, 5118.0: 139, 5200.0: 140, 5217.0: 141, 5500.0: 142, 5525.0: 143, 5553.0: 144, 5558.0: 145, 5572.0: 146, 5600.0: 147, 5670.0: 148, 5750.0: 149, 5760.0: 150, 5900.0: 151, 5959.0: 152, 5966.0: 153, 6000.0: 154, 6039.0: 155, 6063.0: 156, 6125.0: 157, 6180.0: 158, 6200.0: 159, 6225.0: 160, 6240.0: 161, 6262.0: 162, 6356.0: 163, 6400.0: 164, 6500.0: 165, 6578.0: 166, 6640.0: 167, 6643.0: 168, 6720.0: 169, 6737.0: 170, 6760.0: 171, 6833.0: 172, 6900.0: 173, 6908.0: 174, 6918.0: 175, 6930.0: 176, 7000.0: 177, 7015.0: 178, 7072.0: 179, 7200.0: 180, 7212.0: 181, 7220.0: 182, 7300.0: 183, 7500.0: 184, 7515.0: 185, 7566.0: 186, 7600.0: 187, 7644.0: 188, 7800.0: 189, 7865.0: 190, 7900.0: 191, 7920.0: 192, 7941.0: 193, 7944.0: 194, 8000.0: 195, 8100.0: 196, 8118.0: 197, 8120.0: 198, 8146.0: 199, 8160.0: 200, 8162.0: 201, 8180.0: 202, 8200.0: 203, 8376.0: 204, 8400.0: 205, 8500.0: 206, 8529.0: 207, 8583.0: 208, 8600.0: 209, 8640.0: 210, 8683.0: 211, 8700.0: 212, 8768.0: 213, 8795.0: 214, 8800.0: 215, 8840.0: 216, 8937.0: 217, 8980.0: 218, 9000.0: 219, 9032.0: 220, 9100.0: 221, 9120.0: 222, 9180.0: 223, 9200.0: 224, 9336.0: 225, 9360.0: 226, 9400.0: 227, 9600.0: 228, 9680.0: 229, 9732.0: 230, 9800.0: 231, 9880.0: 232, 9900.0: 233, 9947.0: 234, 9961.0: 235, 10000.0: 236, 10063.0: 237, 10142.0: 238, 10183.0: 239, 10211.0: 240, 10400.0: 241, 10401.0: 242, 10479.0: 243, 10500.0: 244, 10523.0: 245, 10560.0: 246, 10700.0: 247, 10740.0: 248, 10800.0: 249, 10916.0: 250, 10982.0: 251, 11000.0: 252, 11006.0: 253, 11040.0: 254, 11054.0: 255, 11200.0: 256, 11250.0: 257, 11267.0: 258, 11300.0: 259, 11412.0: 260, 11440.0: 261, 11476.0: 262, 11500.0: 263, 11520.0: 264, 11665.0: 265, 11673.0: 266, 11711.0: 267, 11759.0: 268, 11901.0: 269, 11943.0: 270, 12000.0: 271, 12012.0: 272, 12102.0: 273, 12152.0: 274, 12422.0: 275, 12484.0: 276, 12500.0: 277, 12574.0: 278, 12618.0: 279, 12652.0: 280, 12657.0: 281, 12694.0: 282, 12695.0: 283, 12772.0: 284, 13000.0: 285, 13194.0: 286, 13260.0: 287, 13272.0: 288, 13282.0: 289, 13400.0: 290, 13500.0: 291, 13552.0: 292, 13600.0: 293, 13698.0: 294, 13800.0: 295, 13915.0: 296, 14000.0: 297, 14100.0: 298, 14400.0: 299, 14500.0: 300, 14692.0: 301, 14700.0: 302, 14788.0: 303, 14798.0: 304, 14800.0: 305, 14817.0: 306, 14916.0: 307, 14974.0: 308, 14976.0: 309, 15000.0: 310, 15084.0: 311, 15184.0: 312, 15250.0: 313, 15472.0: 314, 15500.0: 315, 15537.0: 316, 15560.0: 317, 15600.0: 318, 15791.0: 319, 15900.0: 320, 16000.0: 321, 16219.0: 322, 16320.0: 323, 16352.0: 324, 16387.0: 325, 16390.0: 326, 16400.0: 327, 16401.0: 328, 16407.0: 329, 16464.0: 330, 16471.0: 331, 16560.0: 332, 16577.0: 333, 16700.0: 334, 16800.0: 335, 17000.0: 336, 17200.0: 337, 17205.0: 338, 17300.0: 339, 17420.0: 340, 17481.0: 341, 17500.0: 342, 17617.0: 343, 17636.0: 344, 17703.0: 345, 17849.0: 346, 17900.0: 347, 17930.0: 348, 18000.0: 349, 18207.0: 350, 18209.0: 351, 18310.0: 352, 18361.0: 353, 18392.0: 354, 18500.0: 355, 18560.0: 356, 18700.0: 357, 18720.0: 358, 18783.0: 359, 18800.0: 360, 18865.0: 361, 18928.0: 362, 18962.0: 363, 18985.0: 364, 19000.0: 365, 19200.0: 366, 19300.0: 367, 19500.0: 368, 19600.0: 369, 19644.0: 370, 19672.0: 371, 20000.0: 372, 20142.0: 373, 20208.0: 374, 20304.0: 375, 20400.0: 376, 20677.0: 377, 20699.0: 378, 20800.0: 379, 20812.0: 380, 21000.0: 381, 21310.0: 382, 21504.0: 383, 21521.0: 384, 21600.0: 385, 21638.0: 386, 21680.0: 387, 21776.0: 388, 21939.0: 389, 21982.0: 390, 22000.0: 391, 22300.0: 392, 22324.0: 393, 22396.0: 394, 22411.0: 395, 22420.0: 396, 22493.0: 397, 22534.0: 398, 22741.0: 399, 22824.0: 400, 23000.0: 401, 23103.0: 402, 23519.0: 403, 23642.0: 404, 23662.0: 405, 23700.0: 406, 23741.0: 407, 23755.0: 408, 23940.0: 409, 24000.0: 410, 24105.0: 411, 24174.0: 412, 24332.0: 413, 24334.0: 414, 24375.0: 415, 24413.0: 416, 24500.0: 417, 24744.0: 418, 24880.0: 419, 24910.0: 420, 24936.0: 421, 25000.0: 422, 25023.0: 423, 25280.0: 424, 25500.0: 425, 25620.0: 426, 25680.0: 427, 26000.0: 428, 26024.0: 429, 26100.0: 430, 26167.0: 431, 26300.0: 432, 26455.0: 433, 26500.0: 434, 26699.0: 435, 26742.0: 436, 26865.0: 437, 26930.0: 438, 27000.0: 439, 27071.0: 440, 27189.0: 441, 27227.0: 442, 27449.0: 443, 27500.0: 444, 27595.0: 445, 27600.0: 446, 27623.0: 447, 27953.0: 448, 27978.0: 449, 28000.0: 450, 28400.0: 451, 28500.0: 452, 28566.0: 453, 28700.0: 454, 28800.0: 455, 28817.0: 456, 29000.0: 457, 29290.0: 458, 29300.0: 459, 29304.0: 460, 29414.0: 461, 29499.0: 462, 29500.0: 463, 29700.0: 464, 30000.0: 465, 30121.0: 466, 30248.0: 467, 30341.0: 468, 30500.0: 469, 30602.0: 470, 30663.0: 471, 31000.0: 472, 31200.0: 473, 31256.0: 474, 31712.0: 475, 31900.0: 476, 32000.0: 477, 32199.0: 478, 32478.0: 479, 32500.0: 480, 32565.0: 481, 32600.0: 482, 32764.0: 483, 32794.0: 484, 33000.0: 485, 33280.0: 486, 33400.0: 487, 33500.0: 488, 33600.0: 489, 33678.0: 490, 33700.0: 491, 33824.0: 492, 34000.0: 493, 34522.0: 494, 34578.0: 495, 34700.0: 496, 34975.0: 497, 35000.0: 498, 35301.0: 499, 35303.0: 500, 35400.0: 501, 35482.0: 502, 35709.0: 503, 36000.0: 504, 36139.0: 505, 36355.0: 506, 36436.0: 507, 36525.0: 508, 36686.0: 509, 36806.0: 510, 36920.0: 511, 37000.0: 512, 37279.0: 513, 37400.0: 514, 37406.0: 515, 37442.0: 516, 37500.0: 517, 38000.0: 518, 38447.0: 519, 38475.0: 520, 39000.0: 521, 39038.0: 522, 39072.0: 523, 39491.0: 524, 40000.0: 525, 40100.0: 526, 40500.0: 527, 41000.0: 528, 41325.0: 529, 41480.0: 530, 41600.0: 531, 42000.0: 532, 42654.0: 533, 42794.0: 534, 43000.0: 535, 43253.0: 536, 43500.0: 537, 44000.0: 538, 45000.0: 539, 45438.0: 540, 45700.0: 541, 46000.0: 542, 46024.0: 543, 46364.0: 544, 46800.0: 545, 46838.0: 546, 47000.0: 547, 47256.0: 548, 47292.0: 549, 47462.0: 550, 47643.0: 551, 47661.0: 552, 47800.0: 553, 47901.0: 554, 47908.0: 555, 48000.0: 556, 48134.0: 557, 48224.0: 558, 48329.0: 559, 48373.0: 560, 49000.0: 561, 49114.0: 562, 49164.0: 563, 49171.0: 564, 49614.0: 565, 50000.0: 566, 50100.0: 567, 50700.0: 568, 50930.0: 569, 50989.0: 570, 51000.0: 571, 51598.0: 572, 51900.0: 573, 52000.0: 574, 52078.0: 575, 52500.0: 576, 53000.0: 577, 53558.0: 578, 54000.0: 579, 54568.0: 580, 55000.0: 581, 55892.0: 582, 56000.0: 583, 56362.0: 584, 57000.0: 585, 57196.0: 586, 57883.0: 587, 58000.0: 588, 58286.0: 589, 59000.0: 590, 59500.0: 591, 60000.0: 592, 61000.0: 593, 61143.0: 594, 62000.0: 595, 63000.0: 596, 63750.0: 597, 64000.0: 598, 64350.0: 599, 64421.0: 600, 65000.0: 601, 65125.0: 602, 66000.0: 603, 67000.0: 604, 67312.0: 605, 68760.0: 606, 69000.0: 607, 70000.0: 608, 70250.0: 609, 72000.0: 610, 74000.0: 611, 74547.0: 612, 75000.0: 613, 75327.0: 614, 76000.0: 615, 76283.0: 616, 77000.0: 617, 77380.0: 618, 77500.0: 619, 78659.0: 620, 79750.0: 621, 80000.0: 622, 80400.0: 623, 80841.0: 624, 83000.0: 625, 85513.0: 626, 86000.0: 627, 89000.0: 628, 90000.0: 629, 92000.0: 630, 93000.0: 631, 94000.0: 632, 95000.0: 633, 96000.0: 634, 100000.0: 635, 111000.0: 636, 115000.0: 637, 120000.0: 638, 124584.0: 639, 125000.0: 640, 126000.0: 641, 130000.0: 642, 132000.0: 643, 133200.0: 644, 135000.0: 645, 195516.0: 646, 999999.0: 647, 37800.0: 648, 2300.0: 649, 138000.0: 650, 53528.0: 651, 37802.0: 652, 16985.0: 653, 860.0: 654, 17220.0: 655, 10472.0: 656, 4030.0: 657, 428.0: 658, 24383.0: 659, 24300.0: 660, 21800.0: 661, 18544.0: 662, 13289.0: 663, 34400.0: 664, 100450.0: 665, 365.0: 666, 85000.0: 667, 250.0: 668, 18270.0: 669, 13325.0: 670, 12643.0: 671, 34140.0: 672, 8074.0: 673, 46500.0: 674, 37364.0: 675, 51930.0: 676, 7283.0: 677, 28903.0: 678, 4930.0: 679, 21597.0: 680, 7040.0: 681, 2991.0: 682, 2550.0: 683, 14822.0: 684, 3040.0: 685, 260.0: 686, 2336.0: 687, 3143.0: 688, 21462.0: 689, 665.0: 690, 84000.0: 691, 20622.0: 692, 20700.0: 693, 31864.0: 694, 18328.0: 695, 4395.0: 696, 22800.0: 697, 30264.0: 698, 7120.0: 699, 8420.0: 700, 15838.0: 701, 20373.0: 702, 59700.0: 703, 15840.0: 704, 25192.0: 705, 80674.0: 706, 21341.0: 707, 42300.0: 708, 4312.0: 709, 18900.0: 710, 22131.0: 711, 67131.0: 712, 14146.0: 713, 25358.0: 714, 16784.0: 715, 6800.0: 716, 36400.0: 717, 20778.0: 718, 5250.0: 719, 110000.0: 720, 638.0: 721, 21500.0: 722, 105000.0: 723, 11947.0: 724, 17600.0: 725, 22975.0: 726, 15100.0: 727, 6525.0: 728, 37200.0: 729, 22500.0: 730, 26200.0: 731, 11932.0: 732, 9425.0: 733, 10873.0: 734, 3859.0: 735, 15826.0: 736, 615.0: 737, 350.0: 738, 14820.0: 739, 21199.0: 740, 15391.0: 741, 18090.0: 742, 22320.0: 743, 23200.0: 744, 3924.0: 745, 8720.0: 746, 27676.0: 747, 6031.0: 748, 1190.0: 749, 18555.0: 750, 13970.0: 751, 45081.0: 752, 29120.0: 753, 38800.0: 754, 105600.0: 755, 9500.0: 756, 5694.0: 757, 8273.0: 758, 7238.0: 759, 4140.0: 760, 5980.0: 761, 31280.0: 762, 3240.0: 763, 15552.0: 764, 3618.0: 765, 101000.0: 766, 5400.0: 767, 14825.0: 768, 22387.0: 769, 14440.0: 770, 4286.0: 771, 36735.0: 772, 3752.0: 773, 34110.0: 774, 62756.0: 775, 10447.0: 776, 1300.0: 777, 6864.0: 778, 17800.0: 779, 44366.0: 780, 5234.0: 781, 11371.0: 782, 25983.0: 783, 32828.0: 784, 10482.0: 785, 8430.0: 786, 19127.0: 787, 47320.0: 788, 170.0: 789, 40327.0: 790, 35464.0: 791, 13200.0: 792, 13014.0: 793, 20560.0: 794, 14985.0: 795, 7787.0: 796, 23450.0: 797, 15896.0: 798, 51486.0: 799, 5117.0: 800, 1332.0: 801, 6300.0: 802, 6362.0: 803, 840.0: 804, 280.0: 805, 38479.0: 806, 19654.0: 807, 23255.0: 808, 13051.0: 809, 31083.0: 810, 19283.0: 811, 16084.0: 812, 520.0: 813, 27960.0: 814, 14560.0: 815, 33800.0: 816, 17700.0: 817, 20825.0: 818, 31693.0: 819, 37546.0: 820, 27909.0: 821, 118000.0: 822, 30390.0: 823, 2792.0: 824, 23897.0: 825, 62400.0: 826, 11855.0: 827, 2354.0: 828, 725.0: 829, 93400.0: 830, 6009.0: 831, 948.0: 832, 13848.0: 833, 31622.0: 834, 28941.0: 835, 16480.0: 836, 16960.0: 837, 24900.0: 838, 18975.0: 839, 6700.0: 840, 42320.0: 841, 15846.0: 842, 167.0: 843, 25839.0: 844, 5228.0: 845, 3718.0: 846, 12260.0: 847, 17802.0: 848, 3100.0: 849, 34076.0: 850}, {-9999.0: 0, -7000.0: 1, -4000.0: 2, -3850.0: 3, -2005.0: 4, -2000.0: 5, -1200.0: 6, -900.0: 7, -650.0: 8, -200.0: 9, -100.0: 10, 0.0: 11, 1.0: 12, 50.0: 13, 60.0: 14, 75.0: 15, 270.0: 16, 288.0: 17, 355.0: 18, 500.0: 19, 600.0: 20, 700.0: 21, 900.0: 22, 1000.0: 23, 1320.0: 24, 1500.0: 25, 1863.0: 26, 2000.0: 27, 2300.0: 28, 2400.0: 29, 2700.0: 30, 2726.0: 31, 2734.0: 32, 2839.0: 33, 2880.0: 34, 3000.0: 35, 3169.0: 36, 3500.0: 37, 3568.0: 38, 3775.0: 39, 3900.0: 40, 4000.0: 41, 4200.0: 42, 4370.0: 43, 4500.0: 44, 4782.0: 45, 5000.0: 46, 5600.0: 47, 5820.0: 48, 5900.0: 49, 6000.0: 50, 6530.0: 51, 6690.0: 52, 6800.0: 53, 7000.0: 54, 7500.0: 55, 8000.0: 56, 8200.0: 57, 8772.0: 58, 9000.0: 59, 9500.0: 60, 9600.0: 61, 10000.0: 62, 10006.0: 63, 11000.0: 64, 11090.0: 65, 11600.0: 66, 12000.0: 67, 12986.0: 68, 13000.0: 69, 14000.0: 70, 14536.0: 71, 15000.0: 72, 16000.0: 73, 18000.0: 74, 19000.0: 75, 20000.0: 76, 24000.0: 77, 24500.0: 78, 25000.0: 79, 27000.0: 80, 28000.0: 81, 28857.0: 82, 29000.0: 83, 30000.0: 84, 32000.0: 85, 33000.0: 86, 35000.0: 87, 35800.0: 88, 36000.0: 89, 38000.0: 90, 40000.0: 91, 42000.0: 92, 43358.0: 93, 44566.0: 94, 45000.0: 95, 50000.0: 96, 50500.0: 97, 60000.0: 98, 64000.0: 99, 65000.0: 100, 70000.0: 101, 75000.0: 102, 75622.0: 103, 80000.0: 104, 85000.0: 105, 123515.0: 106, 999999.0: 107, 3759.0: 108, -1530.0: 109, -850.0: 110, 5500.0: 111, 1200.0: 112, -1536.0: 113, 10800.0: 114, -1950.0: 115, 4435.0: 116, 22484.0: 117, 460.0: 118, 598.0: 119, 2064.0: 120, 12500.0: 121, 82000.0: 122, 19700.0: 123, 4800.0: 124, 43000.0: 125, 47000.0: 126, 8500.0: 127, 3771.0: 128, 417.0: 129, 62000.0: 130, 5196.0: 131, 555.0: 132, 18880.0: 133, 8400.0: 134, 33658.0: 135, 300.0: 136, 66000.0: 137, 5200.0: 138, -2500.0: 139, 54950.0: 140}, {0.0: 0, 500.0: 1, 1200.0: 2, 999999.0: 3, -750.0: 4, 4000.0: 5, 3670.0: 6}, {0.0: 0, 116.0: 1, 200.0: 2, 300.0: 3, 303.0: 4, 379.0: 5, 600.0: 6, 630.0: 7, 654.0: 8, 664.0: 9, 695.0: 10, 699.0: 11, 755.0: 12, 900.0: 13, 960.0: 14, 1000.0: 15, 1018.0: 16, 1100.0: 17, 1120.0: 18, 1158.0: 19, 1200.0: 20, 1400.0: 21, 1440.0: 22, 1446.0: 23, 1540.0: 24, 1600.0: 25, 1616.0: 26, 1716.0: 27, 1800.0: 28, 1920.0: 29, 1992.0: 30, 2000.0: 31, 2106.0: 32, 2160.0: 33, 2237.0: 34, 2254.0: 35, 2268.0: 36, 2363.0: 37, 2378.0: 38, 2400.0: 39, 2424.0: 40, 2500.0: 41, 2580.0: 42, 2620.0: 43, 2640.0: 44, 2650.0: 45, 2652.0: 46, 2700.0: 47, 2778.0: 48, 2800.0: 49, 2801.0: 50, 2842.0: 51, 2900.0: 52, 2916.0: 53, 2976.0: 54, 3000.0: 55, 3010.0: 56, 3022.0: 57, 3100.0: 58, 3108.0: 59, 3112.0: 60, 3150.0: 61, 3240.0: 62, 3264.0: 63, 3347.0: 64, 3370.0: 65, 3372.0: 66, 3406.0: 67, 3500.0: 68, 3515.0: 69, 3588.0: 70, 3599.0: 71, 3600.0: 72, 3646.0: 73, 3718.0: 74, 3720.0: 75, 3743.0: 76, 3751.0: 77, 3780.0: 78, 3837.0: 79, 3840.0: 80, 3876.0: 81, 3888.0: 82, 3912.0: 83, 3965.0: 84, 4000.0: 85, 4008.0: 86, 4018.0: 87, 4020.0: 88, 4080.0: 89, 4104.0: 90, 4116.0: 91, 4152.0: 92, 4200.0: 93, 4272.0: 94, 4295.0: 95, 4296.0: 96, 4300.0: 97, 4311.0: 98, 4369.0: 99, 4417.0: 100, 4451.0: 101, 4452.0: 102, 4464.0: 103, 4486.0: 104, 4498.0: 105, 4500.0: 106, 4536.0: 107, 4547.0: 108, 4608.0: 109, 4656.0: 110, 4679.0: 111, 4690.0: 112, 4700.0: 113, 4800.0: 114, 4876.0: 115, 4896.0: 116, 4900.0: 117, 4956.0: 118, 4992.0: 119, 5000.0: 120, 5019.0: 121, 5028.0: 122, 5062.0: 123, 5087.0: 124, 5088.0: 125, 5100.0: 126, 5160.0: 127, 5184.0: 128, 5194.0: 129, 5196.0: 130, 5200.0: 131, 5207.0: 132, 5256.0: 133, 5290.0: 134, 5292.0: 135, 5316.0: 136, 5364.0: 137, 5376.0: 138, 5398.0: 139, 5400.0: 140, 5424.0: 141, 5448.0: 142, 5500.0: 143, 5568.0: 144, 5578.0: 145, 5600.0: 146, 5604.0: 147, 5652.0: 148, 5664.0: 149, 5672.0: 150, 5688.0: 151, 5700.0: 152, 5724.0: 153, 5750.0: 154, 5760.0: 155, 5796.0: 156, 5820.0: 157, 5878.0: 158, 5892.0: 159, 5952.0: 160, 5998.0: 161, 6000.0: 162, 6048.0: 163, 6095.0: 164, 6120.0: 165, 6167.0: 166, 6168.0: 167, 6192.0: 168, 6200.0: 169, 6214.0: 170, 6240.0: 171, 6251.0: 172, 6295.0: 173, 6298.0: 174, 6360.0: 175, 6400.0: 176, 6418.0: 177, 6440.0: 178, 6480.0: 179, 6516.0: 180, 6540.0: 181, 6585.0: 182, 6600.0: 183, 6680.0: 184, 6694.0: 185, 6696.0: 186, 6700.0: 187, 6706.0: 188, 6732.0: 189, 6744.0: 190, 6780.0: 191, 6800.0: 192, 6816.0: 193, 6840.0: 194, 6888.0: 195, 7000.0: 196, 7020.0: 197, 7092.0: 198, 7164.0: 199, 7200.0: 200, 7224.0: 201, 7235.0: 202, 7294.0: 203, 7318.0: 204, 7320.0: 205, 7367.0: 206, 7378.0: 207, 7400.0: 208, 7402.0: 209, 7416.0: 210, 7436.0: 211, 7440.0: 212, 7450.0: 213, 7463.0: 214, 7488.0: 215, 7500.0: 216, 7548.0: 217, 7590.0: 218, 7600.0: 219, 7650.0: 220, 7654.0: 221, 7655.0: 222, 7704.0: 223, 7726.0: 224, 7763.0: 225, 7800.0: 226, 7806.0: 227, 7811.0: 228, 7883.0: 229, 7942.0: 230, 8000.0: 231, 8015.0: 232, 8016.0: 233, 8027.0: 234, 8040.0: 235, 8150.0: 236, 8160.0: 237, 8244.0: 238, 8273.0: 239, 8362.0: 240, 8400.0: 241, 8412.0: 242, 8422.0: 243, 8496.0: 244, 8500.0: 245, 8520.0: 246, 8560.0: 247, 8626.0: 248, 8636.0: 249, 8660.0: 250, 8700.0: 251, 8928.0: 252, 9000.0: 253, 9042.0: 254, 9120.0: 255, 9336.0: 256, 9360.0: 257, 9408.0: 258, 9468.0: 259, 9480.0: 260, 9600.0: 261, 9649.0: 262, 9706.0: 263, 9708.0: 264, 9779.0: 265, 9800.0: 266, 9899.0: 267, 10000.0: 268, 10138.0: 269, 10344.0: 270, 10346.0: 271, 10357.0: 272, 10367.0: 273, 10510.0: 274, 10584.0: 275, 10642.0: 276, 10703.0: 277, 10716.0: 278, 10870.0: 279, 10944.0: 280, 11000.0: 281, 11123.0: 282, 11747.0: 283, 12000.0: 284, 12063.0: 285, 14000.0: 286, 14040.0: 287, 15000.0: 288, 15600.0: 289, 23090.0: 290, 99999.0: 291, 2411.0: 292, 3480.0: 293, 9166.0: 294, 7674.0: 295, 7860.0: 296, 5950.0: 297, 4031.0: 298, 6960.0: 299, 2568.0: 300, 10175.0: 301, 8758.0: 302, 2748.0: 303, 11688.0: 304, 11660.0: 305, 7079.0: 306, 4846.0: 307, 8268.0: 308, 8446.0: 309, 7786.0: 310, 6408.0: 311, 4164.0: 312, 1248.0: 313, 7668.0: 314, 1872.0: 315, 10401.0: 316, 2278.0: 317, 7920.0: 318, 1847.0: 319, 4606.0: 320, 3970.0: 321, 6624.0: 322, 4560.0: 323, 730.0: 324, 5940.0: 325, 7728.0: 326, 12770.0: 327, 3960.0: 328, 3984.0: 329, 2544.0: 330, 1544.0: 331, 7462.0: 332, 4512.0: 333, 1050.0: 334, 7836.0: 335, 9078.0: 336, 3340.0: 337, 10570.0: 338, 4368.0: 339, 8028.0: 340, 8100.0: 341, 5304.0: 342, 1825.0: 343, 4440.0: 344, 9612.0: 345, 1451.0: 346, 4284.0: 347, 8628.0: 348, 6180.0: 349, 13000.0: 350, 3276.0: 351, 8134.0: 352, 2856.0: 353, 5710.0: 354, 5262.0: 355, 1027.0: 356, 4804.0: 357, 1884.0: 358, 414.0: 359, 5900.0: 360, 13057.0: 361, 3543.0: 362, 4400.0: 363, 5603.0: 364, 1010.0: 365, 336.0: 366, 5280.0: 367, 2076.0: 368, 7560.0: 369, 10560.0: 370, 9576.0: 371, 4906.0: 372, 7322.0: 373, 6768.0: 374, 5512.0: 375, 7344.0: 376, 7426.0: 377, 4680.0: 378, 7720.0: 379, 4572.0: 380, 3192.0: 381, 3828.0: 382, 7919.0: 383, 6323.0: 384, 1053.0: 385, 4644.0: 386, 6852.0: 387, 700.0: 388, 7774.0: 389, 8232.0: 390, 5928.0: 391, 4776.0: 392, 4294.0: 393, 5016.0: 394, 7524.0: 395, 2950.0: 396, 3336.0: 397, 8292.0: 398, 5374.0: 399, 6226.0: 400, 3946.0: 401, 11867.0: 402, 1798.0: 403, 6467.0: 404, 1294.0: 405, 3300.0: 406, 6362.0: 407, 11375.0: 408, 1388.0: 409, 2527.0: 410}, {0.0: 0, 5.0: 1, 9.0: 2, 75.0: 3, 100.0: 4, 182.0: 5, 341.0: 6, 348.0: 7, 385.0: 8, 400.0: 9, 405.0: 10, 460.0: 11, 500.0: 12, 520.0: 13, 620.0: 14, 630.0: 15, 700.0: 16, 840.0: 17, 936.0: 18, 948.0: 19, 1000.0: 20, 1011.0: 21, 1128.0: 22, 1176.0: 23, 1200.0: 24, 1308.0: 25, 1440.0: 26, 1500.0: 27, 1572.0: 28, 1632.0: 29, 1680.0: 30, 1800.0: 31, 1965.0: 32, 1980.0: 33, 2016.0: 34, 2100.0: 35, 2172.0: 36, 2280.0: 37, 2300.0: 38, 2400.0: 39, 2448.0: 40, 2500.0: 41, 2556.0: 42, 2580.0: 43, 2628.0: 44, 2640.0: 45, 2680.0: 46, 3000.0: 47, 3084.0: 48, 3180.0: 49, 3215.0: 50, 3360.0: 51, 3420.0: 52, 3500.0: 53, 3540.0: 54, 3600.0: 55, 3624.0: 56, 3642.0: 57, 3744.0: 58, 3820.0: 59, 3864.0: 60, 3982.0: 61, 4002.0: 62, 4012.0: 63, 4032.0: 64, 4075.0: 65, 4080.0: 66, 4092.0: 67, 4100.0: 68, 4296.0: 69, 4320.0: 70, 4416.0: 71, 4500.0: 72, 4595.0: 73, 4683.0: 74, 4800.0: 75, 5040.0: 76, 5196.0: 77, 5316.0: 78, 5676.0: 79, 5688.0: 80, 5880.0: 81, 6000.0: 82, 6012.0: 83, 6140.0: 84, 6196.0: 85, 6360.0: 86, 6500.0: 87, 6504.0: 88, 6516.0: 89, 6695.0: 90, 6696.0: 91, 6700.0: 92, 6720.0: 93, 6840.0: 94, 6960.0: 95, 6996.0: 96, 7000.0: 97, 7002.0: 98, 7060.0: 99, 7116.0: 100, 7200.0: 101, 7224.0: 102, 7500.0: 103, 7560.0: 104, 7600.0: 105, 7764.0: 106, 7956.0: 107, 8000.0: 108, 8016.0: 109, 8040.0: 110, 8100.0: 111, 8136.0: 112, 8208.0: 113, 8240.0: 114, 8280.0: 115, 8328.0: 116, 8338.0: 117, 8448.0: 118, 8508.0: 119, 8988.0: 120, 9600.0: 121, 9688.0: 122, 9888.0: 123, 15189.0: 124, 99999.0: 125, 4488.0: 126, 5998.0: 127, 4000.0: 128, 3120.0: 129, 8256.0: 130, 782.0: 131, 4600.0: 132, 5760.0: 133, 2052.0: 134, 8300.0: 135, 888.0: 136, 7320.0: 137, 5116.0: 138, 5000.0: 139, 1344.0: 140, 4848.0: 141, 2508.0: 142, 660.0: 143, 234.0: 144, 465.0: 145, 716.0: 146, 1464.0: 147, 1700.0: 148, 1998.0: 149, 3070.0: 150, 1620.0: 151, 1504.0: 152, 2040.0: 153, 3432.0: 154, 6016.0: 155, 2496.0: 156, 5644.0: 157, 2000.0: 158, 4944.0: 159, 6144.0: 160, 2544.0: 161, 8004.0: 162, 798.0: 163, 7024.0: 164, 8500.0: 165, 7464.0: 166, 4560.0: 167, 1250.0: 168, 1864.0: 169, 6576.0: 170, 180.0: 171, 350.0: 172, 5200.0: 173, 3318.0: 174, 694.0: 175, 2320.0: 176, 470.0: 177, 924.0: 178}, {0.0: 0, 19.0: 1, 45.0: 2, 50.0: 3, 60.0: 4, 75.0: 5, 100.0: 6, 150.0: 7, 166.0: 8, 256.0: 9, 300.0: 10, 316.0: 11, 368.0: 12, 400.0: 13, 480.0: 14, 500.0: 15, 560.0: 16, 600.0: 17, 636.0: 18, 664.0: 19, 700.0: 20, 701.0: 21, 720.0: 22, 800.0: 23, 833.0: 24, 840.0: 25, 864.0: 26, 876.0: 27, 900.0: 28, 984.0: 29, 1000.0: 30, 1008.0: 31, 1049.0: 32, 1100.0: 33, 1176.0: 34, 1272.0: 35, 1295.0: 36, 1320.0: 37, 1328.0: 38, 1376.0: 39, 1476.0: 40, 1500.0: 41, 1503.0: 42, 1600.0: 43, 1675.0: 44, 1700.0: 45, 1764.0: 46, 1800.0: 47, 1920.0: 48, 2000.0: 49, 2025.0: 50, 2050.0: 51, 2184.0: 52, 2253.0: 53, 2400.0: 54, 2490.0: 55, 2520.0: 56, 2600.0: 57, 2640.0: 58, 2700.0: 59, 2880.0: 60, 2976.0: 61, 3000.0: 62, 3470.0: 63, 3500.0: 64, 3600.0: 65, 3650.0: 66, 3732.0: 67, 4000.0: 68, 4020.0: 69, 4140.0: 70, 4300.0: 71, 4316.0: 72, 4400.0: 73, 4500.0: 74, 4800.0: 75, 5000.0: 76, 5100.0: 77, 5112.0: 78, 5400.0: 79, 6000.0: 80, 6494.0: 81, 6526.0: 82, 7200.0: 83, 8000.0: 84, 8400.0: 85, 9888.0: 86, 10000.0: 87, 12000.0: 88, 15000.0: 89, 15600.0: 90, 19200.0: 91, 28641.0: 92, 99999.0: 93, 1200.0: 94, 240.0: 95, 9600.0: 96, 2800.0: 97, 6800.0: 98, 200.0: 99, 1150.0: 100, 6456.0: 101, 2940.0: 102, 4260.0: 103, 1308.0: 104, 3400.0: 105, 80.0: 106, 830.0: 107, 1756.0: 108, 2176.0: 109, 2825.0: 110, 2268.0: 111, 124.0: 112, 1120.0: 113, 19187.0: 114, 1900.0: 115, 120.0: 116, 3700.0: 117, 5872.0: 118, 2550.0: 119, 19260.0: 120, 681.0: 121, 3092.0: 122, 301.0: 123, 14400.0: 124}, {889086544.0: 0, 1684325040.0: 1, 3214953198.0: 2}, {1303016265.0: 0, 1684325040.0: 1, 3748236362.0: 2}]
target = ''
target_column = 56
important_idxs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]
ignore_idxs = []
classifier_type = 'NN'
num_attr = 56
n_classes = 2
model_cap = 117
w_h = np.array([[-0.10192162543535233, 0.15252402424812317, -0.257895290851593, -0.0925103947520256, -0.43713563680648804, -0.24505503475666046, -0.05530545487999916, -0.36190614104270935, -0.013847474940121174, 0.06247832253575325, -0.07490334659814835, -0.32661372423171997, -0.34514129161834717, -0.0992293581366539, 0.04045015573501587, 0.2327200472354889, 0.18494874238967896, -0.12075725942850113, 0.019896019250154495, -0.40845710039138794, -0.39258819818496704, 0.029441671445965767, -0.32050666213035583, 0.03128787875175476, -0.3297593593597412, -0.2613264322280884, -0.3531663119792938, 0.05589357018470764, -0.1506781280040741, 0.11900730431079865, 0.08196097612380981, 0.05840907245874405, -0.1083281934261322, -0.2148185670375824, -0.18583083152770996, -0.0821080282330513, 0.03023117408156395, -0.34832820296287537, 0.1491069793701172, -0.1561485230922699, -0.39482858777046204, -0.2953045666217804, 0.1673336625099182, 0.17808817327022552, -0.41028445959091187, 0.10721973329782486, 0.055369410663843155, 0.08373111486434937, 0.17319564521312714, -0.2652200758457184, -0.004969736095517874, 0.03882533684372902, -0.20320092141628265, -0.01765933446586132, -0.20171861350536346, 0.07324560731649399], [-0.2173636555671692, -0.11990368366241455, 0.08097098767757416, 0.11408144980669022, -0.2855863869190216, -0.09979678690433502, 0.05590887740254402, -0.13495565950870514, -0.02622881717979908, -0.19213661551475525, -0.011794822290539742, -0.345439612865448, -0.3139532804489136, -0.23415319621562958, 0.16461049020290375, -0.2073523849248886, -0.06082981452345848, -0.27235057950019836, 0.20677421987056732, 0.20091359317302704, -0.36080193519592285, -0.36164772510528564, -0.33511221408843994, -0.13174821436405182, -0.20676825940608978, -0.33521270751953125, -0.21973392367362976, -0.06842335313558578, -0.295736163854599, -0.061533596366643906, -0.19997163116931915, -0.2817789912223816, -0.08253093808889389, 0.13702243566513062, -0.34387537837028503, -0.10439452528953552, -0.24778234958648682, 0.1935470849275589, -0.41460856795310974, -0.4277673661708832, 0.20676106214523315, -0.3337002992630005, -0.0647646114230156, -0.15941324830055237, -0.2072138637304306, -0.1847429722547531, 0.08321979641914368, -0.1094493567943573, -0.43593740463256836, -0.30802419781684875, -0.2304585576057434, 0.04370706155896187, -0.31908494234085083, 0.17516396939754486, -0.17644533514976501, -0.1868467479944229]])
b_h = np.array([-0.13861209154129028, -0.3770175576210022])
w_o = np.array([[0.9926838278770447, 0.6058546900749207]])
b_o = np.array(-2.698664903640747)


class PredictorError(Exception):

    def __init__(self, msg, code):
        self.msg = msg
        self.code = code

    def __str__(self):
        return self.msg
def __column_norm(column, mappings):
    normalized_col = np.zeros(column.shape[0])
    for i, val in enumerate(column.reshape(-1)):
        if val not in mappings:
            mappings[val] = int(max(mappings.values())) + 1
        normalized_col[i] = mappings[val]
    return normalized_col


def __normalize(arr):
    for i,mapping in zip(list_of_cols_to_normalize, column_mappings):
        if i >= arr.shape[1]:
            break
        col = arr[:, i]
        normcol = __column_norm(col, mapping)
        arr[:, i] = normcol
    return arr


def __convert(cell):
    value = str(cell)
    try:
        result = int(value)
        return result
    except ValueError:
        try:
            result = float(value)
            if math.isnan(result):
                raise PredictorError('NaN value found. Aborting.', code=1)
            return result
        except ValueError:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            return result
        except Exception as e:
            raise e


def __get_key(val, dictionary):
    if dictionary == {}:
        return val
    for key, value in dictionary.items():
        if val == value:
            return key
    if val not in dictionary.values():
        raise PredictorError(f"Label {val} key does not exist", code=2)


def __confusion_matrix(y_true, y_pred, json):
    stats = {}
    labels = np.array(list(mapping.keys()))
    sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
    for class_i in range(n_classes):
        class_i_label = __get_key(class_i, mapping)
        stats[int(class_i)] = {}
        class_i_indices = np.argwhere(y_true == class_i_label)
        not_class_i_indices = np.argwhere(y_true != class_i_label)
        # None represents N/A in this case
        stats[int(class_i)]['TP'] = TP = int(np.sum(y_pred[class_i_indices] == class_i_label)) if class_i_indices.size > 0 else None
        stats[int(class_i)]['FN'] = FN = int(np.sum(y_pred[class_i_indices] != class_i_label)) if class_i_indices.size > 0 else None
        stats[int(class_i)]['TN'] = TN = int(np.sum(y_pred[not_class_i_indices] != class_i_label)) if not_class_i_indices.size > 0 else None
        stats[int(class_i)]['FP'] = FP = int(np.sum(y_pred[not_class_i_indices] == class_i_label)) if not_class_i_indices.size > 0 else None
        if TP is None or FN is None or (TP + FN == 0):
            stats[int(class_i)]['TPR'] = None
        else:
            stats[int(class_i)]['TPR'] = (TP / (TP + FN))
        if TN is None or FP is None or (TN + FP == 0):
            stats[int(class_i)]['TNR'] = None
        else:
            stats[int(class_i)]['TNR'] = (TN / (TN + FP))
        if TP is None or FP is None or (TP + FP == 0):
            stats[int(class_i)]['PPV'] = None
        else:
            stats[int(class_i)]['PPV'] = (TP / (TP + FP))
        if TN is None or FN is None or (TN + FN == 0):
            stats[int(class_i)]['NPV'] = None
        else:
            stats[int(class_i)]['NPV'] = (TN / (TN + FN))
        if TP is None or FP is None or FN is None or (TP + FP + FN == 0):
            stats[int(class_i)]['F1'] = None
        else:
            stats[int(class_i)]['F1'] = ((2 * TP) / (2 * TP + FP + FN))
        if TP is None or FP is None or FN is None or (TP + FP + FN == 0):
            stats[int(class_i)]['TS'] = None
        else:
            stats[int(class_i)]['TS'] = (TP / (TP + FP + FN))

    if not report_cmat:
        return np.array([]), stats

    label_to_ind = {label: i for i, label in enumerate(labels)}
    y_pred = np.array([label_to_ind.get(x, n_classes + 1) for x in y_pred])
    y_true = np.array([label_to_ind.get(x, n_classes + 1) for x in y_true])

    ind = np.logical_and(y_pred < n_classes, y_true < n_classes)
    y_pred = y_pred[ind]
    y_true = y_true[ind]
    sample_weight = sample_weight[ind]

    cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_classes, n_classes), dtype=np.int64).toarray()
    with np.errstate(all='ignore'):
        cm = np.nan_to_num(cm)

    return cm, stats


def __preprocess_and_clean_in_memory(arr):
    clean_arr = np.zeros((len(arr), len(important_idxs)))
    for i, row in enumerate(arr):
        try:
            row_used_cols_only = [row[i] for i in important_idxs]
        except IndexError:
            error_str = f"The input has shape ({len(arr)}, {len(row)}) but the expected shape is (*, {len(ignorecolumns) + len(important_idxs)})."
            if len(arr) == num_attr and len(arr[0]) != num_attr:
                error_str += "\n\nNote: You may have passed an input directly to 'preprocess_and_clean_in_memory' or 'predict_in_memory' "
                error_str += "rather than as an element of a list. Make sure that even single instances "
                error_str += "are enclosed in a list. Example: predict_in_memory(0) is invalid but "
                error_str += "predict_in_memory([0]) is valid."
            raise PredictorError(error_str, 3)
        clean_arr[i] = [float(__convert(field)) for field in row_used_cols_only]
    return clean_arr


def __classify(arr, return_probabilities=False):
    h = np.dot(arr, w_h.T) + b_h
    relu = np.maximum(h, np.zeros_like(h))
    out = np.dot(relu, w_o.T) + b_o
    if return_probabilities:
        exp_o = np.zeros((out.shape[0],))
        idxs_negative = np.argwhere(out < 0.).reshape(-1)
        if idxs_negative.shape[0] > 0:
            exp_o[idxs_negative] = 1. - 1. / (1. + np.exp(out[idxs_negative])).reshape(-1)
        idxs_positive = np.argwhere(out >= 0.).reshape(-1)
        if idxs_positive.shape[0] > 0:
            exp_o[idxs_positive] = 1. / (1. + np.exp(-out[idxs_positive])).reshape(-1)
        exp_o = exp_o.reshape(-1, 1)
        output = np.concatenate((1. - exp_o, exp_o), axis=1)
    else:
        output = (out >= 0).astype('int').reshape(-1)
    return output



def __validate_kwargs(kwargs):
    for key in kwargs:

        if key not in ['return_probabilities']:
            raise PredictorError(f'{key} is not a keyword argument for Brainome\'s {classifier_type} predictor. Please see the documentation.', 4)


def __validate_data(row_or_arr, validate, row_num=None):
    if validate:
        expected_columns = len(important_idxs) + len(ignore_idxs) + 1
    else:
        expected_columns = len(important_idxs) + len(ignore_idxs)

    input_is_array = isinstance(row_or_arr, np.ndarray)
    n_cols = row_or_arr.shape[1] if input_is_array else len(row_or_arr)

    if n_cols != expected_columns:

        if row_num is None:
            err_str = f"Your data contains {n_cols} columns but {expected_columns} are required."
        else:
            err_str = f"At row {row_num}, your data contains {n_cols} columns but {expected_columns} are required."

        if validate:
            err_str += " The predictor's validate() method works on data that has the same columns in the same order as were present in the training CSV."
            err_str += " This includes the target column and features that are not used by the model but existed in the training CSV."
            if n_cols == 1 + len(important_idxs):
                err_str += f" We suggest confirming that the {len(ignore_idxs)} unused features are present in the data."
            elif n_cols == len(important_idxs):
                err_str += f" We suggest confirming that the {len(ignore_idxs)} unused features are present in the data as well as the target column. "
            elif n_cols == len(important_idxs) + len(ignore_idxs):
                err_str += " We suggest confirming that the target column present in the data. "
            err_str += " To make predictions, see the predictor's predict() method."
        else:
            err_str += " The predictor's predict() method works on data that has the same feature columns in the same relative order as were present in the training CSV."
            err_str += " This DOES NOT include the target column but DOES include features that are not used by the model but existed in the training CSV."
            if n_cols == 1 + len(important_idxs):
                err_str += f" We suggest confirming that the {len(ignore_idxs)} unused features are present in the data and that the target column is not present."
            elif n_cols == len(important_idxs):
                err_str += f" We suggest confirming that the {len(ignore_idxs)} unused features are present in the data."
            elif n_cols == 1 + len(important_idxs) + len(ignore_idxs):
                err_str += " We suggest confirming that the target column is not present."
            err_str += " To receive a performance summary, instead of make predictions, see the predictor's validate() method."

        raise PredictorError(err_str, 5)

    else:

        if not input_is_array:
            return row_or_arr


def __write_predictions(arr, header, headerless, trim, outfile=None):
    predictions = predict(arr)

    if not headerless:
        if trim:
            header = ','.join([x for i, x in enumerate(header) if i in important_idxs] + ['Prediction'])
        else:
            header = ','.join(header.tolist() + ['Prediction'])
        if outfile is None:
            print(header)
        else:
            print(header, file=outfile)

    for row, prediction in zip(arr, predictions):
        if trim:
            row = ['"' + field + '"' if ',' in field else field for i, field in enumerate(row) if i in important_idxs]
        else:
            row = ['"' + field + '"' if ',' in field else field for field in row]
        row.append(prediction)
        if outfile is None:
            print(','.join(row))
        else:
            print(','.join(row), file=outfile)


def load_data(csvfile, headerless, validate):
    """
    Parameters
    ----------
    csvfile : str
        The path to the CSV file containing the data.

    headerless : bool
        True if the CSV does not contain a header.

    validate : bool
        True if the data should be loaded to be used by the predictor's validate() method.
        False if the data should be loaded to be used by the predictor's predict() method.

    Returns
    -------
    arr : np.ndarray
        The data (observations and labels) found in the CSV without any header.

    data : np.ndarray or NoneType
        None if validate is False, otherwise the observations (data without the labels) found in the CSV.

    labels : np.ndarray or NoneType
        None if the validate is False, otherwise the labels found in the CSV.

    header : np.ndarray or NoneType
        None if the CSV is headerless, otherwise the header.
    """

    with open(csvfile, 'r', encoding='utf-8') as csvinput:
        arr = np.array([__validate_data(row, validate, row_num=i) for i, row in enumerate(csv.reader(csvinput)) if row != []], dtype=str)
    if headerless:
        header = None
    else:
        header = arr[0]
        arr = arr[1:]
    if validate:
        labels = arr[:, target_column]
        feature_columns = [i for i in range(arr.shape[1]) if i != target_column]
        data = arr[:, feature_columns]
    else:
        data, labels = None, None

    if validate and ignorelabels != []:
        idxs_to_keep = np.argwhere(np.logical_not(np.isin(labels, ignorelabels))).reshape(-1)
        labels = labels[idxs_to_keep]
        data = data[idxs_to_keep]

    return arr, data, labels, header


def predict(arr, remap=True, **kwargs):
    """
    Parameters
    ----------
    arr : list[list]
        An array of inputs to be cleaned by 'preprocess_and_clean_in_memory'. This
        should contain all the features that were present in the training data,
        regardless of whether or not they are used by the model, with the same
        relative order as in the training data. There should be no target column.


    remap : bool
        If True and 'return_probs' is False, remaps the output to the original class
        label. If 'return_probs' is True this instead adds a header indicating which
        original class label each column of output corresponds to.

    **kwargs :
        return_probabilities : bool
            If true, return class membership probabilities instead of classifications.

    Returns
    -------
    output : np.ndarray

        A numpy array of
            1. Class predictions if 'return_probabilities' is False.
            2. Class probabilities if 'return_probabilities' is True.

    """
    if not isinstance(arr, np.ndarray) and not isinstance(arr, list):
        raise PredictorError(f'Data must be provided to \'predict\' and \'validate\' as a list or np.ndarray, but an input of type {type(arr).__name__} was found.', 6)
    if isinstance(arr, list):
        arr = np.array(arr, dtype=str)

    kwargs = kwargs or {}
    __validate_kwargs(kwargs)
    __validate_data(arr, False)
    remove_bad_chars = lambda x: str(x).replace('"', '').replace(',', '').replace('(', '').replace(')', '').replace("'", '')
    arr = [[remove_bad_chars(field) for field in row] for row in arr]
    arr = __preprocess_and_clean_in_memory(arr)

    arr = __normalize(arr)

    output = __classify(arr, **kwargs)

    if remap:
        if kwargs.get('return_probabilities'):
            header = np.array([__get_key(i, mapping) for i in range(output.shape[1])], dtype=str).reshape(1, -1)
            output = np.concatenate((header, output), axis=0)
        else:
            output = np.array([__get_key(prediction, mapping) for prediction in output])

    return output


def validate(arr, labels):
    """
    Parameters
    ----------
    cleanarr : np.ndarray
        An array of float values that has undergone each pre-
        prediction step.

    Returns
    -------
    count : int
        A count of the number of instances in cleanarr.

    correct_count : int
        A count of the number of correctly classified instances in
        cleanarr.

    numeachclass : dict
        A dictionary mapping each class to its number of instances.

    outputs : np.ndarray
        The output of the predictor's '__classify' method on cleanarr.
    """
    predictions = predict(arr)
    correct_count = int(np.sum(predictions.reshape(-1) == labels.reshape(-1)))
    count = predictions.shape[0]
    
    class_0, class_1 = __get_key(0, mapping), __get_key(1, mapping)
    num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0
    num_TP = int(np.sum(np.logical_and(predictions.reshape(-1) == class_1, labels.reshape(-1) == class_1)))
    num_TN = int(np.sum(np.logical_and(predictions.reshape(-1) == class_0, labels.reshape(-1) == class_0)))
    num_FN = int(np.sum(np.logical_and(predictions.reshape(-1) == class_0, labels.reshape(-1) == class_1)))
    num_FP = int(np.sum(np.logical_and(predictions.reshape(-1) == class_1, labels.reshape(-1) == class_0)))
    num_class_0 = int(np.sum(labels.reshape(-1) == class_0))
    num_class_1 = int(np.sum(labels.reshape(-1) == class_1))
    return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, predictions


def __main():
    parser = argparse.ArgumentParser(description='Predictor trained on ' + str(TRAINFILE))
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    parser.add_argument('-json', action="store_true", default=False, help="report measurements as json")
    parser.add_argument('-trim', action="store_true", help="If true, the prediction will not output ignored columns.")
    args = parser.parse_args()
    faulthandler.enable()

    arr, data, labels, header = load_data(csvfile=args.csvfile, headerless=args.headerless, validate=args.validate)

    if not args.validate:
        __write_predictions(arr, header, args.headerless, args.trim)
    else:

        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds = validate(data, labels)

        classcounts = np.bincount(np.array([mapping[label] for label in labels], dtype='int32')).reshape(-1)
        class_balance = (classcounts[np.argwhere(classcounts > 0)] / arr.shape[0]).reshape(-1).tolist()
        best_guess = round(100.0 * np.max(class_balance), 2)
        H = float(-1.0 * sum([class_balance[i] * math.log(class_balance[i]) / math.log(2) for i in range(len(class_balance))]))
        modelacc = int(float(correct_count * 10000) / count) / 100.0
        mtrx, stats = __confusion_matrix(np.array(labels).reshape(-1), np.array(preds).reshape(-1), args.json)

        if args.json:
            json_dict = {'instance_count': count,
                         'classifier_type': classifier_type,
                         'classes': n_classes,
                         'number_correct': correct_count,
                         'accuracy': {
                             'best_guess': (best_guess/100),
                             'improvement': (modelacc - best_guess)/100,
                              'model_accuracy': (modelacc/100),
                         },
                         'model_capacity': model_cap,
                         'generalization_ratio': int(float(correct_count * 100) / model_cap) / 100.0 * H,
                         'model_efficiency': int(100 * (modelacc - best_guess) / model_cap) / 100.0,
                         'shannon_entropy_of_labels': H,
                         'class_balance': class_balance,
                         'confusion_matrix': mtrx.tolist(),
                         'multiclass_stats': stats}

            print(json.dumps(json_dict))
        else:
            pad = lambda s, length, pad_right: str(s) + ' ' * max(0, length - len(str(s))) if pad_right else ' ' * max(0, length - len(str(s))) + str(s)
            labels = np.array(list(mapping.keys())).reshape(-1, 1)
            max_class_name_len = max([len(clss) for clss in mapping.keys()] + [7])

            max_TP_len = max([len(str(stats[key]['TP'])) for key in stats.keys()] + [2])
            max_FP_len = max([len(str(stats[key]['FP'])) for key in stats.keys()] + [2])
            max_TN_len = max([len(str(stats[key]['TN'])) for key in stats.keys()] + [2])
            max_FN_len = max([len(str(stats[key]['FN'])) for key in stats.keys()] + [2])

            cmat_template_1 = "    {} | {}"
            cmat_template_2 = "    {} | " + " {} " * n_classes
            acc_by_class_template_1 = "    {} | {}  {}  {}  {}  {}  {}  {}  {}  {}  {}"

            acc_by_class_lengths = [max_class_name_len, max_TP_len, max_FP_len, max_TN_len, max_FN_len, 7, 7, 7, 7, 7, 7]
            acc_by_class_header_fields = ['target', 'TP', 'FP', 'TN', 'FN', 'TPR', 'TNR', 'PPV', 'NPV', 'F1', 'TS']
            print("Classifier Type:                    Neural Network")

            print(f"System Type:                        {n_classes}-way classifier\n")

            print("Accuracy:")
            print("    Best-guess accuracy:            {:.2f}%".format(best_guess))
            print("    Model accuracy:                 {:.2f}%".format(modelacc) + " (" + str(int(correct_count)) + "/" + str(count) + " correct)")
            print("    Improvement over best guess:    {:.2f}%".format(modelacc - best_guess) + " (of possible " + str(round(100 - best_guess, 2)) + "%)\n")

            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(correct_count * 100) / model_cap) / 100.0 * H) + " bits/bit")

            if report_cmat:
                max_cmat_entry_len = len(str(int(np.max(mtrx))))
                mtrx = np.concatenate((labels, mtrx.astype('str')), axis=1).astype('str')
                max_pred_len = (mtrx.shape[1] - 1) * max_cmat_entry_len + n_classes * 2 - 1
                print("\nConfusion Matrix:\n")
                print(cmat_template_1.format(pad("Actual", max_class_name_len, False), "Predicted"))
                print(cmat_template_1.format("-" * max_class_name_len, "-" * max(max_pred_len, 9)))
                for row in mtrx:
                    print(cmat_template_2.format(
                        *[pad(field, max_class_name_len if i == 0 else max_cmat_entry_len, False) for i, field in enumerate(row)]))

            print("\nAccuracy by Class:\n")
            print(acc_by_class_template_1.format(
                *[pad(header_field, length, False) for i, (header_field, length) in enumerate(zip(acc_by_class_header_fields, acc_by_class_lengths))]))
            print(acc_by_class_template_1.format(
                *["-" * length for length in acc_by_class_lengths]))

            pct_format_string = "{:8.2%}"      # width = 8, decimals = 2
            for raw_class in mapping.keys():
                class_stats = stats[int(mapping[raw_class])]
                TP, FP, TN, FN = class_stats.get('TP', None), class_stats.get('FP', None), class_stats.get('TN', None), class_stats.get('FN', None)
                TPR = pct_format_string.format(class_stats['TPR']) if class_stats['TPR'] is not None else 'N/A'
                TNR = pct_format_string.format(class_stats['TNR']) if class_stats['TNR'] is not None else 'N/A'
                PPV = pct_format_string.format(class_stats['PPV']) if class_stats['PPV'] is not None else 'N/A'
                NPV = pct_format_string.format(class_stats['NPV']) if class_stats['NPV'] is not None else 'N/A'
                F1 = pct_format_string.format(class_stats['F1']) if class_stats['F1'] is not None else 'N/A'
                TS = pct_format_string.format(class_stats['TS']) if class_stats['TS'] is not None else 'N/A'
                line_fields = [raw_class, TP, FP, TN, FN, TPR, TNR, PPV, NPV, F1, TS]
                print(acc_by_class_template_1.format(
                    *[pad(field, length, False) for i, (field, length) in enumerate(zip(line_fields, acc_by_class_lengths))]))


if __name__ == "__main__":
    try:
        __main()
    except PredictorError as e:
        print(e, file=sys.stderr)
        sys.exit(e.code)
    except Exception as e:
        print(f"An unknown exception of type {type(e).__name__} occurred.", file=sys.stderr)
        sys.exit(-1)
